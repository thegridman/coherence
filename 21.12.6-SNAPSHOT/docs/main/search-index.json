{
    "docs": [
        {
            "location": "/examples/guides/460-topics/README",
            "text": " What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; ",
            "title": "Data Model"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. ",
            "title": "Topics Cache Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = session.createPublisher(\"public-messages\"); // create a subscriber to receive public messages subscriberPublic = session.createSubscriber(\"public-messages\"); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = session.createPublisher(\"private-messages\"); // create a subscriber to receive private messages subscriberPrivate = session.createSubscriber(\"private-messages\", inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } ",
            "title": "The Chat Application"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = session.createPublisher(\"public-messages\"); // create a subscriber to receive public messages subscriberPublic = session.createSubscriber(\"public-messages\"); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = session.createPublisher(\"private-messages\"); // create a subscriber to receive private messages subscriberPrivate = session.createSubscriber(\"private-messages\", inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? ",
            "title": "Build and Run the Example"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " In this tutorial you have learned how use Coherence Topics. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Topics Overview and Configuration Performing Topics Operations ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " This tutorial walks through the steps to use Coherence Topics using a simple Chat Application. Table of Contents What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also What You Will Build You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . What You Need About 15 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Initial Project Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = session.createPublisher(\"public-messages\"); // create a subscriber to receive public messages subscriberPublic = session.createSubscriber(\"public-messages\"); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = session.createPublisher(\"private-messages\"); // create a subscriber to receive private messages subscriberPrivate = session.createSubscriber(\"private-messages\", inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } Build and Run the Example Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? Summary In this tutorial you have learned how use Coherence Topics. See Also Topics Overview and Configuration Performing Topics Operations ",
            "title": "Topics"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " About 30 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This example shows how to build a custom aggregator which we will use to count how many times a particular word occurs in documents stored in Coherence maps. The Document class is a standard POJO with an identifier, and a string for the document contents. What You Need About 30 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The data model consists of the Document class which represents a document with text contents that we are going to search. <markup lang=\"java\" >public class Document implements Serializable { private String id; private String contents; Review the Example Code The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " In this guide we have shown you how to create custom aggregators that allow you to process data stored in Coherence in parallel. You have created a custom aggregator to count the number of times a word appears in documents stored in Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " Performing Data Grid Operations ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This guide walks you through how to create custom aggregators that allow you to process data stored in Coherence in parallel. Coherence supports entry aggregators that perform operations against all, or a subset of entries to obtain a single result. This aggregation is carried out in parallel across the cluster and is a map-reduce type of operation which can be performed efficiently across large amounts of data. See the Coherence Documentation for detailed information on Aggregations. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build This example shows how to build a custom aggregator which we will use to count how many times a particular word occurs in documents stored in Coherence maps. The Document class is a standard POJO with an identifier, and a string for the document contents. What You Need About 30 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the Document class which represents a document with text contents that we are going to search. <markup lang=\"java\" >public class Document implements Serializable { private String id; private String contents; Review the Example Code The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. Summary In this guide we have shown you how to create custom aggregators that allow you to process data stored in Coherence in parallel. You have created a custom aggregator to count the number of times a word appears in documents stored in Coherence. See Also Performing Data Grid Operations ",
            "title": "Custom Aggregators"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. ",
            "title": "Coherence NamedMap "
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Obtain a NamedMap Instance"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionaly there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . ",
            "title": "A Quick Word About Serialization"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } ",
            "title": "The Put Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . ",
            "title": "The Get Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . ",
            "title": "Get Multiple Values"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . ",
            "title": "The Remove Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . ",
            "title": "The Remove Mapping Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. ",
            "title": " NamedCache Transient Data"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " This guide walks you through the basic CRUD operations on a Coherence NamedMap . What You Will Build The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Coherence NamedMap The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. Create the Test Class The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. A Quick Word About Serialization In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionaly there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . The Put Method The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } The Get Method We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . Get Multiple Values The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . The Remove Method We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . The Remove Mapping Method An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . NamedCache Transient Data As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. Summary You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Put Get and Remove Operations"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " Coherence guarantees a MapEvent , which represents a change to an Entry in the source Map , will be delivered exactly once given the client remains a member of the associated service. Note Each NamedMap is associated to a Service and typically this service is a PartitionedService therefore provides distributed storage and partitioned/sharded access. The remaining description of MapEvent guarantees will assume the use of a PartitionedService and therefore providing resilience to process, machine, rack, or site failure. Importantly this guarantee is maintained regardless of failures in the system that the services are configured to handle. Therefore a backup-count of 1 results in the service being able to tolerate the loss of a single unit where unit could be a node (JVM/member), machine, rack or site. Upon encountering a fault Coherence will restore data and continue service for the affected partitions. This data redundancy is extended to MapEvent delivery to clients. Therefore if a member hosting primary partitions was to die, and said member had sent the backup message for some change but failed to deliver the MapEvent to the client, the new primary member (that was a backup member and went through the automatic promotion protocol) would emit MapEvent messages that had not been confirm by clients. The client is aware of MapEvent messages it had already processed, therefore if the MapEvent was received by the client but the primary did not receive the ACK thus causing the backup to send a duplicate, the client will simply not replay the event. ",
            "title": "MapEvent Guarantees"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " While it is rare, there are some scenarios that result in abnormal service termination. This has affected Coherence users by causing references to Service s or NamedMap s to become invalid and therefore unusable. Instead of having numerous applications, and internal call sites, be defensive to these invalid handles Coherence chose to introduce a 'safe' layer between the call site and the raw/internal service. The 'safe' layer remains valid when services abnormally terminate and additionally may cause the underlying service to restart. In the case of a 'safe' NamedMap any MapEvent listeners registered will automatically be re-registered. However, any events that had occurred after the member left the service and before it re-joined will be lost, or more formally not observed by this member&#8217;s listener. This has been worked around in many cases by these members/clients re-synchronizing their local state with the remote data; this is the approach taken for ContinuousQueryCache s. ",
            "title": "Abnormal Service Termination"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " Coherence Extend provides a means for a client to connect to a cluster via a conduit referred to as a proxy. An extend client wraps up the intended request, forwards to a proxy which executes said request with the results either streamed back to the client or sent as a single response. There are many reasons why one would chose using extend over being a member of the cluster; further documentation on extend can be found here . The liveness of the proxy is incredibly important to the extend clients that are connected to it. If the proxy does become unresponsive, or simply dies the extend client will transparently reconnect to another proxy. Similar to the aforementioned 'abnormal service termination' use case there is a potential of not observing MapEvents that had occurred at the source due to the proxy leaving the associated service or the extend client reconnecting to a different proxy and therefore re-registering the listener. Once again, there are means to work around this situation by observing the proxy disconnect / re-connect and causing a re-synchronization of extend client and the source. However, extend proxy failover is a significantly more likely event and has been raised by Coherence users. ",
            "title": "Extend Proxy Failover"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " A logical client receiving MapEvents may experience a process restart and it may be desirable to continue receiving MapEvents after the last received event, opposed to only receiving events after registration. For example, a client may be responsible for updating some auxiliary system by applying the contents of each MapEvent to that system. Additionally, it may be tracking the last received events and therefore could inform the source of the last event it received. A capable source could replay all events that were missed. ",
            "title": "Process Death"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " This level of redundancy and guarantee is sufficient for many applications and users of Coherence have been able to live with/workaround a particular shortcoming such that the product has not attempted to address it. The aforementioned guarantee of MapEvent delivery are all valid under the assumption that the member, that has registered for MapEvents, does not leave the service. If it does leave the associated service then these guarantees no longer apply. This can be problematic under a few contexts (described below) and has led to the need deliver a more general feature of event replay. Abnormal Service Termination While it is rare, there are some scenarios that result in abnormal service termination. This has affected Coherence users by causing references to Service s or NamedMap s to become invalid and therefore unusable. Instead of having numerous applications, and internal call sites, be defensive to these invalid handles Coherence chose to introduce a 'safe' layer between the call site and the raw/internal service. The 'safe' layer remains valid when services abnormally terminate and additionally may cause the underlying service to restart. In the case of a 'safe' NamedMap any MapEvent listeners registered will automatically be re-registered. However, any events that had occurred after the member left the service and before it re-joined will be lost, or more formally not observed by this member&#8217;s listener. This has been worked around in many cases by these members/clients re-synchronizing their local state with the remote data; this is the approach taken for ContinuousQueryCache s. Extend Proxy Failover Coherence Extend provides a means for a client to connect to a cluster via a conduit referred to as a proxy. An extend client wraps up the intended request, forwards to a proxy which executes said request with the results either streamed back to the client or sent as a single response. There are many reasons why one would chose using extend over being a member of the cluster; further documentation on extend can be found here . The liveness of the proxy is incredibly important to the extend clients that are connected to it. If the proxy does become unresponsive, or simply dies the extend client will transparently reconnect to another proxy. Similar to the aforementioned 'abnormal service termination' use case there is a potential of not observing MapEvents that had occurred at the source due to the proxy leaving the associated service or the extend client reconnecting to a different proxy and therefore re-registering the listener. Once again, there are means to work around this situation by observing the proxy disconnect / re-connect and causing a re-synchronization of extend client and the source. However, extend proxy failover is a significantly more likely event and has been raised by Coherence users. Process Death A logical client receiving MapEvents may experience a process restart and it may be desirable to continue receiving MapEvents after the last received event, opposed to only receiving events after registration. For example, a client may be responsible for updating some auxiliary system by applying the contents of each MapEvent to that system. Additionally, it may be tracking the last received events and therefore could inform the source of the last event it received. A capable source could replay all events that were missed. ",
            "title": "Raison D&#8217;etre"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " In considering how to address these various scenarios that result in event loss it became evident that the product needs an ability to retain events beyond receipt of delivery such that they can be replayed on request. This pushes certain requirements on different parts of the system: storage nodes must track, by storing, the MapEvent s as they are generated expose a monotonically increasing version per partition within a single cache ensure version semantics are maintained regardless of faults [TODO] ensure MapEvent storage is redundant [TODO] provide a MapEvent storage retention policy client nodes have a trivial facility to suggest received MapEvent versions are tracked and therefore events replayed when faced with restart provide a more advanced means such that the client can control the tracking of event versions With the above a MapListener can opt in by suggesting they are version aware and Coherence will automatically start tracking versions on the client. This does require a complementing server-side configuration in which the storage servers are tracking MapEvent s. For example: Start a storage server that is tracking events: <markup lang=\"bash\" >$ java -Dcoherence.distributed.persistence.mode=actice -Dcoherence.distributed.persistence.events.dir=/tmp/events-dir -jar coherence.jar Start a client that registers a version aware MapEvent listener. A snippet taken from a functional test in the repo has been provided below: <markup lang=\"java\" >List&lt;MapEvent&gt; listEvents = Collections.synchronizedList(new ArrayList&lt;&gt;()); MapListener&lt;Integer, String&gt; listener = new SimpleMapListener&lt;Integer, String&gt;() .addEventHandler(listEvents::add) .versioned(); cache.addMapListener(listener, 1, false); The above registration results in the client receiving events and tracking the latest version per partition. Upon abnormal service restart the client will automatically re-register itself and request versions after the last received version. More advanced clients can implement the VersionAwareMapListener interface directly. Implementors must return an implementation of VersionedPartitions which will be interrogated by Coherence upon registration either directly due to a call to NamedMap.addMapListener or indirectly due to a service restart. These versions are sent to the relevant storage servers and if a version is returned for a partition Coherence will return all known versions larger than or equal to the specified version. Additionally, certain formal constants are defined to allow a client to request storage servers to send: all known events current head and all future events (previously known as priming events) all future events (the current behavior) Note There is a natural harmony between the registration mechanism and the partitions returned from VersionedPartitions that occurs but is worth noting. For example, when registering a MapListener against a specific key only MapEvent s for said key will be received by this MapListener and therefore only versions for the associated partition will be tracked. The VersionedPartitions returned by this VersionAwareMapListener will only return a version for a single partition. However, this is worth being aware of if you do implement your own VersionAwareMapListener or VersionedPartitions data structure. ",
            "title": "Generic Event Replay"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " This feature is NOT production ready and we do NOT recommend using it in production at this point. There are some features that we believe are required prior to this feature graduating to production ready status that we will detail below for transparency. However, we are making this feature available in its current form to garner feedback from the community prior to locking down the APIs and semantics. The following improvements will be required prior to this feature being production ready: Redundant MapEvent storage MapEvent retention policy MapEvent delivery flow control Extend and gRPC client support Snapshots of MapEvent storage Allow a logical client to store its VersionedPartitions in Coherence Monitoring metrics ",
            "title": "Production Ready?"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " To get started please take a look at our guide on Durable Events . ",
            "title": "Get Started"
        },
        {
            "location": "/docs/core/06_durable_events",
            "text": " Coherence has provided the ability for clients to asynchronously observe data changes for almost two decades. This has proven to be an incredibly powerful feature allowing Coherence to offer components such as NearCaches and ViewCaches/CQCs , in addition to allowing customers to build truly event driven systems. A comprehensive overview of MapEvents in terms of the call back interface in addition to the registration mechanisms is provided in the official documentation . However, it is worth drawing attention to some of the guarantees offered by MapEvents to provide context of why the Durable Events feature is useful. MapEvent Guarantees Coherence guarantees a MapEvent , which represents a change to an Entry in the source Map , will be delivered exactly once given the client remains a member of the associated service. Note Each NamedMap is associated to a Service and typically this service is a PartitionedService therefore provides distributed storage and partitioned/sharded access. The remaining description of MapEvent guarantees will assume the use of a PartitionedService and therefore providing resilience to process, machine, rack, or site failure. Importantly this guarantee is maintained regardless of failures in the system that the services are configured to handle. Therefore a backup-count of 1 results in the service being able to tolerate the loss of a single unit where unit could be a node (JVM/member), machine, rack or site. Upon encountering a fault Coherence will restore data and continue service for the affected partitions. This data redundancy is extended to MapEvent delivery to clients. Therefore if a member hosting primary partitions was to die, and said member had sent the backup message for some change but failed to deliver the MapEvent to the client, the new primary member (that was a backup member and went through the automatic promotion protocol) would emit MapEvent messages that had not been confirm by clients. The client is aware of MapEvent messages it had already processed, therefore if the MapEvent was received by the client but the primary did not receive the ACK thus causing the backup to send a duplicate, the client will simply not replay the event. Raison D&#8217;etre This level of redundancy and guarantee is sufficient for many applications and users of Coherence have been able to live with/workaround a particular shortcoming such that the product has not attempted to address it. The aforementioned guarantee of MapEvent delivery are all valid under the assumption that the member, that has registered for MapEvents, does not leave the service. If it does leave the associated service then these guarantees no longer apply. This can be problematic under a few contexts (described below) and has led to the need deliver a more general feature of event replay. Abnormal Service Termination While it is rare, there are some scenarios that result in abnormal service termination. This has affected Coherence users by causing references to Service s or NamedMap s to become invalid and therefore unusable. Instead of having numerous applications, and internal call sites, be defensive to these invalid handles Coherence chose to introduce a 'safe' layer between the call site and the raw/internal service. The 'safe' layer remains valid when services abnormally terminate and additionally may cause the underlying service to restart. In the case of a 'safe' NamedMap any MapEvent listeners registered will automatically be re-registered. However, any events that had occurred after the member left the service and before it re-joined will be lost, or more formally not observed by this member&#8217;s listener. This has been worked around in many cases by these members/clients re-synchronizing their local state with the remote data; this is the approach taken for ContinuousQueryCache s. Extend Proxy Failover Coherence Extend provides a means for a client to connect to a cluster via a conduit referred to as a proxy. An extend client wraps up the intended request, forwards to a proxy which executes said request with the results either streamed back to the client or sent as a single response. There are many reasons why one would chose using extend over being a member of the cluster; further documentation on extend can be found here . The liveness of the proxy is incredibly important to the extend clients that are connected to it. If the proxy does become unresponsive, or simply dies the extend client will transparently reconnect to another proxy. Similar to the aforementioned 'abnormal service termination' use case there is a potential of not observing MapEvents that had occurred at the source due to the proxy leaving the associated service or the extend client reconnecting to a different proxy and therefore re-registering the listener. Once again, there are means to work around this situation by observing the proxy disconnect / re-connect and causing a re-synchronization of extend client and the source. However, extend proxy failover is a significantly more likely event and has been raised by Coherence users. Process Death A logical client receiving MapEvents may experience a process restart and it may be desirable to continue receiving MapEvents after the last received event, opposed to only receiving events after registration. For example, a client may be responsible for updating some auxiliary system by applying the contents of each MapEvent to that system. Additionally, it may be tracking the last received events and therefore could inform the source of the last event it received. A capable source could replay all events that were missed. Generic Event Replay In considering how to address these various scenarios that result in event loss it became evident that the product needs an ability to retain events beyond receipt of delivery such that they can be replayed on request. This pushes certain requirements on different parts of the system: storage nodes must track, by storing, the MapEvent s as they are generated expose a monotonically increasing version per partition within a single cache ensure version semantics are maintained regardless of faults [TODO] ensure MapEvent storage is redundant [TODO] provide a MapEvent storage retention policy client nodes have a trivial facility to suggest received MapEvent versions are tracked and therefore events replayed when faced with restart provide a more advanced means such that the client can control the tracking of event versions With the above a MapListener can opt in by suggesting they are version aware and Coherence will automatically start tracking versions on the client. This does require a complementing server-side configuration in which the storage servers are tracking MapEvent s. For example: Start a storage server that is tracking events: <markup lang=\"bash\" >$ java -Dcoherence.distributed.persistence.mode=actice -Dcoherence.distributed.persistence.events.dir=/tmp/events-dir -jar coherence.jar Start a client that registers a version aware MapEvent listener. A snippet taken from a functional test in the repo has been provided below: <markup lang=\"java\" >List&lt;MapEvent&gt; listEvents = Collections.synchronizedList(new ArrayList&lt;&gt;()); MapListener&lt;Integer, String&gt; listener = new SimpleMapListener&lt;Integer, String&gt;() .addEventHandler(listEvents::add) .versioned(); cache.addMapListener(listener, 1, false); The above registration results in the client receiving events and tracking the latest version per partition. Upon abnormal service restart the client will automatically re-register itself and request versions after the last received version. More advanced clients can implement the VersionAwareMapListener interface directly. Implementors must return an implementation of VersionedPartitions which will be interrogated by Coherence upon registration either directly due to a call to NamedMap.addMapListener or indirectly due to a service restart. These versions are sent to the relevant storage servers and if a version is returned for a partition Coherence will return all known versions larger than or equal to the specified version. Additionally, certain formal constants are defined to allow a client to request storage servers to send: all known events current head and all future events (previously known as priming events) all future events (the current behavior) Note There is a natural harmony between the registration mechanism and the partitions returned from VersionedPartitions that occurs but is worth noting. For example, when registering a MapListener against a specific key only MapEvent s for said key will be received by this MapListener and therefore only versions for the associated partition will be tracked. The VersionedPartitions returned by this VersionAwareMapListener will only return a version for a single partition. However, this is worth being aware of if you do implement your own VersionAwareMapListener or VersionedPartitions data structure. Production Ready? This feature is NOT production ready and we do NOT recommend using it in production at this point. There are some features that we believe are required prior to this feature graduating to production ready status that we will detail below for transparency. However, we are making this feature available in its current form to garner feedback from the community prior to locking down the APIs and semantics. The following improvements will be required prior to this feature being production ready: Redundant MapEvent storage MapEvent retention policy MapEvent delivery flow control Extend and gRPC client support Snapshots of MapEvent storage Allow a logical client to store its VersionedPartitions in Coherence Monitoring metrics Get Started To get started please take a look at our guide on Durable Events . ",
            "title": "Durable Events (experimental)"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/internal/template/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/internal/template/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "Sub-Heading"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "Summary"
        },
        {
            "location": "/examples/internal/template/README",
            "text": "",
            "title": "See Also"
        },
        {
            "location": "/examples/internal/template/README",
            "text": " What You Will Build What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Sub-Heading Summary See Also ",
            "title": "Title"
        },
        {
            "location": "/coherence-mp/README",
            "text": " Coherence provides a number of additional modules that provide support for different Microprofile APIs. Microprofile Config Using Coherence as a Microprofile config source. Microprofile Metrics Configure Coherence to publish metrics via the Microprofile metrics API. ",
            "title": "Coherence MP"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " Note The documentation on this site covers new features and improvements that are currently only available in the open source Coherence Community Edition (CE). For complete documentation covering all the features that are available both in the latest commercial editions (Enterprise and Grid Edition) and the Community Edition, please refer to the Official Documentation . Coherence is scalable, fault-tolerant, cloud-ready, distributed platform for building grid-based applications and reliably storing data. The product is used at scale, for both compute and raw storage, in a vast array of industries such as critical financial trading systems, high performance telecommunication products, and eCommerce applications. Typically, these deployments do not tolerate any downtime and Coherence is chosen due its novel features in death detection, application data evolvability, and the robust, battle-hardened core of the product that enables it to be seamlessly deployed and adapted within any ecosystem. At a high level, Coherence provides an implementation of the familiar Map&lt;K,V&gt; interface but rather than storing the associated data in the local process, it is partitioned (or sharded) across a number of designated remote nodes. This partitioning enables applications to not only distribute (and therefore scale) their storage across multiple processes, machines, racks, and data centers, but also to perform grid-based processing to truly harness the CPU resources of the machines. The Coherence interface NamedMap&lt;K,V&gt; (an extension of Map&lt;K,V&gt; provides methods to query, aggregate (map/reduce style), and compute (send functions to storage nodes for locally executed mutations) the data set. These capabilities, in addition to numerous other features, enable Coherence to be used as a framework to write robust, distributed applications. ",
            "title": "Overview"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " assistant Coherence What is Oracle Coherence? fa-rocket Quick Start A quick-start guide to using Coherence. fa-graduation-cap Guides & Tutorials Guides, examples and tutorial about Coherence features and best practice. import_contacts Docs Oracle Coherence commercial edition product documentation. library_books API Docs Browse the Coherence CE API Docs. ",
            "title": "Get Going"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " fa-cubes Core Coherence Core Improvements. forward_to_inbox Topics Coherence Topics (Messaging, Streams) Improvements. fa-bolt Distributed Concurrency Distributed implementations of executors, locks, atomics, etc. extension CDI Coherence CDI extensions. fa-cogs Microprofile Coherence Microprofile support. settings_ethernet gRPC Coherence gRPC server and client. ",
            "title": "New Features"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " fa-plug Plugins Build tool plugins to aid Coherence application development. fa-th Container Images Example Coherence OCI container (Docker) images. ",
            "title": "Tools"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " An exact topic mapping maps a specific topic name to a topic scheme definition. An application must provide the exact name as specified in the mapping to use a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below creates a single topic mapping that maps the topic name exampleTopic to a paged-topic-scheme definition with the scheme name `example-topic-scheme . <markup lang=\"xml\" title=\"Sample Exact Topic Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;example-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Using Exact Topic Mappings"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Name pattern topic mappings allow applications to use patterns when specifying a topic name. Patterns use the asterisk ( * ) wildcard. Name patterns alleviate an application from having to know the exact name of a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below a topic mappings using the wildcard ( \\* ) to map any topic name with the prefix account- to a paged-topic-scheme definition with the scheme name account-topic-scheme . <markup lang=\"xml\" title=\"Sample Topic Name Pattern Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;account-*&lt;/topic-name&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;AccountDistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Using Named Pattern Topic Mappings"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " A topic can have zero, one, or more durable subscriber groups defined in the topic-mapping for the topic. The subscriber group(s) are created along with the topic and are therefore ensured to exist before any data is published to the topic. A subscriber group does not have to be defined on a topic’s topic-mapping for a subscriber to be able to join its group. Groups can be created and destroyed dynamically at runtime in application code. The example below adds the subscriber group durableSubscription to the exampleTopic mapping. The subscriber-groups element can contain multiple subscriber-group elements to add as many groups as the application requires. <markup lang=\"xml\" title=\"Sample Durable Subscriber Group\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;durableSubscription&lt;/name&gt; &lt;/subscriber-group&gt; &lt;subscriber-groups&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Subscriber Group"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " A topic mapping maps a topic name to a paged topic scheme definition.The mappings provide a level of separation between applications and the underlying topic definitions. The separation allows topic implementations to be changed as required without having to change application code. Topic mappings have optional initialization parameters that are applied to the underlying paged topic scheme definition. Topic mappings are defined using a &lt;topic-mapping&gt; element within the &lt;topic-scheme-mapping&gt; node. Any number of topic mappings can be created. The topic mapping must include the topic name, and the scheme name to which the topic name is mapped. Using Exact Topic Mappings An exact topic mapping maps a specific topic name to a topic scheme definition. An application must provide the exact name as specified in the mapping to use a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below creates a single topic mapping that maps the topic name exampleTopic to a paged-topic-scheme definition with the scheme name `example-topic-scheme . <markup lang=\"xml\" title=\"Sample Exact Topic Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;example-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Using Named Pattern Topic Mappings Name pattern topic mappings allow applications to use patterns when specifying a topic name. Patterns use the asterisk ( * ) wildcard. Name patterns alleviate an application from having to know the exact name of a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below a topic mappings using the wildcard ( \\* ) to map any topic name with the prefix account- to a paged-topic-scheme definition with the scheme name account-topic-scheme . <markup lang=\"xml\" title=\"Sample Topic Name Pattern Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;account-*&lt;/topic-name&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;AccountDistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Subscriber Group A topic can have zero, one, or more durable subscriber groups defined in the topic-mapping for the topic. The subscriber group(s) are created along with the topic and are therefore ensured to exist before any data is published to the topic. A subscriber group does not have to be defined on a topic’s topic-mapping for a subscriber to be able to join its group. Groups can be created and destroyed dynamically at runtime in application code. The example below adds the subscriber group durableSubscription to the exampleTopic mapping. The subscriber-groups element can contain multiple subscriber-group elements to add as many groups as the application requires. <markup lang=\"xml\" title=\"Sample Durable Subscriber Group\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;durableSubscription&lt;/name&gt; &lt;/subscriber-group&gt; &lt;subscriber-groups&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Defining Topic Mappings"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The &lt;paged-topic-scheme&gt; element is used to define distributed topics. A distributed topic utilizes a distributed (partitioned) topic service instance. Any number of distributed topics can be defined in a cache configuration file. The example below defines a basic distributed topic that uses distributed-topic as the scheme name and is mapped to the topic name example-topic . The &lt;autostart&gt; element is set to true to start the service on a cache server node. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;example-topic&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A paged-topic-scheme has various configuration elements, discussed further below. ",
            "title": "Sample Distributed Topic Definition"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. ",
            "title": "Very Small Channel Count"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. ",
            "title": "Vary Large Channel Count"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Channels are used by topics both as a way to increase parallel processing of messages and also to retain published ordering. The number of channels in a topic is configurable using the &lt;channel-count&gt; sub-element of the &lt;paged-topic-scheme&gt; . The default number of channels is based on the partition count of the underlying cache service used by the topic. With the Coherence default partition count of 257 giving a default topic channel count of 17. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;channel-count&gt;3&lt;/channel-count&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to have a channel count of 3 . Whether increasing or decreasing the channel count makes sense depends on how an application will publish messages, and the ordering guarantees required. To help show the pros and cons we&#8217;ll look at both the very small and the very big. Very Small Channel Count The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. Vary Large Channel Count Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. ",
            "title": "Channel Count"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " By default, messages in a topic are removed after all the currently connected anonymous subscribers and all the subscriber groups have processed and committed a message. This means a message can only be read once. When an anonymous subscriber connects to a topic it starts receiving messages with the next message published to the topic after it connects. The subscribers in a new subscriber group will also receive messages that were published after the group was created. Sometimes it is desirable to have a topic behave more like a persistent log structure, where a subscriber, or subscriber group, in an application can receive the ordered messages from the log, then go back and re-read them as required. The &lt;retain-consumed&gt; sub-element of the &lt;paged-topic-scheme&gt; element controls this behaviour. The &lt;retain-consumed&gt; sub-element&#8217;s value is a boolean, true to retain elements, false to remove consumed elements. In topics configured with &lt;retain-consumed&gt; set to true , new anonymous subscribers will start to receive messages from the beginning (head) of the topic, rather than the tail; new subscriber groups will also start from the head of the topic. Messages in a retained topic are never deleted (unless the topic is also configured with expiry). <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-consumed&gt;true&lt;/retain-consumed&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to retain messages. Topics configuration, like cache configuration, supports parameterizing certain configuration elements on a per-topic basis using parameter macros. For example, the configuration below has tow topic mappings, topic-one and topic-two . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-one&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;keep-messages&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-two&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-values&gt;{keep-messages false}&lt;/retain-values&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The topic-one mapping contains an init-param named keep-messages with a value of true The topic-two mapping contains no init-params The topic scheme contains the retain-values sub-element, but instead of a simple boolean value it uses a macro (a value inside curly brackets). The {keep-messages false} macro says to use the keep-messages parameter for the value of the retain-values sub-element, and default to false if keep-messages is not set. So, topic-one , which sets keep-values to true will use a configuration that retains messages, whereas topic-two has no init-params so keep-values will not be set and will default to `false. ",
            "title": "Retaining Messages (Topics as Persistent Logs)"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The &lt;serializer&gt; sub-element of &lt;paged-topic-scheme&gt; element enables specifying predefined serializers pof or java (default), or a custom Serializer implementation. The serializer is used to serialize and deserialize the message payload. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;serializer&gt;pof&lt;/serializer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets the serializer for all topics mapped to the distributed-topic scheme to POF. ",
            "title": "Topic Values Serializer"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The subscriber-timeout sub-element configures the maximum amount of time that can elapse after a subscribers that is part of a subscriber group polls for messages before that subscriber is considered dead. Each time a subscriber in a group calls on of the receive methods it sends a heartbeat to the server (heartbeats can also be sent manually by application code during long-running processing). If the server does not receive a heartbeat within the timeout the subscriber is considered dead and any channels that it owned will be redistributed to any remaining subscribers. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;subscriber-timeout&gt;1m&lt;/subscriber-timeout&gt; &lt;/paged-topic-scheme&gt; The subscriber timeout has been set to 1 minute. The purpose of timing out subscribers is to stop channels being starved of subscriptions due to badly behaved, dead, or deadlocked subscribers. If a dead subscriber stayed connected and its channels were not redistributed, any messages published to those channels would never be processed. A timed-out subscriber is not closed, application code that calls receive on a timed-ot subscriber will cause that subscriber to reconnect and be re-initialised with new channel ownership. The default value for the subscriber timeout is five minutes. This should be sufficient for most applications unless the message processing code takes a very long time, for example it talks to other external slow system. ",
            "title": "Subscriber Timeout"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The &lt;storage&gt; sub-element allows specification of on-heap , ramjournal and flashjournal to store the messages and metadata for a topic. The default is to use on-heap storage. The ramjournal and flashjournal options use the Elastic Data Feature to, which is a commercial only Coherence feature. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;on-heap&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to on-heap , so topic data is stored in memory. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;flashjournal&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to flashjournal , so topic data is stored on disc, using Coherence&#8217;s commercial Elastic Data feature. ",
            "title": "Storage Options for Topic Values"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " A paged topic scheme configures a topic that stores data in pages. This is how Coherence scales topic data across the cluster, by distributing pages across storage enabled members of the cluster. Each channel has pages and each page holds a number of messages. The page size can be configured to determine how many messages can fit into a page. Publishers publish messages to the tail page in a channel, and when that page is full the page is sealed, and the next page becomes the tail. Page size is configured in the &lt;page-size&gt; element of the &lt;paged-topic-scheme&gt; in the cache configuration file. The format of the page size value is a positive integer, optionally followed by a byte size suffix (B, KB, MB). When the page size is configured with a value using a byte size suffix, the size of the serialized message payload is used to determine the page size. In this case different pages may contain different numbers of messages if the serialized message size is different. In the example below, the page size is set to 10 mega-bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;10MB&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; If a page size element has a value without a byte size suffix it is treated as a number of messages rather than a binary size. For example, using the configuration below, each page will have a maximum size of 100 messages, regardless of the message&#8217;s size in bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;100&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; Pages are used by subscribers as a way to batch receive calls. When receive is called on a subscriber, the server can return a batch of messages, upto a whole page, in the response. The subscriber then stores those messages locally and uses them to respond to further receive calls without needing to make a remote request back to the page. The default maximum batch size used in responses is the minimum of the page size, and the MTU of the network interface being used by Coherence, so that a batch fits into a network packet. As with many configuration elements, there are pros and cons to making the value too small or too large. A page that is too large will store a lot of data in a single page on a single storage member before moving to the next page, causing lumpy data distribution. Pages that are too small will cause an excessive number of pages wasting storage where the data structures created per page may exceed the storage used by the messages themselves. The default page size is 1Mb. Page numbers are stored in a Java long value, so an application is unlikely to consume all 9,223,372,036,854,775,807 pages. Using the default page size of 1Mb, that is 9 peta-bytes of messages. Due to the way publishing works, a full page usually slightly exceeds the configured page size. This is because during publishing, messages are accepted into a page until the pages size is &gt;= the configured maximum. For example, if a page has been configured with a maximum size of 1Mb and currently has 900kb of messages, so is below the maximum size. If the next message published that is 150kb, that message will still be accepted, as the page has some free space, pushing the total size to 1.5Mb. The page would then be considered sealed and accept no more messages, the next message published would go to the next page. ",
            "title": "Page Size"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Adding a &lt;high-units&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the storage size of the values retained for the topic. The topic is considered full if this storage limit is reached. Not exceeding this high water-mark is managed by using flow control. When subscriber(s) are lagging in processing outstanding values retained on the topic, the publishers are throttled until there is space available. See Managing the Publisher Flow Control to Place Upper Bound on Topics Storage. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;high-units&gt;100MB&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The &lt;high-units&gt; has been set to 100Mb so the total size of the topic will not exceed 100 mega-bytes. ",
            "title": "Size Limited Topics"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Adding a &lt;expiry-delay&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the length of time that the published messages live in a topic, waiting to be received by a Subscriber. Once the expiry delay is past, those expired messages will never be received by subscribers. The default expiry-delay is zero, meaning elements never expire. Messages will be expired regardless of the type of subscriber used. For example, even with a durable subscriber group, expired messaged will not be received after their expiry delay has passed. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;expiry-delay&gt;30d&lt;/expiry-delay&gt; &lt;/paged-topic-scheme&gt; The &lt;expiry-delay&gt; has been set to 30 days, so messages will be removed from the topic 30 days after publishing, regardless of whether they have been received by subscribers. ",
            "title": "Topics with Expiring Values"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Topic schemes are used to define the topic services that are available to an application.Topic schemes provide a declarative mechanism that allows topics to be defined independent of the applications that use them. This removes the responsibility of defining topics from the application and allows topics to change without having to change an application&#8217;s code. Topic schemes also promote topic definition reuse by allowing many applications to use the same topic definition. Topic schemes are defined within the &lt;caching-schemes&gt; element. A &lt;paged-topic-scheme&gt; scheme element and its properties are used to define a topic of that type. Sample Distributed Topic Definition The &lt;paged-topic-scheme&gt; element is used to define distributed topics. A distributed topic utilizes a distributed (partitioned) topic service instance. Any number of distributed topics can be defined in a cache configuration file. The example below defines a basic distributed topic that uses distributed-topic as the scheme name and is mapped to the topic name example-topic . The &lt;autostart&gt; element is set to true to start the service on a cache server node. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;example-topic&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A paged-topic-scheme has various configuration elements, discussed further below. Channel Count Channels are used by topics both as a way to increase parallel processing of messages and also to retain published ordering. The number of channels in a topic is configurable using the &lt;channel-count&gt; sub-element of the &lt;paged-topic-scheme&gt; . The default number of channels is based on the partition count of the underlying cache service used by the topic. With the Coherence default partition count of 257 giving a default topic channel count of 17. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;channel-count&gt;3&lt;/channel-count&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to have a channel count of 3 . Whether increasing or decreasing the channel count makes sense depends on how an application will publish messages, and the ordering guarantees required. To help show the pros and cons we&#8217;ll look at both the very small and the very big. Very Small Channel Count The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. Vary Large Channel Count Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. Retaining Messages (Topics as Persistent Logs) By default, messages in a topic are removed after all the currently connected anonymous subscribers and all the subscriber groups have processed and committed a message. This means a message can only be read once. When an anonymous subscriber connects to a topic it starts receiving messages with the next message published to the topic after it connects. The subscribers in a new subscriber group will also receive messages that were published after the group was created. Sometimes it is desirable to have a topic behave more like a persistent log structure, where a subscriber, or subscriber group, in an application can receive the ordered messages from the log, then go back and re-read them as required. The &lt;retain-consumed&gt; sub-element of the &lt;paged-topic-scheme&gt; element controls this behaviour. The &lt;retain-consumed&gt; sub-element&#8217;s value is a boolean, true to retain elements, false to remove consumed elements. In topics configured with &lt;retain-consumed&gt; set to true , new anonymous subscribers will start to receive messages from the beginning (head) of the topic, rather than the tail; new subscriber groups will also start from the head of the topic. Messages in a retained topic are never deleted (unless the topic is also configured with expiry). <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-consumed&gt;true&lt;/retain-consumed&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to retain messages. Topics configuration, like cache configuration, supports parameterizing certain configuration elements on a per-topic basis using parameter macros. For example, the configuration below has tow topic mappings, topic-one and topic-two . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-one&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;keep-messages&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-two&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-values&gt;{keep-messages false}&lt;/retain-values&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The topic-one mapping contains an init-param named keep-messages with a value of true The topic-two mapping contains no init-params The topic scheme contains the retain-values sub-element, but instead of a simple boolean value it uses a macro (a value inside curly brackets). The {keep-messages false} macro says to use the keep-messages parameter for the value of the retain-values sub-element, and default to false if keep-messages is not set. So, topic-one , which sets keep-values to true will use a configuration that retains messages, whereas topic-two has no init-params so keep-values will not be set and will default to `false. Topic Values Serializer The &lt;serializer&gt; sub-element of &lt;paged-topic-scheme&gt; element enables specifying predefined serializers pof or java (default), or a custom Serializer implementation. The serializer is used to serialize and deserialize the message payload. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;serializer&gt;pof&lt;/serializer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets the serializer for all topics mapped to the distributed-topic scheme to POF. Subscriber Timeout The subscriber-timeout sub-element configures the maximum amount of time that can elapse after a subscribers that is part of a subscriber group polls for messages before that subscriber is considered dead. Each time a subscriber in a group calls on of the receive methods it sends a heartbeat to the server (heartbeats can also be sent manually by application code during long-running processing). If the server does not receive a heartbeat within the timeout the subscriber is considered dead and any channels that it owned will be redistributed to any remaining subscribers. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;subscriber-timeout&gt;1m&lt;/subscriber-timeout&gt; &lt;/paged-topic-scheme&gt; The subscriber timeout has been set to 1 minute. The purpose of timing out subscribers is to stop channels being starved of subscriptions due to badly behaved, dead, or deadlocked subscribers. If a dead subscriber stayed connected and its channels were not redistributed, any messages published to those channels would never be processed. A timed-out subscriber is not closed, application code that calls receive on a timed-ot subscriber will cause that subscriber to reconnect and be re-initialised with new channel ownership. The default value for the subscriber timeout is five minutes. This should be sufficient for most applications unless the message processing code takes a very long time, for example it talks to other external slow system. Storage Options for Topic Values The &lt;storage&gt; sub-element allows specification of on-heap , ramjournal and flashjournal to store the messages and metadata for a topic. The default is to use on-heap storage. The ramjournal and flashjournal options use the Elastic Data Feature to, which is a commercial only Coherence feature. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;on-heap&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to on-heap , so topic data is stored in memory. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;flashjournal&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to flashjournal , so topic data is stored on disc, using Coherence&#8217;s commercial Elastic Data feature. Page Size A paged topic scheme configures a topic that stores data in pages. This is how Coherence scales topic data across the cluster, by distributing pages across storage enabled members of the cluster. Each channel has pages and each page holds a number of messages. The page size can be configured to determine how many messages can fit into a page. Publishers publish messages to the tail page in a channel, and when that page is full the page is sealed, and the next page becomes the tail. Page size is configured in the &lt;page-size&gt; element of the &lt;paged-topic-scheme&gt; in the cache configuration file. The format of the page size value is a positive integer, optionally followed by a byte size suffix (B, KB, MB). When the page size is configured with a value using a byte size suffix, the size of the serialized message payload is used to determine the page size. In this case different pages may contain different numbers of messages if the serialized message size is different. In the example below, the page size is set to 10 mega-bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;10MB&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; If a page size element has a value without a byte size suffix it is treated as a number of messages rather than a binary size. For example, using the configuration below, each page will have a maximum size of 100 messages, regardless of the message&#8217;s size in bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;100&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; Pages are used by subscribers as a way to batch receive calls. When receive is called on a subscriber, the server can return a batch of messages, upto a whole page, in the response. The subscriber then stores those messages locally and uses them to respond to further receive calls without needing to make a remote request back to the page. The default maximum batch size used in responses is the minimum of the page size, and the MTU of the network interface being used by Coherence, so that a batch fits into a network packet. As with many configuration elements, there are pros and cons to making the value too small or too large. A page that is too large will store a lot of data in a single page on a single storage member before moving to the next page, causing lumpy data distribution. Pages that are too small will cause an excessive number of pages wasting storage where the data structures created per page may exceed the storage used by the messages themselves. The default page size is 1Mb. Page numbers are stored in a Java long value, so an application is unlikely to consume all 9,223,372,036,854,775,807 pages. Using the default page size of 1Mb, that is 9 peta-bytes of messages. Due to the way publishing works, a full page usually slightly exceeds the configured page size. This is because during publishing, messages are accepted into a page until the pages size is &gt;= the configured maximum. For example, if a page has been configured with a maximum size of 1Mb and currently has 900kb of messages, so is below the maximum size. If the next message published that is 150kb, that message will still be accepted, as the page has some free space, pushing the total size to 1.5Mb. The page would then be considered sealed and accept no more messages, the next message published would go to the next page. Size Limited Topics Adding a &lt;high-units&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the storage size of the values retained for the topic. The topic is considered full if this storage limit is reached. Not exceeding this high water-mark is managed by using flow control. When subscriber(s) are lagging in processing outstanding values retained on the topic, the publishers are throttled until there is space available. See Managing the Publisher Flow Control to Place Upper Bound on Topics Storage. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;high-units&gt;100MB&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The &lt;high-units&gt; has been set to 100Mb so the total size of the topic will not exceed 100 mega-bytes. Topics with Expiring Values Adding a &lt;expiry-delay&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the length of time that the published messages live in a topic, waiting to be received by a Subscriber. Once the expiry delay is past, those expired messages will never be received by subscribers. The default expiry-delay is zero, meaning elements never expire. Messages will be expired regardless of the type of subscriber used. For example, even with a durable subscriber group, expired messaged will not be received after their expiry delay has passed. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;expiry-delay&gt;30d&lt;/expiry-delay&gt; &lt;/paged-topic-scheme&gt; The &lt;expiry-delay&gt; has been set to 30 days, so messages will be removed from the topic 30 days after publishing, regardless of whether they have been received by subscribers. ",
            "title": "Defining a Distributed Topic Scheme"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Coherence topics are configured in the cache configuration file. This section includes the following topics: Defining Topic Mappings - A topic mapping maps a topic name to a paged topic scheme definition. Define Subscriber Groups - Subscriber groups can be defined in topic mappings Defining a Distributed Topic Scheme - Topic schemes are used to define the topic services that are available to an application. Channel Count - configure the number of channels in a topic Retaining Messages (Topics as Persistent Logs) - retain messages after consumption (rewindable topics) Configure the message Serializer Configure the Subscriber Timeout - configure the group subscriber timout Storage Options for Topic Values Page Size - configure the size of a page in a paged topic Size Limited Topics Expiring Messages - expiring messages Defining Topic Mappings A topic mapping maps a topic name to a paged topic scheme definition.The mappings provide a level of separation between applications and the underlying topic definitions. The separation allows topic implementations to be changed as required without having to change application code. Topic mappings have optional initialization parameters that are applied to the underlying paged topic scheme definition. Topic mappings are defined using a &lt;topic-mapping&gt; element within the &lt;topic-scheme-mapping&gt; node. Any number of topic mappings can be created. The topic mapping must include the topic name, and the scheme name to which the topic name is mapped. Using Exact Topic Mappings An exact topic mapping maps a specific topic name to a topic scheme definition. An application must provide the exact name as specified in the mapping to use a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below creates a single topic mapping that maps the topic name exampleTopic to a paged-topic-scheme definition with the scheme name `example-topic-scheme . <markup lang=\"xml\" title=\"Sample Exact Topic Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;example-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Using Named Pattern Topic Mappings Name pattern topic mappings allow applications to use patterns when specifying a topic name. Patterns use the asterisk ( * ) wildcard. Name patterns alleviate an application from having to know the exact name of a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below a topic mappings using the wildcard ( \\* ) to map any topic name with the prefix account- to a paged-topic-scheme definition with the scheme name account-topic-scheme . <markup lang=\"xml\" title=\"Sample Topic Name Pattern Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;account-*&lt;/topic-name&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;AccountDistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Subscriber Group A topic can have zero, one, or more durable subscriber groups defined in the topic-mapping for the topic. The subscriber group(s) are created along with the topic and are therefore ensured to exist before any data is published to the topic. A subscriber group does not have to be defined on a topic’s topic-mapping for a subscriber to be able to join its group. Groups can be created and destroyed dynamically at runtime in application code. The example below adds the subscriber group durableSubscription to the exampleTopic mapping. The subscriber-groups element can contain multiple subscriber-group elements to add as many groups as the application requires. <markup lang=\"xml\" title=\"Sample Durable Subscriber Group\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;durableSubscription&lt;/name&gt; &lt;/subscriber-group&gt; &lt;subscriber-groups&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Defining a Distributed Topic Scheme Topic schemes are used to define the topic services that are available to an application.Topic schemes provide a declarative mechanism that allows topics to be defined independent of the applications that use them. This removes the responsibility of defining topics from the application and allows topics to change without having to change an application&#8217;s code. Topic schemes also promote topic definition reuse by allowing many applications to use the same topic definition. Topic schemes are defined within the &lt;caching-schemes&gt; element. A &lt;paged-topic-scheme&gt; scheme element and its properties are used to define a topic of that type. Sample Distributed Topic Definition The &lt;paged-topic-scheme&gt; element is used to define distributed topics. A distributed topic utilizes a distributed (partitioned) topic service instance. Any number of distributed topics can be defined in a cache configuration file. The example below defines a basic distributed topic that uses distributed-topic as the scheme name and is mapped to the topic name example-topic . The &lt;autostart&gt; element is set to true to start the service on a cache server node. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;example-topic&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A paged-topic-scheme has various configuration elements, discussed further below. Channel Count Channels are used by topics both as a way to increase parallel processing of messages and also to retain published ordering. The number of channels in a topic is configurable using the &lt;channel-count&gt; sub-element of the &lt;paged-topic-scheme&gt; . The default number of channels is based on the partition count of the underlying cache service used by the topic. With the Coherence default partition count of 257 giving a default topic channel count of 17. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;channel-count&gt;3&lt;/channel-count&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to have a channel count of 3 . Whether increasing or decreasing the channel count makes sense depends on how an application will publish messages, and the ordering guarantees required. To help show the pros and cons we&#8217;ll look at both the very small and the very big. Very Small Channel Count The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. Vary Large Channel Count Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. Retaining Messages (Topics as Persistent Logs) By default, messages in a topic are removed after all the currently connected anonymous subscribers and all the subscriber groups have processed and committed a message. This means a message can only be read once. When an anonymous subscriber connects to a topic it starts receiving messages with the next message published to the topic after it connects. The subscribers in a new subscriber group will also receive messages that were published after the group was created. Sometimes it is desirable to have a topic behave more like a persistent log structure, where a subscriber, or subscriber group, in an application can receive the ordered messages from the log, then go back and re-read them as required. The &lt;retain-consumed&gt; sub-element of the &lt;paged-topic-scheme&gt; element controls this behaviour. The &lt;retain-consumed&gt; sub-element&#8217;s value is a boolean, true to retain elements, false to remove consumed elements. In topics configured with &lt;retain-consumed&gt; set to true , new anonymous subscribers will start to receive messages from the beginning (head) of the topic, rather than the tail; new subscriber groups will also start from the head of the topic. Messages in a retained topic are never deleted (unless the topic is also configured with expiry). <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-consumed&gt;true&lt;/retain-consumed&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to retain messages. Topics configuration, like cache configuration, supports parameterizing certain configuration elements on a per-topic basis using parameter macros. For example, the configuration below has tow topic mappings, topic-one and topic-two . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-one&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;keep-messages&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-two&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-values&gt;{keep-messages false}&lt;/retain-values&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The topic-one mapping contains an init-param named keep-messages with a value of true The topic-two mapping contains no init-params The topic scheme contains the retain-values sub-element, but instead of a simple boolean value it uses a macro (a value inside curly brackets). The {keep-messages false} macro says to use the keep-messages parameter for the value of the retain-values sub-element, and default to false if keep-messages is not set. So, topic-one , which sets keep-values to true will use a configuration that retains messages, whereas topic-two has no init-params so keep-values will not be set and will default to `false. Topic Values Serializer The &lt;serializer&gt; sub-element of &lt;paged-topic-scheme&gt; element enables specifying predefined serializers pof or java (default), or a custom Serializer implementation. The serializer is used to serialize and deserialize the message payload. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;serializer&gt;pof&lt;/serializer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets the serializer for all topics mapped to the distributed-topic scheme to POF. Subscriber Timeout The subscriber-timeout sub-element configures the maximum amount of time that can elapse after a subscribers that is part of a subscriber group polls for messages before that subscriber is considered dead. Each time a subscriber in a group calls on of the receive methods it sends a heartbeat to the server (heartbeats can also be sent manually by application code during long-running processing). If the server does not receive a heartbeat within the timeout the subscriber is considered dead and any channels that it owned will be redistributed to any remaining subscribers. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;subscriber-timeout&gt;1m&lt;/subscriber-timeout&gt; &lt;/paged-topic-scheme&gt; The subscriber timeout has been set to 1 minute. The purpose of timing out subscribers is to stop channels being starved of subscriptions due to badly behaved, dead, or deadlocked subscribers. If a dead subscriber stayed connected and its channels were not redistributed, any messages published to those channels would never be processed. A timed-out subscriber is not closed, application code that calls receive on a timed-ot subscriber will cause that subscriber to reconnect and be re-initialised with new channel ownership. The default value for the subscriber timeout is five minutes. This should be sufficient for most applications unless the message processing code takes a very long time, for example it talks to other external slow system. Storage Options for Topic Values The &lt;storage&gt; sub-element allows specification of on-heap , ramjournal and flashjournal to store the messages and metadata for a topic. The default is to use on-heap storage. The ramjournal and flashjournal options use the Elastic Data Feature to, which is a commercial only Coherence feature. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;on-heap&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to on-heap , so topic data is stored in memory. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;flashjournal&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to flashjournal , so topic data is stored on disc, using Coherence&#8217;s commercial Elastic Data feature. Page Size A paged topic scheme configures a topic that stores data in pages. This is how Coherence scales topic data across the cluster, by distributing pages across storage enabled members of the cluster. Each channel has pages and each page holds a number of messages. The page size can be configured to determine how many messages can fit into a page. Publishers publish messages to the tail page in a channel, and when that page is full the page is sealed, and the next page becomes the tail. Page size is configured in the &lt;page-size&gt; element of the &lt;paged-topic-scheme&gt; in the cache configuration file. The format of the page size value is a positive integer, optionally followed by a byte size suffix (B, KB, MB). When the page size is configured with a value using a byte size suffix, the size of the serialized message payload is used to determine the page size. In this case different pages may contain different numbers of messages if the serialized message size is different. In the example below, the page size is set to 10 mega-bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;10MB&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; If a page size element has a value without a byte size suffix it is treated as a number of messages rather than a binary size. For example, using the configuration below, each page will have a maximum size of 100 messages, regardless of the message&#8217;s size in bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;100&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; Pages are used by subscribers as a way to batch receive calls. When receive is called on a subscriber, the server can return a batch of messages, upto a whole page, in the response. The subscriber then stores those messages locally and uses them to respond to further receive calls without needing to make a remote request back to the page. The default maximum batch size used in responses is the minimum of the page size, and the MTU of the network interface being used by Coherence, so that a batch fits into a network packet. As with many configuration elements, there are pros and cons to making the value too small or too large. A page that is too large will store a lot of data in a single page on a single storage member before moving to the next page, causing lumpy data distribution. Pages that are too small will cause an excessive number of pages wasting storage where the data structures created per page may exceed the storage used by the messages themselves. The default page size is 1Mb. Page numbers are stored in a Java long value, so an application is unlikely to consume all 9,223,372,036,854,775,807 pages. Using the default page size of 1Mb, that is 9 peta-bytes of messages. Due to the way publishing works, a full page usually slightly exceeds the configured page size. This is because during publishing, messages are accepted into a page until the pages size is &gt;= the configured maximum. For example, if a page has been configured with a maximum size of 1Mb and currently has 900kb of messages, so is below the maximum size. If the next message published that is 150kb, that message will still be accepted, as the page has some free space, pushing the total size to 1.5Mb. The page would then be considered sealed and accept no more messages, the next message published would go to the next page. Size Limited Topics Adding a &lt;high-units&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the storage size of the values retained for the topic. The topic is considered full if this storage limit is reached. Not exceeding this high water-mark is managed by using flow control. When subscriber(s) are lagging in processing outstanding values retained on the topic, the publishers are throttled until there is space available. See Managing the Publisher Flow Control to Place Upper Bound on Topics Storage. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;high-units&gt;100MB&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The &lt;high-units&gt; has been set to 100Mb so the total size of the topic will not exceed 100 mega-bytes. Topics with Expiring Values Adding a &lt;expiry-delay&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the length of time that the published messages live in a topic, waiting to be received by a Subscriber. Once the expiry delay is past, those expired messages will never be received by subscribers. The default expiry-delay is zero, meaning elements never expire. Messages will be expired regardless of the type of subscriber used. For example, even with a durable subscriber group, expired messaged will not be received after their expiry delay has passed. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;expiry-delay&gt;30d&lt;/expiry-delay&gt; &lt;/paged-topic-scheme&gt; The &lt;expiry-delay&gt; has been set to 30 days, so messages will be removed from the topic 30 days after publishing, regardless of whether they have been received by subscribers. ",
            "title": "Configure Coherence Topics"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " In this example you will utilize the built-in aggregators such as count , sum , min , average and top on orders and customers maps. You will also use the Aggregators class and its helpers to simplify aggregator usage. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " The data model consists of the following classes in two maps, customers and orders Customer - Represents a customer Order - Represents and order for a customer and contains order lines OrderLine - Represents an individual order line for an order Address - Represents an address for a customer ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " In this example we will show a number of the built-in aggregation functions in action. The full list is: count distinct average max min top sum All the above aggregators can be implemented using static helpers in the Aggregators class, for example Aggregators.count() . The helpers create the right aggregator type based on the type of method reference/extractor that is passed as an argument. They all return the EntryAggregator implementations which allows them to be passed as argument to the aggregate methods below. public default &lt;R&gt; R aggregate(EntryAggregator&lt;? super K, ? super V, R&gt; aggregator) - Aggregate across all entries in a cache public &lt;R&gt; R aggregate(Collection&lt;? extends K&gt; collKeys, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defined by the keys public &lt;R&gt; R aggregate(Filter filter, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defines by the filter The SimpleAggregationExample runs various aggregations using a number of the above functions. Example Details The runExample() method contains the code that exercises the above aggregators. Refer to the inline code comments for explanations of what each aggregator is doing. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Customer&gt; customers = getCustomers(); NamedMap&lt;Integer, Order&gt; orders = getOrders(); // count the customers using the Aggregators helper int customerCount = customers.aggregate(Aggregators.count()); Logger.info(\"Customer Count = \" + customerCount); // count the orders int orderCount = orders.aggregate(Aggregators.count()); Logger.info(\"Order Count = \" + orderCount); // get the total value of all orders - requires index on Order::getOrderTotal to be efficient Double totalOrders = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Total Order Value \" + formatMoney(totalOrders)); // get the average order value across all orders - requires index to be efficient Double averageOrderValue = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Average Order Value \" + formatMoney(averageOrderValue)); // get the minimum order value where then is only 1 order line - requires index on Order::getOrderLineCount to be efficient Double minOrderValue1Line = orders.aggregate(Filters.equal(Order::getOrderLineCount, 1), Aggregators.min(Order::getOrderTotal)); Logger.info(\"Min Order Value for orders with 1 line \" + formatMoney(minOrderValue1Line)); // get the outstanding balances by state - requires index on the full ValueExtractor to be efficient ValueExtractor&lt;Customer, String&gt; officeState = ValueExtractor.of(Customer::getOfficeAddress).andThen(Address::getState); Map&lt;String, BigDecimal&gt; mapOutstandingByState = customers.aggregate( GroupAggregator.createInstance(officeState, Aggregators.sum(Customer::getOutstandingBalance))); mapOutstandingByState.forEach((k, v) -&gt; Logger.info(\"State: \" + k + \", outstanding total is \" + formatMoney(v))); // get the top 5 order totals by value Logger.info(\"Top 5 orders by value\"); Object[] topOrderValues = orders.aggregate(Aggregators.topN(Order::getOrderTotal, 5)); for (Object value : topOrderValues) { Logger.info(formatMoney((Double) value)); } } ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Run the following to load the data and run the example. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Creating 10000 customers &lt;Info&gt; (thread=main, member=1): Creating orders for customers &lt;Info&gt; (thread=main, member=1): Orders created &lt;Info&gt; (thread=main, member=1): Customer Count = 10000 &lt;Info&gt; (thread=main, member=1): Order Count = 29848 &lt;Info&gt; (thread=main, member=1): Total Order Value $89,689,872.00 &lt;Info&gt; (thread=main, member=1): Average Order Value $3,004.89 &lt;Info&gt; (thread=main, member=1): Min Order Value for orders with 1 line $500.08 &lt;Info&gt; (thread=main, member=1): State: QLD, outstanding total is $567,600.00 &lt;Info&gt; (thread=main, member=1): State: WA, outstanding total is $585,800.00 &lt;Info&gt; (thread=main, member=1): State: SA, outstanding total is $561,900.00 &lt;Info&gt; (thread=main, member=1): State: VIC, outstanding total is $556,500.00 &lt;Info&gt; (thread=main, member=1): State: NT, outstanding total is $528,700.00 &lt;Info&gt; (thread=main, member=1): State: ACT, outstanding total is $566,800.00 &lt;Info&gt; (thread=main, member=1): State: TAS, outstanding total is $563,900.00 &lt;Info&gt; (thread=main, member=1): State: NSW, outstanding total is $530,900.00 &lt;Info&gt; (thread=main, member=1): Top 5 orders by value &lt;Info&gt; (thread=main, member=1): $8,304.27 &lt;Info&gt; (thread=main, member=1): $8,273.82 &lt;Info&gt; (thread=main, member=1): $8,229.51 &lt;Info&gt; (thread=main, member=1): $8,197.35 &lt;Info&gt; (thread=main, member=1): $8,194.63 ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " You have seen how to use built-in aggregators which include count , sum , min , average and top on orders and customers maps. You also used the Aggregators class and its helpers to simplify aggregator usage. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " Performing Data Grid Operations ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " This guide walks you through how to use built-in aggregators such as including count, sum, min, average and top which allow you to process data stored in Coherence in parallel. Coherence supports entry aggregators that perform operations against all, or a subset of entries to obtain a single result. This aggregation is carried out in parallel across the cluster and is a map-reduce type of operation which can be performed efficiently across large amounts of data. See the Coherence Documentation for detailed information on Aggregations. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build In this example you will utilize the built-in aggregators such as count , sum , min , average and top on orders and customers maps. You will also use the Aggregators class and its helpers to simplify aggregator usage. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the following classes in two maps, customers and orders Customer - Represents a customer Order - Represents and order for a customer and contains order lines OrderLine - Represents an individual order line for an order Address - Represents an address for a customer Review the Example Code In this example we will show a number of the built-in aggregation functions in action. The full list is: count distinct average max min top sum All the above aggregators can be implemented using static helpers in the Aggregators class, for example Aggregators.count() . The helpers create the right aggregator type based on the type of method reference/extractor that is passed as an argument. They all return the EntryAggregator implementations which allows them to be passed as argument to the aggregate methods below. public default &lt;R&gt; R aggregate(EntryAggregator&lt;? super K, ? super V, R&gt; aggregator) - Aggregate across all entries in a cache public &lt;R&gt; R aggregate(Collection&lt;? extends K&gt; collKeys, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defined by the keys public &lt;R&gt; R aggregate(Filter filter, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defines by the filter The SimpleAggregationExample runs various aggregations using a number of the above functions. Example Details The runExample() method contains the code that exercises the above aggregators. Refer to the inline code comments for explanations of what each aggregator is doing. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Customer&gt; customers = getCustomers(); NamedMap&lt;Integer, Order&gt; orders = getOrders(); // count the customers using the Aggregators helper int customerCount = customers.aggregate(Aggregators.count()); Logger.info(\"Customer Count = \" + customerCount); // count the orders int orderCount = orders.aggregate(Aggregators.count()); Logger.info(\"Order Count = \" + orderCount); // get the total value of all orders - requires index on Order::getOrderTotal to be efficient Double totalOrders = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Total Order Value \" + formatMoney(totalOrders)); // get the average order value across all orders - requires index to be efficient Double averageOrderValue = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Average Order Value \" + formatMoney(averageOrderValue)); // get the minimum order value where then is only 1 order line - requires index on Order::getOrderLineCount to be efficient Double minOrderValue1Line = orders.aggregate(Filters.equal(Order::getOrderLineCount, 1), Aggregators.min(Order::getOrderTotal)); Logger.info(\"Min Order Value for orders with 1 line \" + formatMoney(minOrderValue1Line)); // get the outstanding balances by state - requires index on the full ValueExtractor to be efficient ValueExtractor&lt;Customer, String&gt; officeState = ValueExtractor.of(Customer::getOfficeAddress).andThen(Address::getState); Map&lt;String, BigDecimal&gt; mapOutstandingByState = customers.aggregate( GroupAggregator.createInstance(officeState, Aggregators.sum(Customer::getOutstandingBalance))); mapOutstandingByState.forEach((k, v) -&gt; Logger.info(\"State: \" + k + \", outstanding total is \" + formatMoney(v))); // get the top 5 order totals by value Logger.info(\"Top 5 orders by value\"); Object[] topOrderValues = orders.aggregate(Aggregators.topN(Order::getOrderTotal, 5)); for (Object value : topOrderValues) { Logger.info(formatMoney((Double) value)); } } Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Run the following to load the data and run the example. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Creating 10000 customers &lt;Info&gt; (thread=main, member=1): Creating orders for customers &lt;Info&gt; (thread=main, member=1): Orders created &lt;Info&gt; (thread=main, member=1): Customer Count = 10000 &lt;Info&gt; (thread=main, member=1): Order Count = 29848 &lt;Info&gt; (thread=main, member=1): Total Order Value $89,689,872.00 &lt;Info&gt; (thread=main, member=1): Average Order Value $3,004.89 &lt;Info&gt; (thread=main, member=1): Min Order Value for orders with 1 line $500.08 &lt;Info&gt; (thread=main, member=1): State: QLD, outstanding total is $567,600.00 &lt;Info&gt; (thread=main, member=1): State: WA, outstanding total is $585,800.00 &lt;Info&gt; (thread=main, member=1): State: SA, outstanding total is $561,900.00 &lt;Info&gt; (thread=main, member=1): State: VIC, outstanding total is $556,500.00 &lt;Info&gt; (thread=main, member=1): State: NT, outstanding total is $528,700.00 &lt;Info&gt; (thread=main, member=1): State: ACT, outstanding total is $566,800.00 &lt;Info&gt; (thread=main, member=1): State: TAS, outstanding total is $563,900.00 &lt;Info&gt; (thread=main, member=1): State: NSW, outstanding total is $530,900.00 &lt;Info&gt; (thread=main, member=1): Top 5 orders by value &lt;Info&gt; (thread=main, member=1): $8,304.27 &lt;Info&gt; (thread=main, member=1): $8,273.82 &lt;Info&gt; (thread=main, member=1): $8,229.51 &lt;Info&gt; (thread=main, member=1): $8,197.35 &lt;Info&gt; (thread=main, member=1): $8,194.63 Summary You have seen how to use built-in aggregators which include count , sum , min , average and top on orders and customers maps. You also used the Aggregators class and its helpers to simplify aggregator usage. See Also Performing Data Grid Operations ",
            "title": "Built-In Aggregators"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " The POF Maven Plugin provides automated instrumentation of classes with the @PortableType annotation to generate consistent (and correct) implementations of Evolvable POF serialization methods. It is a far from a trivial exercise to manually write serialization methods that support serializing inheritance hierarchies that support the Evolvable concept. However, with static type analysis these methods can be deterministically generated. This allows developers to focus on business logic rather than implementing boilerplate code for the above-mentioned methods. Please see Portable Types documentation for more information and detailed instructions on Portable Types creation and usage. ",
            "title": "POF Maven Plugin"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " In order to use the POF Maven Plugin, you need to declare it as a plugin dependency in your pom.xml : <markup lang=\"xml\" > &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; An example Person class (below) when processed with the plugin is below results in the bytecode shown below. <markup lang=\"java\" >@PortableType(id=1000) public class Person { public Person() { } public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Generated bytecode: <markup lang=\"bash\" >$ javap Person.class Compiled from \"Person.java\" public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. ",
            "title": "Usage"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " The simplest way to create a Publisher is from the Coherence Session API, by calling the createPublisher method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\"); The code snippet above creates an anonymous Publisher that publishes to String messages to the topic names test-topic . Alternatively, a Publisher can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Publisher&lt;String&gt; publisher = topic.createPublisher(\"test-topic\"); Both the Session.createPublisher() and NamedTopic.createPublisher() methods also take a var-args array of Publisher.Option instances to further configure the behaviour of the publisher.Some of these options are described below. ",
            "title": "Creating Publishers"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Publishers should ideally be closed when application code finishes with them so that any resources associated with them are also closed and cleaned up. Pubishers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); try (Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\")) { // ... publish messages ... } In the above example, the publisher is used to publish messages inside the try/catch block.Once the try/catch block exits, the publisher is closed. When a publisher is closed, it can no longer be used.Calls to publish methods after closing will throw an IllegalStateException . ",
            "title": "Closing a Publisher"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " The default behaviour in Coherence Topics is that messages published by a publisher on a specific JVM thread are received in order they were published.Messages published on different threads, or by different publishers could be received interleaved with each other but always in order from the viewpoint of the publishing thread. The OrderBy.thread() option can be specified explicitly as shown below: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.thread()); ",
            "title": "OrderBy Thread"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " It is possible to publish messages to a channel based on a function that derives a channel from the value being published.This is achieved using the OrderBy.value(ToIntFunction&lt;? super V&gt; orderIdFunction) option, where the orderIdFunction is a java.util.function.ToIntFunction that takes the value being published and returns an int .The publisher channel the value is published to will be derived by modding the returned int with the number of channels available for the topic.For example, if there were 17 channels (0 - 16), and the function returned 8, the message would be published to channel 8, if the function returned 19, the message would be published to 19 % 17 - which is 2. If the int returned is negative, a positive channel number is calculated equal to n % channelCount + channelCount ) For example, suppose a Publisher is publishing Order messages, and we only care about ordering by the orders customerId (which coincidentally is an int ).We could do something like this: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); OrderBy orderBy = OrderBy.value(order -&gt; order.getCustomerId()); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", orderBy); ",
            "title": "OrderBy Value"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Messages can be published to a fixed channel by using the OrderBy.id(int nOrderId) option, where the nOrderId parameter is used to determine the channel.The exact channel is worked out in the same way that it is for the OrderBy.value() option, by modding the int value used in the OrderBy.id() option with the number of channels.For example, if there were 17 channels, and the option used was OrderBy.id(8) then all messages would be published to channel 8, if the OrderBy.id(19) was used all messages would go to channel 19 % 17 , which is channel 2. For example, to publish all messages from a Publisher to channel 5: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.is(5)); Using the OrderBy.id() option with multiple publishers with the same Id value will cause all those publishers to publish to the same channel.This would mean that message ordering would be global across all of those publishers, with the caveat that there would be more contention with publishers against the tail of the channel. ",
            "title": "OrderBy Id"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Finally, using OrderBy.none() will guarantee no ordering, each message would be published to a random channel.This would allow the least contention in use cases where the order of message processing by subscribers did not matter. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.none()); ",
            "title": "OrderBy None"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " It is possible to use the values themselves to determine ordering by making the message value implement the interface com.tangosol.net.topic.Publisher.Orderable .This interface has a single getOrderId() method that returns an int that is used in the same way as other options above to determine the channel to publish to. Publishing Orderable values will override any OrderBy option that might have been specified for the publisher. This is an similar to the OrderBy.value() option, but in this case the code that creates the publisher does not need to know how to determine the order, this can be different for each type of message that the publisher publishes. For example, if there is a Transaction class with a String customer identifier that should be used to order published Transactions , we might implement it like this: <markup lang=\"java\" title=\"Transaction.java\" >public class Transaction implements Publisher.Orderable { private String customerId; @Override public int getOrderId() { return Objects.hashCode(customerId); } } Now we can configure a publisher: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Transaction&gt; publisher = session.createPublisher(\"test-topic\"); The publisher above did not specify an OrderBy option, so the default or OrderBy.thread() will be used, but as the Transaction class implements Publisher.Orderable then it&#8217;s getOrderId method wil be used to determine message ordering. ",
            "title": "Publish Orderable Messages"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " In a lot of use cases it is important that messages are processed by subscribers in a guaranteed order.The publisher can be configured when it is created to use different ordering guarantees by using the Publisher.OrderBy option. The OrderBy options controls which channel (or channels) in a topic a publisher publishes messages to.Subscribers receive messages from a specific channel in the order that they were published. OrderBy Thread The default behaviour in Coherence Topics is that messages published by a publisher on a specific JVM thread are received in order they were published.Messages published on different threads, or by different publishers could be received interleaved with each other but always in order from the viewpoint of the publishing thread. The OrderBy.thread() option can be specified explicitly as shown below: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.thread()); OrderBy Value It is possible to publish messages to a channel based on a function that derives a channel from the value being published.This is achieved using the OrderBy.value(ToIntFunction&lt;? super V&gt; orderIdFunction) option, where the orderIdFunction is a java.util.function.ToIntFunction that takes the value being published and returns an int .The publisher channel the value is published to will be derived by modding the returned int with the number of channels available for the topic.For example, if there were 17 channels (0 - 16), and the function returned 8, the message would be published to channel 8, if the function returned 19, the message would be published to 19 % 17 - which is 2. If the int returned is negative, a positive channel number is calculated equal to n % channelCount + channelCount ) For example, suppose a Publisher is publishing Order messages, and we only care about ordering by the orders customerId (which coincidentally is an int ).We could do something like this: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); OrderBy orderBy = OrderBy.value(order -&gt; order.getCustomerId()); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", orderBy); OrderBy Id Messages can be published to a fixed channel by using the OrderBy.id(int nOrderId) option, where the nOrderId parameter is used to determine the channel.The exact channel is worked out in the same way that it is for the OrderBy.value() option, by modding the int value used in the OrderBy.id() option with the number of channels.For example, if there were 17 channels, and the option used was OrderBy.id(8) then all messages would be published to channel 8, if the OrderBy.id(19) was used all messages would go to channel 19 % 17 , which is channel 2. For example, to publish all messages from a Publisher to channel 5: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.is(5)); Using the OrderBy.id() option with multiple publishers with the same Id value will cause all those publishers to publish to the same channel.This would mean that message ordering would be global across all of those publishers, with the caveat that there would be more contention with publishers against the tail of the channel. OrderBy None Finally, using OrderBy.none() will guarantee no ordering, each message would be published to a random channel.This would allow the least contention in use cases where the order of message processing by subscribers did not matter. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.none()); Publish Orderable Messages It is possible to use the values themselves to determine ordering by making the message value implement the interface com.tangosol.net.topic.Publisher.Orderable .This interface has a single getOrderId() method that returns an int that is used in the same way as other options above to determine the channel to publish to. Publishing Orderable values will override any OrderBy option that might have been specified for the publisher. This is an similar to the OrderBy.value() option, but in this case the code that creates the publisher does not need to know how to determine the order, this can be different for each type of message that the publisher publishes. For example, if there is a Transaction class with a String customer identifier that should be used to order published Transactions , we might implement it like this: <markup lang=\"java\" title=\"Transaction.java\" >public class Transaction implements Publisher.Orderable { private String customerId; @Override public int getOrderId() { return Objects.hashCode(customerId); } } Now we can configure a publisher: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Transaction&gt; publisher = session.createPublisher(\"test-topic\"); The publisher above did not specify an OrderBy option, so the default or OrderBy.thread() will be used, but as the Transaction class implements Publisher.Orderable then it&#8217;s getOrderId method wil be used to determine message ordering. ",
            "title": "Configure Ordering Guarantees"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Publishers are used to publish messages to a Coherence topic, a publisher publishes to a single topic. Creating Publishers Closing a Publisher Configure Ordering Guarantees Ordering by publishing thread Ordering by message value Ordering by fixed channel Orderable messages Creating Publishers The simplest way to create a Publisher is from the Coherence Session API, by calling the createPublisher method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\"); The code snippet above creates an anonymous Publisher that publishes to String messages to the topic names test-topic . Alternatively, a Publisher can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Publisher&lt;String&gt; publisher = topic.createPublisher(\"test-topic\"); Both the Session.createPublisher() and NamedTopic.createPublisher() methods also take a var-args array of Publisher.Option instances to further configure the behaviour of the publisher.Some of these options are described below. Closing a Publisher Publishers should ideally be closed when application code finishes with them so that any resources associated with them are also closed and cleaned up. Pubishers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); try (Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\")) { // ... publish messages ... } In the above example, the publisher is used to publish messages inside the try/catch block.Once the try/catch block exits, the publisher is closed. When a publisher is closed, it can no longer be used.Calls to publish methods after closing will throw an IllegalStateException . Configure Ordering Guarantees In a lot of use cases it is important that messages are processed by subscribers in a guaranteed order.The publisher can be configured when it is created to use different ordering guarantees by using the Publisher.OrderBy option. The OrderBy options controls which channel (or channels) in a topic a publisher publishes messages to.Subscribers receive messages from a specific channel in the order that they were published. OrderBy Thread The default behaviour in Coherence Topics is that messages published by a publisher on a specific JVM thread are received in order they were published.Messages published on different threads, or by different publishers could be received interleaved with each other but always in order from the viewpoint of the publishing thread. The OrderBy.thread() option can be specified explicitly as shown below: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.thread()); OrderBy Value It is possible to publish messages to a channel based on a function that derives a channel from the value being published.This is achieved using the OrderBy.value(ToIntFunction&lt;? super V&gt; orderIdFunction) option, where the orderIdFunction is a java.util.function.ToIntFunction that takes the value being published and returns an int .The publisher channel the value is published to will be derived by modding the returned int with the number of channels available for the topic.For example, if there were 17 channels (0 - 16), and the function returned 8, the message would be published to channel 8, if the function returned 19, the message would be published to 19 % 17 - which is 2. If the int returned is negative, a positive channel number is calculated equal to n % channelCount + channelCount ) For example, suppose a Publisher is publishing Order messages, and we only care about ordering by the orders customerId (which coincidentally is an int ).We could do something like this: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); OrderBy orderBy = OrderBy.value(order -&gt; order.getCustomerId()); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", orderBy); OrderBy Id Messages can be published to a fixed channel by using the OrderBy.id(int nOrderId) option, where the nOrderId parameter is used to determine the channel.The exact channel is worked out in the same way that it is for the OrderBy.value() option, by modding the int value used in the OrderBy.id() option with the number of channels.For example, if there were 17 channels, and the option used was OrderBy.id(8) then all messages would be published to channel 8, if the OrderBy.id(19) was used all messages would go to channel 19 % 17 , which is channel 2. For example, to publish all messages from a Publisher to channel 5: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.is(5)); Using the OrderBy.id() option with multiple publishers with the same Id value will cause all those publishers to publish to the same channel.This would mean that message ordering would be global across all of those publishers, with the caveat that there would be more contention with publishers against the tail of the channel. OrderBy None Finally, using OrderBy.none() will guarantee no ordering, each message would be published to a random channel.This would allow the least contention in use cases where the order of message processing by subscribers did not matter. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.none()); Publish Orderable Messages It is possible to use the values themselves to determine ordering by making the message value implement the interface com.tangosol.net.topic.Publisher.Orderable .This interface has a single getOrderId() method that returns an int that is used in the same way as other options above to determine the channel to publish to. Publishing Orderable values will override any OrderBy option that might have been specified for the publisher. This is an similar to the OrderBy.value() option, but in this case the code that creates the publisher does not need to know how to determine the order, this can be different for each type of message that the publisher publishes. For example, if there is a Transaction class with a String customer identifier that should be used to order published Transactions , we might implement it like this: <markup lang=\"java\" title=\"Transaction.java\" >public class Transaction implements Publisher.Orderable { private String customerId; @Override public int getOrderId() { return Objects.hashCode(customerId); } } Now we can configure a publisher: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Transaction&gt; publisher = session.createPublisher(\"test-topic\"); The publisher above did not specify an OrderBy option, so the default or OrderBy.thread() will be used, but as the Transaction class implements Publisher.Orderable then it&#8217;s getOrderId method wil be used to determine message ordering. ",
            "title": "Publishers"
        },
        {
            "location": "/examples/README",
            "text": " These guides and tutorials are designed to help you be productive as quickly as possible in whatever use-case you are building with Coherence. Coherence has a long history and having been around for twenty years its APIs have evolved over that time. Occasionally there are multiple ways to implement a specific use-case, typically because to remain backwards compatible with older releases, features cannot be removed from the product. For that reason these guides use the latest Coherence versions and best practice and approaches recommended by the Coherence team for that version. explore Simple Guides fa-graduation-cap Tutorials ",
            "title": "Overview"
        },
        {
            "location": "/examples/README",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor it you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Put Get and Remove This guide walks you through basic CRUD put , get , and remove operations on a NamedMap . Built-in Aggregators This guide walks you through how to use built-in aggregators within Coherence. Custom Aggregators This guide walks you through how to create custom aggregators within Coherence. Topics This guide walks you through how to use Topics within Coherence Near Caching This guide walks you through how to use near caching within Coherence Client Events This guide walks you through how to use client events within Coherence Durable Events This guide walks you through how to use durable events within Coherence Cache Stores This guide walks you through how to use and configure Cache Stores Monitoring StatusHA This guide walks you through how to monitor the High Available (HA) Status for Services ",
            "title": "Guides"
        },
        {
            "location": "/examples/README",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. ",
            "title": "Tutorials"
        },
        {
            "location": "/docs/core/09_backup",
            "text": " Coherence partitioned caches maintain primary and backup partitions. The backup partitions allow the application to simultaneously lose backup-count number of members. These backups are kept as 'strong' as possible by ensuring they are on different machines, racks or sites. In addition customers can choose to forgo backup synchronicity for write performance by enabling asynchronous backups. Described below are two features introduced in 21.12 that either capitalize on these backups to improve read throughput and/or latency, or optimize the asynchronous processing to increase write throughput. These feature are called 'Read Locator' and 'Scheduled Backups' respectively. ",
            "title": "Partition Backup Enhancements"
        },
        {
            "location": "/docs/core/09_backup",
            "text": " Prior to this change all Coherence NamedMap requests are serviced by the primary owner of the associated partition(s) (ignoring client side caches, i.e. NearCache / CQC ). The read-locator feature allows for certain requests (currently only NamedMap.get or NamedMap.getAll ) to be targetted to non-primary partition owners (backups) to balance request load or reduce latency. If the application chooses to target a non-primary partition owner there is an implied tolerance for stale reads. This may be possible as the primary (or other backups) process future/in-flight changes while the targeted member that performed the read has not. Coherence now provides an ability for applications to choose the appropriate read-locator for a cache or service via the cache configuration, as highlighted below: <markup lang=\"xml\" > ... &lt;distributed-scheme&gt; &lt;scheme-name&gt;example-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-locator&gt;closest&lt;/read-locator&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; ... The following read-locator values are supported: primary - (default) target the request to the primary only. closest - find the 'closest' owner based on the machine, rack or site information for each member in the partition&#8217;s ownership chain (primary &amp; backups). random - pick a random owner in the partition&#8217;s ownership chain. random-backup - pick a random backup owner in the partition&#8217;s ownership chain. class-scheme - provide your own implementation that receives the ownership chain and returns the member to target ",
            "title": "Read Locator"
        },
        {
            "location": "/docs/core/09_backup",
            "text": " The following distributed scheme contains an example of setting the scheduled backup interval of ten seconds: <markup lang=\"xml\" > ... &lt;distributed-scheme&gt; &lt;scheme-name&gt;example-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;10s&lt;/async-backup&gt; &lt;/distributed-scheme&gt; ... A default system property can also be used, and will take effect on all distributed schemes used, e.g.: <markup lang=\"text\" >-Dcoherence.distributed.asyncbackup=10s ",
            "title": "Example Configuration"
        },
        {
            "location": "/docs/core/09_backup",
            "text": " As mentioned previously, Coherence provides an ability for applications to favor write throughput over coherent backup copies ( async-backup ). This can result in acknowledged write requests being lost if they were not successfully backed up; acknowledgement comes in the form of control being returned when using the synchronous API against a mutating method ( put / invoke ), or receiving a notification of the completion of a write request via the asynchronous API. Internally this still results in n backup messages being created for n write requests, which has a direct impact on write throughput. To improve write throughput Coherence now supports \"Scheduled\" (or periodic) backups, thus allowing the number of backup messages to be &lt; n . The existing async-backup XML element has been augmented to accept more than a simple true|false value and now supports a time-based value. This allows applications to suggest a soft target of how long they are willing to tolerate stale backups. Coherence at runtime may decide to accelerate backup synchronicity, or increase the staleness based on primary write throughput. Note Care must be taken when choosing the backup interval, since there is a potential for losing updates in the event of losing a primary partition owner. All the updates waiting to be sent by that primary will not be reflected when the corresponding backup owner is restored and becomes primary. Example Configuration The following distributed scheme contains an example of setting the scheduled backup interval of ten seconds: <markup lang=\"xml\" > ... &lt;distributed-scheme&gt; &lt;scheme-name&gt;example-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;async-backup&gt;10s&lt;/async-backup&gt; &lt;/distributed-scheme&gt; ... A default system property can also be used, and will take effect on all distributed schemes used, e.g.: <markup lang=\"text\" >-Dcoherence.distributed.asyncbackup=10s ",
            "title": "Scheduled Backups"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Access Metrics Run the Completed Tutorial Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " About 30-45 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. ",
            "title": "Getting Started"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Configure MicroProfile GraphQL"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Follow the Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") @Counted public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Include the @Counted microprofile metrics annotation to count the number of invocations Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@starwars.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders() { return orders.values(); } Include the @Timed microprofile metrics annotation to time the query In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" ",
            "title": "Create Queries to Show Customer and Orders"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed, and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally, add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@starwars.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally, add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... ",
            "title": "Inject Related Objects"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation @Timed public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } Include the @Timed microprofile metrics annotation to time the mutation In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation @Timed public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } Include the @Timed microprofile metrics annotation to time the mutation The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the the {@link Order} was not found */ @Mutation @Timed public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Include the @Timed microprofile metrics annotation to time the mutation Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. ",
            "title": "Add Mutations"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Finally, we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } ",
            "title": "Add a Dynamic Where Clause"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As we can see from the above examples, metrics can be easily enabled for queries and mutations by including the @Counted or @Timed annotations. After running a number of queries and mutations you can access the metrics end point using the following curl command: The base metrics endpoint is http://localhost:7001/metrics , but we have added the /application path to restrict the metrics returned. <markup lang=\"bash\" >curl -H 'Accept: application/json' http://127.0.0.1:7001/metrics/application | jq { \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.addOrderLineToOrder\": { \"count\": 1, \"meanRate\": 0.020786416474669184, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 63082260, \"max\": 63082260, \"mean\": 63082260, \"stddev\": 0, \"p50\": 63082260, \"p75\": 63082260, \"p95\": 63082260, \"p98\": 63082260, \"p99\": 63082260, \"p999\": 63082260 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createCustomer\": { \"count\": 1, \"meanRate\": 0.02078651489493201, \"oneMinRate\": 0.013536188363841833, \"fiveMinRate\": 0.0031973351962583784, \"fifteenMinRate\": 0.001095787094460976, \"min\": 4184923, \"max\": 4184923, \"mean\": 4184923, \"stddev\": 0, \"p50\": 4184923, \"p75\": 4184923, \"p95\": 4184923, \"p98\": 4184923, \"p99\": 4184923, \"p999\": 4184923 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createOrder\": { \"count\": 1, \"meanRate\": 0.020786437087696893, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 5411268, \"max\": 5411268, \"mean\": 5411268, \"stddev\": 0, \"p50\": 5411268, \"p75\": 5411268, \"p95\": 5411268, \"p98\": 5411268, \"p99\": 5411268, \"p999\": 5411268 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getCustomers\": 1, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getOrders\": { \"count\": 3, \"meanRate\": 0.06235852925082789, \"oneMinRate\": 0.04423984338571901, \"fiveMinRate\": 0.009754115099857198, \"fifteenMinRate\": 0.003305709235676515, \"min\": 6507371, \"max\": 47080043, \"mean\": 20945553.135424484, \"stddev\": 19245930.056725293, \"p50\": 7014199, \"p75\": 47080043, \"p95\": 47080043, \"p98\": 47080043, \"p99\": 47080043, \"p999\": 47080043 } } jq has been used to format the JSON output. This can be downloaded from https://stedolan.github.io/jq/download/ or you can format the output with an alternate utility. ",
            "title": "Access Metrics"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Completed Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. ",
            "title": "Summary"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "See Also"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial walks through the steps to enable access to Coherence data from GraphQL using Helidon’s MicroProfile (MP) GraphQL support and Coherence CDI . Table of Contents What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Access Metrics Run the Completed Tutorial Summary See Also What You Will Build You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . What You Need About 30-45 minutes A favorite text editor or IDE JDK 11 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Getting Started This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. Follow the Tutorial Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. Create Queries to Show Customer and Orders Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") @Counted public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Include the @Counted microprofile metrics annotation to count the number of invocations Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@starwars.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders() { return orders.values(); } Include the @Timed microprofile metrics annotation to time the query In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" Inject Related Objects From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed, and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally, add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@starwars.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally, add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... Add Mutations In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation @Timed public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } Include the @Timed microprofile metrics annotation to time the mutation In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation @Timed public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } Include the @Timed microprofile metrics annotation to time the mutation The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the the {@link Order} was not found */ @Mutation @Timed public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Include the @Timed microprofile metrics annotation to time the mutation Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. Add a Dynamic Where Clause Finally, we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } Access Metrics As we can see from the above examples, metrics can be easily enabled for queries and mutations by including the @Counted or @Timed annotations. After running a number of queries and mutations you can access the metrics end point using the following curl command: The base metrics endpoint is http://localhost:7001/metrics , but we have added the /application path to restrict the metrics returned. <markup lang=\"bash\" >curl -H 'Accept: application/json' http://127.0.0.1:7001/metrics/application | jq { \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.addOrderLineToOrder\": { \"count\": 1, \"meanRate\": 0.020786416474669184, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 63082260, \"max\": 63082260, \"mean\": 63082260, \"stddev\": 0, \"p50\": 63082260, \"p75\": 63082260, \"p95\": 63082260, \"p98\": 63082260, \"p99\": 63082260, \"p999\": 63082260 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createCustomer\": { \"count\": 1, \"meanRate\": 0.02078651489493201, \"oneMinRate\": 0.013536188363841833, \"fiveMinRate\": 0.0031973351962583784, \"fifteenMinRate\": 0.001095787094460976, \"min\": 4184923, \"max\": 4184923, \"mean\": 4184923, \"stddev\": 0, \"p50\": 4184923, \"p75\": 4184923, \"p95\": 4184923, \"p98\": 4184923, \"p99\": 4184923, \"p999\": 4184923 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createOrder\": { \"count\": 1, \"meanRate\": 0.020786437087696893, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 5411268, \"max\": 5411268, \"mean\": 5411268, \"stddev\": 0, \"p50\": 5411268, \"p75\": 5411268, \"p95\": 5411268, \"p98\": 5411268, \"p99\": 5411268, \"p999\": 5411268 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getCustomers\": 1, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getOrders\": { \"count\": 3, \"meanRate\": 0.06235852925082789, \"oneMinRate\": 0.04423984338571901, \"fiveMinRate\": 0.009754115099857198, \"fifteenMinRate\": 0.003305709235676515, \"min\": 6507371, \"max\": 47080043, \"mean\": 20945553.135424484, \"stddev\": 19245930.056725293, \"p50\": 7014199, \"p75\": 47080043, \"p95\": 47080043, \"p98\": 47080043, \"p99\": 47080043, \"p999\": 47080043 } } jq has been used to format the JSON output. This can be downloaded from https://stedolan.github.io/jq/download/ or you can format the output with an alternate utility. Run the Completed Tutorial Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } Summary In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. See Also Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "GraphQL"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " In order to use Coherence gRPC client, you need to declare it as a dependency in your pom.xml . There also needs to be a corresponding Coherence server running the gRPC proxy for the client to connect to. <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " The Coherence Java gRPC Client is a library that allows a Java application to connect to a Coherence gRPC proxy server. Usage In order to use Coherence gRPC client, you need to declare it as a dependency in your pom.xml . There also needs to be a corresponding Coherence server running the gRPC proxy for the client to connect to. <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Coherence Java gRPC Client"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " Client gRPC sessions to can be configured using system properties. By default, if no properties are provided a gRPC session named default will be configured to connect to localhost:1408 . For example, the code below will create a gRPC Session named default : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder().build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); ",
            "title": "Obtain a Remote Session"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " Client sessions can be named, by providing a name to the configuration builder: For example, the code below will create a gRPC Session named foo : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder(\"foo\").build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); ",
            "title": "Named gRPC Sessions"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " Client gRPC sessions can be configured using system properties. The system property names follow the format coherence.grpc.channels.&lt;name&gt;.xyz , where &lt;name&gt; is replaced with the session name. For example the property to set the the port to connect to is coherence.grpc.channels.&lt;name&gt;.port , so to configure the port for the default session to 9099, set the property -Dcoherence.grpc.channels.default.port=9099 Property Description coherence.grpc.channels.&lt;name&gt;.host The host name to connect to coherence.grpc.channels.&lt;name&gt;.port The port to connect to coherence.grpc.channels.&lt;name&gt;.target As an alternative to setting the host and port, setting target creates a channel using the ManagedChannelBuilder.forTarget(target); method (see the gRPC Java documentation). (replace &lt;name&gt; with the session name being configured). ",
            "title": "Session Configuration via Properties"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " By default, the Channel used by the gRPC session will be configured as a plain text connection. TLS can be configured by setting the required properties. Property Description coherence.grpc.channels.&lt;name&gt;.credentials Set to one of plaintext , insecure or tls . The default is plaintext and will create an insecure plain text channel. Using insecure will enable TLS but not verify the server certificate (useful in testing). Using tls will enable TLS on the client. coherence.grpc.channels.&lt;name&gt;.tls.ca The location of a CA file if required to verify the server certs. (replace &lt;name&gt; with the session name being configured). If the server has been configured for mutual verification the client&#8217;s key and certificate can also be provided: Property Description coherence.grpc.channels.&lt;name&gt;.tls.cert The location of a client certificate file. coherence.grpc.channels.&lt;name&gt;.tls.key The location of a client key file. coherence.grpc.channels.&lt;name&gt;.tls.password The optional password for the client key file. (replace &lt;name&gt; with the session name being configured). ",
            "title": "Using TLS"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " When client code has finished with a Session it can be closed to free up and close any gRPC requests that are still open by calling the session.close() method. This will also locally release (but not destroy) all Coherence resources manged by that Session . ",
            "title": "Close a Session"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " If a fully custom channel configuration is required application code can configure the session with a Channel . <markup lang=\"java\" >Channel channel = ManagedChannelBuilder.forAddress(\"localhost\", 1408) .usePlaintext() .build(); SessionConfiguration config = GrpcSessionConfiguration.builder(channel) .named(\"foo\") .build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); The example above creates a simple gRPC channel to connect to localhost:1408 . A Session has been created with this channel by specifying the GrpcSessions.channel(channel) option. Calls to Session.create() with the same parameters, in this case channel, will return the same Session instance. Most gRPC Channel implementations do not implement an equals() method, so the same Session will only be returned for the exact same Channel instance. Close a Session When client code has finished with a Session it can be closed to free up and close any gRPC requests that are still open by calling the session.close() method. This will also locally release (but not destroy) all Coherence resources manged by that Session . ",
            "title": "Create a Session with a Custom Channel"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " The Serializer used by the remote session will default to Java serialization, unless the system property coherence.pof.enabled is set to true , in which case POF will be used for the serializer. The serializer for a session can be set specifically when creating a Session . <markup lang=\"java\" >Serializer serializer = new JsonSerializer(); String format = \"json\"; SessionConfiguration config = GrpcSessionConfiguration.builder() .withSerializer(serializer, format) .build(); Session session = Session.create(config); In the example above a json serializer is being used. The GrpcSessions.serializer(ser, format) session option is used to specify the serializer and its format name. The format name will be used by the server to select the correct server side serializer to process the session requests and responses. The serializer format configured must also have a compatible serializer available on the server so that the server can deserialize message payloads. ",
            "title": "Specify a Serializer"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " In most cases a Coherence server only has a single ConfigurableCacheFactory , but it is possible to run multiple and hence multiple different cache services managed by a different ConfigurableCacheFactory . Typically, a scope name will be used to isolate different ConfigurableCacheFactory instances. A gRPC client session can be created for a specific server side scope name by specifying the scope as an option when creating the session. <markup lang=\"java\" >SessionConfiguration config = GrpcSessionConfiguration.builder() .withScopeName(\"foo\") .build(); Session session = Session.create(config); In the example above the GrpcSessions.scope(\"foo\") option is used to specify that the Session created should connect to resources on the server managed by the server side Session with the scope foo . ",
            "title": "Specify a Scope Name"
        },
        {
            "location": "/coherence-java-client/README",
            "text": " The simplest way to access remote Coherence resources, such as a NamedMap when using the gRPC client is via a Coherence Session . Obtain a Remote Session Client gRPC sessions to can be configured using system properties. By default, if no properties are provided a gRPC session named default will be configured to connect to localhost:1408 . For example, the code below will create a gRPC Session named default : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder().build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); Named gRPC Sessions Client sessions can be named, by providing a name to the configuration builder: For example, the code below will create a gRPC Session named foo : <markup lang=\"java\" >import com.oracle.coherence.client.GrpcSessionConfiguration; import com.tangosol.net.Session; import com.tangosol.net.SessionConfiguration; SessionConfiguration config = GrpcSessionConfiguration.builder(\"foo\").build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); Session Configuration via Properties Client gRPC sessions can be configured using system properties. The system property names follow the format coherence.grpc.channels.&lt;name&gt;.xyz , where &lt;name&gt; is replaced with the session name. For example the property to set the the port to connect to is coherence.grpc.channels.&lt;name&gt;.port , so to configure the port for the default session to 9099, set the property -Dcoherence.grpc.channels.default.port=9099 Property Description coherence.grpc.channels.&lt;name&gt;.host The host name to connect to coherence.grpc.channels.&lt;name&gt;.port The port to connect to coherence.grpc.channels.&lt;name&gt;.target As an alternative to setting the host and port, setting target creates a channel using the ManagedChannelBuilder.forTarget(target); method (see the gRPC Java documentation). (replace &lt;name&gt; with the session name being configured). Using TLS By default, the Channel used by the gRPC session will be configured as a plain text connection. TLS can be configured by setting the required properties. Property Description coherence.grpc.channels.&lt;name&gt;.credentials Set to one of plaintext , insecure or tls . The default is plaintext and will create an insecure plain text channel. Using insecure will enable TLS but not verify the server certificate (useful in testing). Using tls will enable TLS on the client. coherence.grpc.channels.&lt;name&gt;.tls.ca The location of a CA file if required to verify the server certs. (replace &lt;name&gt; with the session name being configured). If the server has been configured for mutual verification the client&#8217;s key and certificate can also be provided: Property Description coherence.grpc.channels.&lt;name&gt;.tls.cert The location of a client certificate file. coherence.grpc.channels.&lt;name&gt;.tls.key The location of a client key file. coherence.grpc.channels.&lt;name&gt;.tls.password The optional password for the client key file. (replace &lt;name&gt; with the session name being configured). Create a Session with a Custom Channel If a fully custom channel configuration is required application code can configure the session with a Channel . <markup lang=\"java\" >Channel channel = ManagedChannelBuilder.forAddress(\"localhost\", 1408) .usePlaintext() .build(); SessionConfiguration config = GrpcSessionConfiguration.builder(channel) .named(\"foo\") .build(); Session session = Session.create(config); NamedMap&lt;String, String&gt; map = session.getMap(\"foo\"); The example above creates a simple gRPC channel to connect to localhost:1408 . A Session has been created with this channel by specifying the GrpcSessions.channel(channel) option. Calls to Session.create() with the same parameters, in this case channel, will return the same Session instance. Most gRPC Channel implementations do not implement an equals() method, so the same Session will only be returned for the exact same Channel instance. Close a Session When client code has finished with a Session it can be closed to free up and close any gRPC requests that are still open by calling the session.close() method. This will also locally release (but not destroy) all Coherence resources manged by that Session . Specify a Serializer The Serializer used by the remote session will default to Java serialization, unless the system property coherence.pof.enabled is set to true , in which case POF will be used for the serializer. The serializer for a session can be set specifically when creating a Session . <markup lang=\"java\" >Serializer serializer = new JsonSerializer(); String format = \"json\"; SessionConfiguration config = GrpcSessionConfiguration.builder() .withSerializer(serializer, format) .build(); Session session = Session.create(config); In the example above a json serializer is being used. The GrpcSessions.serializer(ser, format) session option is used to specify the serializer and its format name. The format name will be used by the server to select the correct server side serializer to process the session requests and responses. The serializer format configured must also have a compatible serializer available on the server so that the server can deserialize message payloads. Specify a Scope Name In most cases a Coherence server only has a single ConfigurableCacheFactory , but it is possible to run multiple and hence multiple different cache services managed by a different ConfigurableCacheFactory . Typically, a scope name will be used to isolate different ConfigurableCacheFactory instances. A gRPC client session can be created for a specific server side scope name by specifying the scope as an option when creating the session. <markup lang=\"java\" >SessionConfiguration config = GrpcSessionConfiguration.builder() .withScopeName(\"foo\") .build(); Session session = Session.create(config); In the example above the GrpcSessions.scope(\"foo\") option is used to specify that the Session created should connect to resources on the server managed by the server side Session with the scope foo . ",
            "title": "Access Coherence Resources"
        },
        {
            "location": "/examples/tutorials/000-overview",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. ",
            "title": "Tutorials"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Coherence gRPC provides the protobuf definitions necessary to interact with a Coherence data management services over gRPC. This library also provides utilities for making low-level cache requests over gRPC, converting between gRPC and Coherence binary implementations. Given this, unless there is a plan to develop a Coherence gRPC client in another language or to create new services in Java, there is little need for developers to depend on this library. ",
            "title": "Coherence gRPC"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " In order to use Coherence gRPC, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Proto File Usage services.proto defines the RPCs for interacting with a Coherence data management services requests.proto defines the request/response structs for making requests to and receiving responses from Coherence data management services ",
            "title": "Protobuf Definitions"
        },
        {
            "location": "/docs/core/03_parallel_recovery",
            "text": " Coherence introduced a disk-based persistence feature from version 12.2.1. This feature accommodates for the loss of an entire cluster, and/or simultaneous loss of a primary and backup ensuring data is recovered from disk and made available. This process of making data available is parallel across the cluster with each storage member recovering a fair share of partitions. While this recovery is in parallel across different members/processes, each member uses a single thread to recover. As of version 20.12, Coherence now recovers data in parallel within a member/process as well as in parallel across the cluster. This allows the cluster, and more importantly the associated data, to be made available as quickly as possible. Ultimately the goal is to have recovery be only limited by the throughput and latency of underlying device. Therefore this feature does assume increasing usage of the device (by accessing data in parallel) will provide some benefit and reduce the overall time to recover data. The number of threads Coherence uses can be tweaked by the following JVM argument: coherence.distributed.persistence.recover.threads This argument may be removed in a future release as we work towards deriving an optimal value. The default value is based on the following formula: num-cores * (max-heap-size / machine-memory) + 2 ",
            "title": "Parallel Recovery"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Each of the features above is backed by one or more Coherence caches, possibly with preconfigured interceptors, but for the most part you shouldn&#8217;t care about that: all interaction with lower level Coherence primitives is hidden behind various factory classes that allow you to get the instances of the classes you need. For example, you will use factory methods within Atomics class to get instances of various atomic types, Locks to get lock instances, Latches and Semaphores to get, well, latches and semaphores. ",
            "title": "Factory Classes"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " In many cases the factory classes will allow you to get both the local and the remote instances of various constructs. For example, Locks.localLock will give you an instance of a standard java.util.concurrent.locks.ReentrantLock , while Locks.remoteLock will return an instance of a RemoteLock . In cases where JDK doesn&#8217;t provide a standard interface, which is the case with atomics, latches and semaphores, we&#8217;ve extracted the interface from the existing JDK class, and created a thin wrapper around the corresponding JDK implementation. For example, Coherence Concurrent provides a Semaphore interface, and LocalSemaphore class that wraps java.util.concurrent.Semaphore . The same is true for the CountDownLatch , and all atomic types. The main advantage of using factory classes to construct both the local and the remote instances is that it allows you to name local locks the same way you have to name remote locks: calling Locks.localLock(\"foo\") will always return the same Lock instance, as the Locks class internally caches both the local and the remote instances it created. Of course, in the case of remote locks, every locally cached remote lock instance is ultimately backed by a shared lock instance somewhere in the cluster, which is used to synchronize lock state across the processes. ",
            "title": "Local vs Remote"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent supports both Java serialization and POF out-of-the-box, with Java serialization being the default. If you want to use POF instead, you will need to specify that by setting coherence.concurrent.serializer system property to pof . You will also need to include coherence-concurrent-pof-config.xml into your own POF configuration file, in order to register built-in Coherence Concurrent types. ",
            "title": "Serialization"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent supports both active and on-demand persistence, but just like in the rest of Coherence it is set to on-demand by default. In order to use active persistence you should set coherence.concurrent.persistence.environment system property to default-active , or another persistence environment that has active persistence enabled. ",
            "title": "Persistence"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent module provides distributed implementations of the concurrency primitives from the java.util.concurrent package that you are already familiar with, such as executors, atomics, locks, semaphores and latches. This allows you to implement concurrent applications using the constructs you are already familiar with, but to expand the \"scope\" of concurrency from a single process to potentially hundreds of processes within a Coherence cluster. You can use executors to submit tasks to be executed somewhere in the cluster; you can use locks, latches and semaphores to synchronize execution across many cluster members; you can use atomics to implement global counters across many processes, etc. Please keep in mind that while these features are extremely powerful and allow you to reuse the knowledge you already have, they may have detrimental effect on scalability and/or performance. Whenever you synchronize execution via locks, latches or semaphores, you are introducing a potential bottleneck into the architecture. Whenever you use a distributed atomic to implement a global counter, you are turning very simple operations that take mere nanoseconds locally, such as increment and decrement, into fairly expensive network calls that could take milliseconds (and potentially block even longer under heavy load). So, use these features sparingly. In many cases there is a better, faster and more scalable way to accomplish the same goal using Coherence primitives such as entry processors, aggregators and events, which were designed to perform and scale well in a distributed environment from the get go. Factory Classes Each of the features above is backed by one or more Coherence caches, possibly with preconfigured interceptors, but for the most part you shouldn&#8217;t care about that: all interaction with lower level Coherence primitives is hidden behind various factory classes that allow you to get the instances of the classes you need. For example, you will use factory methods within Atomics class to get instances of various atomic types, Locks to get lock instances, Latches and Semaphores to get, well, latches and semaphores. Local vs Remote In many cases the factory classes will allow you to get both the local and the remote instances of various constructs. For example, Locks.localLock will give you an instance of a standard java.util.concurrent.locks.ReentrantLock , while Locks.remoteLock will return an instance of a RemoteLock . In cases where JDK doesn&#8217;t provide a standard interface, which is the case with atomics, latches and semaphores, we&#8217;ve extracted the interface from the existing JDK class, and created a thin wrapper around the corresponding JDK implementation. For example, Coherence Concurrent provides a Semaphore interface, and LocalSemaphore class that wraps java.util.concurrent.Semaphore . The same is true for the CountDownLatch , and all atomic types. The main advantage of using factory classes to construct both the local and the remote instances is that it allows you to name local locks the same way you have to name remote locks: calling Locks.localLock(\"foo\") will always return the same Lock instance, as the Locks class internally caches both the local and the remote instances it created. Of course, in the case of remote locks, every locally cached remote lock instance is ultimately backed by a shared lock instance somewhere in the cluster, which is used to synchronize lock state across the processes. Serialization Coherence Concurrent supports both Java serialization and POF out-of-the-box, with Java serialization being the default. If you want to use POF instead, you will need to specify that by setting coherence.concurrent.serializer system property to pof . You will also need to include coherence-concurrent-pof-config.xml into your own POF configuration file, in order to register built-in Coherence Concurrent types. Persistence Coherence Concurrent supports both active and on-demand persistence, but just like in the rest of Coherence it is set to on-demand by default. In order to use active persistence you should set coherence.concurrent.persistence.environment system property to default-active , or another persistence environment that has active persistence enabled. ",
            "title": "Distributed Concurrency"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides a facility to dispatch tasks, either a Runnable or Callable to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. ",
            "title": "Overview"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. ",
            "title": "Usage Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. ",
            "title": "Configuration Elements"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; ",
            "title": "Configuration Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread Fixed thread Creates an ExecutorService with a fixed number of threads Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; ",
            "title": "Configuration"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. ",
            "title": "ExecutorMBean Attributes"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. ",
            "title": "Operations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. ",
            "title": "Management"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON ",
            "title": "Management over REST"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Overview Coherence Concurrent provides a facility to dispatch tasks, either a Runnable or Callable to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. Usage Examples By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. Configuration Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread Fixed thread Creates an ExecutorService with a fixed number of threads Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; Management The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. Management over REST Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON CDI Support RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . ",
            "title": "Executors"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. ",
            "title": "Asynchronous Implementations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides distributed implementations of atomic types, such as AtomicInteger , AtomicLong and AtomicReference . It also provides local implementations of the same types. The local implementations are just thin wrappers around existing java.util.concurrent.atomic types, which implement the same interface as their distributed variants, in order to be interchangeable. To create instances of atomic types you need to call the appropriate factory method on the Atomics class: <markup lang=\"java\" >AtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); AtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); AtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); creates a local, in-process instance of named AtomicInteger with an implicit initial value of 0 creates a remote, distributed instance of named AtomicInteger , distinct from the local instance foo , with an implicit initial value of 0 creates a remote, distributed instance of named AtomicLong , with an initial value of 5 Note that the AtomicInteger and AtomicLong types used above are not types from the java.util.concurrent.atomic package they you are familiar with&#8201;&#8212;&#8201;they are actually interfaces defined within com.oracle.coherence.concurrent.atomic package, that both LocalAtomicXyz and RemoteAtomicXyz classes implement, which are the instances that are actually returned by the methods above. That means that the above code could be rewritten as: <markup lang=\"java\" >LocalAtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); RemoteAtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); RemoteAtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); However, we strongly suggest that you use interfaces instead of concrete types, as they make it easy to switch between local and distributed implementations when necessary. Once created, these instances can be used the same way you would use any of the corresponding java.util.concurrent.atomic types: <markup lang=\"java\" >int counter1 = remoteFoo.incrementAndGet(); long counter5 = remoteBar.addAndGet(5L); Asynchronous Implementations The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. CDI Support Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. ",
            "title": "Atomics"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } ",
            "title": "Exclusive Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock() try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock() try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } ",
            "title": "Read/Write Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides distributed implementations of Lock and ReadWriteLock interfaces from the java.util.concurrent.locks package, allowing you to implement lock-based concurrency control across cluster members when necessary. Unlike local JDK implementations, the classes in this package use cluster member/process ID and thread ID to identify lock owner, and store shared lock state within a Coherence NamedMap . However, that also implies that the calls to acquire and release locks are remote, network calls, as they need to update shared state that is likely stored on a different cluster member, which will have an impact on performance of lock and unlock operations. Exclusive Locks A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } Read/Write Locks A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock() try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock() try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } CDI Support You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. ",
            "title": "Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. ",
            "title": "Count Down Latch"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. ",
            "title": "Semaphore"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent also provides distributed implementations of a CountDownLatch and Semaphore classes from java.util.concurrent package, allowing you to implement synchronization of execution across multiple Coherence cluster members as easily as you can implement it within a single process using those two JDK classes. It also provides interfaces for those two concurrency primitives, that both remote and local implementations conform to. Just like with atomics, the local implementations are nothing more than thin wrappers around corresponding JDK classes. Count Down Latch The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. Semaphore The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. CDI Support You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "Latches and Semaphores"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " In order to use Coherence Concurrent features, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-concurrent&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using the features it provides, as the following sections describe. Executors Executors Overview Executors Usage Executors Configuration Executors Configuration Examples Executors Management Executors Management over REST CDI Support for Executors Atomics Non-blocking Atomics CDI Support for Atomics Locks Exclusive Locks Read/Write Locks CDI Support for Locks Latches and Semaphores Count Down Latch Semaphore CDI Support for Latches and Semaphores Executors Overview Coherence Concurrent provides a facility to dispatch tasks, either a Runnable or Callable to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. Usage Examples By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. Configuration Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread Fixed thread Creates an ExecutorService with a fixed number of threads Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; Management The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. Management over REST Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON CDI Support RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . Atomics Coherence Concurrent provides distributed implementations of atomic types, such as AtomicInteger , AtomicLong and AtomicReference . It also provides local implementations of the same types. The local implementations are just thin wrappers around existing java.util.concurrent.atomic types, which implement the same interface as their distributed variants, in order to be interchangeable. To create instances of atomic types you need to call the appropriate factory method on the Atomics class: <markup lang=\"java\" >AtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); AtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); AtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); creates a local, in-process instance of named AtomicInteger with an implicit initial value of 0 creates a remote, distributed instance of named AtomicInteger , distinct from the local instance foo , with an implicit initial value of 0 creates a remote, distributed instance of named AtomicLong , with an initial value of 5 Note that the AtomicInteger and AtomicLong types used above are not types from the java.util.concurrent.atomic package they you are familiar with&#8201;&#8212;&#8201;they are actually interfaces defined within com.oracle.coherence.concurrent.atomic package, that both LocalAtomicXyz and RemoteAtomicXyz classes implement, which are the instances that are actually returned by the methods above. That means that the above code could be rewritten as: <markup lang=\"java\" >LocalAtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); RemoteAtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); RemoteAtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); However, we strongly suggest that you use interfaces instead of concrete types, as they make it easy to switch between local and distributed implementations when necessary. Once created, these instances can be used the same way you would use any of the corresponding java.util.concurrent.atomic types: <markup lang=\"java\" >int counter1 = remoteFoo.incrementAndGet(); long counter5 = remoteBar.addAndGet(5L); Asynchronous Implementations The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. CDI Support Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. Locks Coherence Concurrent provides distributed implementations of Lock and ReadWriteLock interfaces from the java.util.concurrent.locks package, allowing you to implement lock-based concurrency control across cluster members when necessary. Unlike local JDK implementations, the classes in this package use cluster member/process ID and thread ID to identify lock owner, and store shared lock state within a Coherence NamedMap . However, that also implies that the calls to acquire and release locks are remote, network calls, as they need to update shared state that is likely stored on a different cluster member, which will have an impact on performance of lock and unlock operations. Exclusive Locks A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } Read/Write Locks A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock() try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock() try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } CDI Support You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. Latches and Semaphores Coherence Concurrent also provides distributed implementations of a CountDownLatch and Semaphore classes from java.util.concurrent package, allowing you to implement synchronization of execution across multiple Coherence cluster members as easily as you can implement it within a single process using those two JDK classes. It also provides interfaces for those two concurrency primitives, that both remote and local implementations conform to. Just like with atomics, the local implementations are nothing more than thin wrappers around corresponding JDK classes. Count Down Latch The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. Semaphore The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. CDI Support You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "Usage"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example H2 R2DBC Non Blocking Entry Store Example Pluggable Cache Stores Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. ",
            "title": "CacheLoader and CacheStore Interfaces"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). ",
            "title": "Simple CacheLoader"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll ",
            "title": "Simple CacheStore"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Enable Write Behind"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Simple Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One ",
            "title": "File Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSqlDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"Customer\"); reloadCustomersDB(); } @Test public void testHSqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >long start = System.nanoTime(); // issue a get and it will load the existing customer Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, namedMap.size()); assertNotNull(customer); assertEquals(1, customer.getId()); assertEquals(\"Customer 1\", customer.getName()); You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >// issue a get again and it should be quicker start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"no read-through\")); You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >// remove a customer number 1 namedMap.remove(1); // we should have one less customer in the database assertEquals(MAX_CUSTOMERS - 1, getCustomerDBCount()); assertNull(namedMap.get(1)); // customer should not exist in DB assertNull(getCustomerFromDB(1)); Issue a get for another customer and then update the customer details. <markup lang=\"java\" >// Load customer 2 Customer customer2 = namedMap.get(2); assertNotNull(customer2); // update customer 2 with \"New Address\" namedMap.compute(2, (k, v)-&gt;{ v.setAddress(\"New Address\"); return v; }); // customer should have new address in cache and DB assertEquals(\"New Address\", namedMap.get(2).getAddress()); assertEquals(\"New Address\", getCustomerFromDB(2).getAddress()); Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >// add a new customer 1010 namedMap.put(101, new Customer(101, \"Customer Name 101\", \"Customer address 101\", 20000)); assertTrue(namedMap.containsKey(101)); assertEquals(\"Customer address 101\", getCustomerFromDB(101).getAddress()); namedMap.remove(101); assertFalse(namedMap.containsKey(101)); assertNull(getCustomerFromDB(101)); Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >// clean the cache and reset the database namedMap.clear(); reloadCustomersDB(); assertEquals(0, namedMap.size()); // demonstrate loading the cache from the current contents of the DB // this can be done many ways but for this exercise you could fetch all the // customer id' from the DB but as we know there are 1..100 we can pretend we have. Set&lt;Integer&gt; keySet = IntStream.rangeClosed(1, 100).boxed().collect(Collectors.toSet()); namedMap.invokeAll(keySet, new PreloadRequest&lt;&gt;()); // cache should be fully primed assertEquals(MAX_CUSTOMERS, namedMap.size()); ",
            "title": "HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerExpiring\"); reloadCustomersDB(); } @Test public void testHSQLDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue a get for customer 1 and log the time to load <markup lang=\"java\" >// expiry delay is setup to 20s for the cache and refresh ahead is 0.5 which // means that after 10s if the entry is read the old value is returned but after which a // refresh is done which means that subsequents reads will be fast as the new value is already present long start = System.nanoTime(); Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, customer.getId()); Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >// update the database updateCustomerCreditLimitInDB(1, 10000); // sleep for 11 seconds get the cache entry, we should still get the original value Base.sleep(11000L); assertEquals(5000, namedMap.get(1).getCreditLimit()); The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >// wait for another 10 seconds and the refresh-ahead should have completed Base.sleep(10000L); start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"after refresh-ahead\")); Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms ",
            "title": "Refresh Ahead HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreWriteBehindTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerWriteBehind\"); } @Test public void testHsqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain no customers assertEquals(0, getCustomerDBCount()); Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >// add 10 customers Map&lt;Integer, Customer&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100; i++) { map.put(i, new Customer(i, \"Name \" + i, \"Address \" + i, i * 1000)); } namedMap.putAll(map); // initial check of the database should return 0 as we have write-delay set assertEquals(0, getCustomerDBCount()); Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >// sleep for 15 seconds and the database should be populated as write-delay has elapsed Base.sleep(15000L); // Issuing Eventually assertThat in case of heavily loaded machine Eventually.assertThat(invoking(this).getCustomerDBCount(), is(100)); You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries ",
            "title": "Write Behind HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this H2 R2DBC cache store example, we use the H2Person cache which implements the NonBlockingEntryStore for non-blocking APIs and access to entries in their serialized ( BinaryEntry ) form. Review the Cache Configuration The h2r2dbc-entry-store-cache-config.xml below shows the H2Person cache specifying the class name of the NonBlockingEntryStore implementation. <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;H2Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.H2R2DBCEntryStore&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Unit Test Next we will run the H2R2DBCEntryStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { createTable(); startupCoherence(\"h2r2dbc-entry-store-cache-config.xml\"); } /** * Performs some cache manipulations. */ @Test public void testNonBlockingEntryStore() { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"H2Person\", TypeAssertion.withTypes(Long.class, Person.class)); Person person1 = namedMap.get(Long.valueOf(101)); assertEquals(\"Robert\", person1.getFirstname()); Insert 1 person using a put operation and confirm the data is in the cache. <markup lang=\"java\" >Person person2 = new Person(Long.valueOf(102), 40, \"Tony\", \"Soprano\"); namedMap.put(Long.valueOf(102), person2); Person person3 = namedMap.get(Long.valueOf(102)); assertEquals(\"Tony\", person3.getFirstname()); Delete a couple records and verify the state of the cache. <markup lang=\"java\" >namedMap.remove(Long.valueOf(101)); namedMap.remove(Long.valueOf(102)); assertEquals(null, namedMap.get(Long.valueOf(101))); assertEquals(null, namedMap.get(Long.valueOf(102))); Insert 10 persons using a putAll operation and confirm the data is in the cache. The actual database operations take place in parallel.s <markup lang=\"java\" >Map&lt;Long, Person&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 10; i++) { map.put(Long.valueOf(i), new Person(Long.valueOf(i), 20 + i, \"firstname\" + i, \"lastname\" + i)); } namedMap.putAll(map); Person person5 = namedMap.get(Long.valueOf(5)); assertEquals(\"firstname5\", person5.getFirstname()); assertEquals(10, namedMap.size()); You should see messages indicating activity on the store side: 2021-06-29 15:01:36.365/5.583 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.495/5.713 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore store 2021-06-29 15:01:36.501/5.720 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.504/5.722 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.507/5.726 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.508/5.727 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.509/5.728 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.512/5.730 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Could not find row for key: 101 2021-06-29 15:01:36.515/5.734 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore storeAll ",
            "title": "H2 R2DBC Non Blocking Entry Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); ",
            "title": "Pluggable Cache Stores"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " You have seen how to use and configure Cache Stores within Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This guide walks you through how to use and configure Cache Stores within Coherence. Coherence supports transparent read/write caching of any data source, including databases, web services, packaged applications and file systems; however, databases are the most common use case. As shorthand, \"database\" is used to describe any back-end data source. Effective caches must support both intensive read-only and read/write operations, and for read/write operations, the cache and database must be kept fully synchronized. To accomplish caching of data sources, Coherence supports Read-Through, Write-Through, Refresh-Ahead and Write-Behind caching. Coherence also supports BinaryEntryStore which provides access to the serialized form of entries for data sources capable of manipulating those. A variant of BinaryEntryStore is the NonBlockingEntryStore which, besides providing access to entries in their BinaryEntry form, integrates with data sources with non-blocking APIs such as R2DBC or Kafka. See the Coherence Documentation for detailed information on Cache Stores. Table of Contents What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example H2 R2DBC Non Blocking Entry Store Example Pluggable Cache Stores Summary See Also What You Will Build This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. CacheLoader and CacheStore Interfaces Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. Simple Cache Store Example Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. File Cache Store Example In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One HSQLDb Cache Store Example In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSqlDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"Customer\"); reloadCustomersDB(); } @Test public void testHSqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >long start = System.nanoTime(); // issue a get and it will load the existing customer Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, namedMap.size()); assertNotNull(customer); assertEquals(1, customer.getId()); assertEquals(\"Customer 1\", customer.getName()); You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >// issue a get again and it should be quicker start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"no read-through\")); You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >// remove a customer number 1 namedMap.remove(1); // we should have one less customer in the database assertEquals(MAX_CUSTOMERS - 1, getCustomerDBCount()); assertNull(namedMap.get(1)); // customer should not exist in DB assertNull(getCustomerFromDB(1)); Issue a get for another customer and then update the customer details. <markup lang=\"java\" >// Load customer 2 Customer customer2 = namedMap.get(2); assertNotNull(customer2); // update customer 2 with \"New Address\" namedMap.compute(2, (k, v)-&gt;{ v.setAddress(\"New Address\"); return v; }); // customer should have new address in cache and DB assertEquals(\"New Address\", namedMap.get(2).getAddress()); assertEquals(\"New Address\", getCustomerFromDB(2).getAddress()); Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >// add a new customer 1010 namedMap.put(101, new Customer(101, \"Customer Name 101\", \"Customer address 101\", 20000)); assertTrue(namedMap.containsKey(101)); assertEquals(\"Customer address 101\", getCustomerFromDB(101).getAddress()); namedMap.remove(101); assertFalse(namedMap.containsKey(101)); assertNull(getCustomerFromDB(101)); Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >// clean the cache and reset the database namedMap.clear(); reloadCustomersDB(); assertEquals(0, namedMap.size()); // demonstrate loading the cache from the current contents of the DB // this can be done many ways but for this exercise you could fetch all the // customer id' from the DB but as we know there are 1..100 we can pretend we have. Set&lt;Integer&gt; keySet = IntStream.rangeClosed(1, 100).boxed().collect(Collectors.toSet()); namedMap.invokeAll(keySet, new PreloadRequest&lt;&gt;()); // cache should be fully primed assertEquals(MAX_CUSTOMERS, namedMap.size()); Refresh Ahead HSQLDb Cache Store Example In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerExpiring\"); reloadCustomersDB(); } @Test public void testHSQLDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue a get for customer 1 and log the time to load <markup lang=\"java\" >// expiry delay is setup to 20s for the cache and refresh ahead is 0.5 which // means that after 10s if the entry is read the old value is returned but after which a // refresh is done which means that subsequents reads will be fast as the new value is already present long start = System.nanoTime(); Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, customer.getId()); Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >// update the database updateCustomerCreditLimitInDB(1, 10000); // sleep for 11 seconds get the cache entry, we should still get the original value Base.sleep(11000L); assertEquals(5000, namedMap.get(1).getCreditLimit()); The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >// wait for another 10 seconds and the refresh-ahead should have completed Base.sleep(10000L); start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"after refresh-ahead\")); Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms Write Behind HSQLDb Cache Store Example In this HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreWriteBehindTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerWriteBehind\"); } @Test public void testHsqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain no customers assertEquals(0, getCustomerDBCount()); Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >// add 10 customers Map&lt;Integer, Customer&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100; i++) { map.put(i, new Customer(i, \"Name \" + i, \"Address \" + i, i * 1000)); } namedMap.putAll(map); // initial check of the database should return 0 as we have write-delay set assertEquals(0, getCustomerDBCount()); Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >// sleep for 15 seconds and the database should be populated as write-delay has elapsed Base.sleep(15000L); // Issuing Eventually assertThat in case of heavily loaded machine Eventually.assertThat(invoking(this).getCustomerDBCount(), is(100)); You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries H2 R2DBC Non Blocking Entry Store Example In this H2 R2DBC cache store example, we use the H2Person cache which implements the NonBlockingEntryStore for non-blocking APIs and access to entries in their serialized ( BinaryEntry ) form. Review the Cache Configuration The h2r2dbc-entry-store-cache-config.xml below shows the H2Person cache specifying the class name of the NonBlockingEntryStore implementation. <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;H2Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.H2R2DBCEntryStore&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Unit Test Next we will run the H2R2DBCEntryStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { createTable(); startupCoherence(\"h2r2dbc-entry-store-cache-config.xml\"); } /** * Performs some cache manipulations. */ @Test public void testNonBlockingEntryStore() { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"H2Person\", TypeAssertion.withTypes(Long.class, Person.class)); Person person1 = namedMap.get(Long.valueOf(101)); assertEquals(\"Robert\", person1.getFirstname()); Insert 1 person using a put operation and confirm the data is in the cache. <markup lang=\"java\" >Person person2 = new Person(Long.valueOf(102), 40, \"Tony\", \"Soprano\"); namedMap.put(Long.valueOf(102), person2); Person person3 = namedMap.get(Long.valueOf(102)); assertEquals(\"Tony\", person3.getFirstname()); Delete a couple records and verify the state of the cache. <markup lang=\"java\" >namedMap.remove(Long.valueOf(101)); namedMap.remove(Long.valueOf(102)); assertEquals(null, namedMap.get(Long.valueOf(101))); assertEquals(null, namedMap.get(Long.valueOf(102))); Insert 10 persons using a putAll operation and confirm the data is in the cache. The actual database operations take place in parallel.s <markup lang=\"java\" >Map&lt;Long, Person&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 10; i++) { map.put(Long.valueOf(i), new Person(Long.valueOf(i), 20 + i, \"firstname\" + i, \"lastname\" + i)); } namedMap.putAll(map); Person person5 = namedMap.get(Long.valueOf(5)); assertEquals(\"firstname5\", person5.getFirstname()); assertEquals(10, namedMap.size()); You should see messages indicating activity on the store side: 2021-06-29 15:01:36.365/5.583 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.495/5.713 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore store 2021-06-29 15:01:36.501/5.720 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.504/5.722 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.507/5.726 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.508/5.727 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.509/5.728 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.512/5.730 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Could not find row for key: 101 2021-06-29 15:01:36.515/5.734 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore storeAll Pluggable Cache Stores A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); Summary You have seen how to use and configure Cache Stores within Coherence. See Also Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "Cache Stores"
        },
        {
            "location": "/examples/guides/000-overview",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor it you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Put Get and Remove This guide walks you through basic CRUD put , get , and remove operations on a NamedMap . Built-in Aggregators This guide walks you through how to use built-in aggregators within Coherence. Custom Aggregators This guide walks you through how to create custom aggregators within Coherence. Topics This guide walks you through how to use Topics within Coherence Near Caching This guide walks you through how to use near caching within Coherence Client Events This guide walks you through how to use client events within Coherence Durable Events This guide walks you through how to use durable events within Coherence Cache Stores This guide walks you through how to use and configure Cache Stores Monitoring StatusHA This guide walks you through how to monitor the High Available (HA) Status for Services ",
            "title": "Guides"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " In this example you will run a number of tests and that show the following features of client events including: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " The example code comprises the ClientEventsTest class, which runs a test showing various aspects of client events. The testMapListeners runs the following test code for various scenarios testStandardMapListener - standard MapListener implementation listening to all events testMultiplexingMapListener - MultiplexingMapListener listening to all events through the onMapEvent() method testSimpleMapListener - SimpleMapListener allows the use of lambdas to add event handlers to listen to events testListenOnQueries - listening for only for events on New York customers testEventTypes - listening for new or updated GOLD customers Review the Customer class All the tests use the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private int id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review the test boostrap and cleanup to start the cluster before all the tests and shutdown after the tests <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); coherence.start().join(); customers = coherence.getSession().getMap(\"customers\"); } <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Review the testStandardMapListener code This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. <markup lang=\"java\" >/** * Simple {@link MapListener} implementation for Customers. */ public static class CustomerMapListener implements MapListener&lt;Integer, Customer&gt; { private final AtomicInteger insertCount = new AtomicInteger(); private final AtomicInteger updateCount = new AtomicInteger(); private final AtomicInteger removeCount = new AtomicInteger(); private final AtomicInteger liteEvents = new AtomicInteger(); @Override public void entryInserted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"New customer: new key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getNewValue()); insertCount.incrementAndGet(); if (mapEvent.getNewValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryUpdated(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Updated customer key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); updateCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryDeleted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Deleted customer: old key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getOldValue()); removeCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } public int getInsertCount() { return insertCount.get(); } public int getUpdateCount() { return updateCount.get(); } public int getRemoveCount() { return removeCount.get(); } public int getLiteEvents() { return liteEvents.get(); } } Implements MapListener interface AtomicIntegers for test validation Respond to insert events with new value Respond to update events with old and new values Respond to delete events with old value <markup lang=\"java\" >Logger.info(\"*** testStandardMapListener\"); customers.clear(); CustomerMapListener mapListener = new CustomerMapListener(); customers.addMapListener(mapListener); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.invoke(1, Processors.update(Customer::setCreditLimit, 2000L)); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(1)); Eventually.assertThat(invoking(mapListener).getRemoveCount(), is(1)); customers.removeMapListener(mapListener); Create the MapListener Add the MapListener to listen for all events Add the customers Update the credit limit for customer 1 Remove customer 1 Wait for all events Review the testMultiplexingMapListener code This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"java\" >/** * Simple {@link MultiplexingMapListener} implementation for Customers. */ public static class MultiplexingCustomerMapListener extends MultiplexingMapListener&lt;Integer, Customer&gt; { private final AtomicInteger counter = new AtomicInteger(); @Override protected void onMapEvent(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"isInsert=\" + mapEvent.isInsert() + \", isDelete=\" + mapEvent.isDelete() + \", isUpdate=\" + mapEvent.isUpdate()); Logger.info(\"key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); Logger.info(mapEvent.toString()); counter.incrementAndGet(); } public int getCount() { return counter.get(); } } Extends abstract class MultiplexingMapListener AtomicInteger for test validation Respond to all events and use MapEvent methods to determine type of event <markup lang=\"java\" >Logger.info(\"*** testMultiplexingMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; multiplexingMapListener = new MultiplexingCustomerMapListener(); // Multiplexing MapListener listening on all entries customers.addMapListener(multiplexingMapListener); customer1 = new Customer(1, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.invoke(1, Processors.update(Customer::setAddress, \"Updated address\")); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking((MultiplexingCustomerMapListener) multiplexingMapListener).getCount(), is(3)); customers.removeMapListener(multiplexingMapListener); Create the MapListener Add the MapListener to listen for all events Mutate the customers Wait for all events Review the testSimpleMapListener code This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"java\" >Logger.info(\"*** testSimpleMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; simpleMapListener = new SimpleMapListener&lt;Integer, Customer&gt;() .addInsertHandler((e) -&gt; Logger.info(\"New Customer added with id=\" + e.getNewValue().getId())) .addDeleteHandler((e) -&gt; Logger.info(\"Deleted customer id =\" + e.getOldValue().getId())) .addInsertHandler((e) -&gt; insertCount.incrementAndGet()) .addDeleteHandler((e) -&gt; deleteCount.incrementAndGet()); customers.addMapListener(simpleMapListener, 1, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.clear(); // should only be 1 insert and 1 delete as we are listening on the key Eventually.assertThat(invoking(this).getInsertCount(), is(1)); Eventually.assertThat(invoking(this).getDeleteCount(), is(1)); Create the SimpleMapListener instance Add an insert handler to display new customers Add delete a handler to display deleted customers Add an insert handler to increment an atomic Add delete a handler to increment an atomic Register the listener on the key 1 (customer id 1) wait for all events Review the testListenOnQueries code This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"java\" >Logger.info(\"*** testListenOnQueries\"); customers.clear(); mapListener = new CustomerMapListener(); // MapListener listening only to new customers from NY Filter&lt;Customer&gt; filter = Filters.like(Customer::getAddress, \"%NY%\"); MapEventFilter&lt;Integer, Customer&gt; eventFilter = new MapEventFilter&lt;&gt;(filter); customer1 = new Customer(1, \"Tim\", \"123 James Street, Perth, Australia\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street, New York, NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customer4 = new Customer(4, \"James Stewart\", \"123 5th Ave, New York, NY\", Customer.SILVER, 200); // Listen only for events where address is in New York customers.addMapListener(mapListener, eventFilter, true); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); customers.put(customer4.getId(), customer4); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); // ensure we only receive lite events Eventually.assertThat(invoking(mapListener).getLiteEvents(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the MapListener instance Create a like filter to select only customers whose address contains NY Add the map listener and specify a MapEventFilter which takes the filter created above as well as specifying the event is lite event where the new and old values may not necessarily be present wait for all events Review the testEventTypes code This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"java\" >Logger.info(\"*** testEventTypes\"); customers.clear(); mapListener = new CustomerMapListener(); filter = Filters.equal(Customer::getCustomerType, Customer.GOLD); // listen only for events where customers has been inserted as GOLD or updated to GOLD status or were changed from GOLD int mask = MapEventFilter.E_INSERTED | MapEventFilter.E_UPDATED_ENTERED| MapEventFilter.E_UPDATED_LEFT; eventFilter = new MapEventFilter&lt;&gt;(mask, filter); customers.addMapListener(mapListener, eventFilter, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); // update customer 1 from BRONZE to GOLD customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); customers.invoke(2, Processors.update(Customer::setCustomerType, Customer.SILVER)); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(1)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the CustomerMapListener instance Create an equals filter to select only GOLD customers Create a mask for inserted events for when the filter is matched or events that are updated and now the filter matches Add the map listener and specify a MapEventFilter which takes the filter created above/ wait for all events ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.clientevents.ClientEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code testStandardMapListener Output This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. Output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testStandardMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer: old key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} Insert event from new customer id 1 Insert event from new customer id 2 Update event from updating of customer 1&#8217;s credit limit Delete event containing old version of deleted customer 1 testMultiplexingMapListener Output This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=true, isDelete=false, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=null, new=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=false, isUpdate=true &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=true, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000}, new=null Insert event from new customer id 1 Update event from an update of customer 1 address Delete event from customer 1 testSimpleMapListener Output This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testSimpleMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New Customer added with id=1 &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer id =1 testListenOnQueries Output This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/null &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=4/null Both above queries only return the key because they are lite events and only customer 2 and 4 are returned as they are the only ones with NY in the address. testEventTypes Output This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testEventTypes &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='GOLD', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=2, old=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='SILVER', balance=10000} Insert event from new GOLD customer id 2 Update event changing customer type from BRONZE to GOLD for customer id 1 Update event changing customer type from GOLD to BRONZE for customer id 2 ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " In this example you have seen how to use the following features of client events: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " This guide walks you through how to use client events within Coherence to listen for insert, update or delete events on a Coherence NamedMap . An application object that implements the MapListener interface can sign up for events from any Coherence NamedMap simply by passing an instance of the application&#8217;s MapListener implementation to a addMapListener() method. The MapListener can be registered against all entries, a specific key, or a Filter. Registrations with filters can use MapEventFilter which provide more fine-grained control for event registrations or InKeySetFilter which can be used to register against a Set of keys. The MapListener interface provides a call back mechanism for NamedMap events where any changes that happen to the source (NamedMap) are delivered to relevant clients asynchronously. The MapEvent object that is passed to the MapListener carries all the necessary information about the event that has occurred Including the event type (insert, update, or delete), the key, old value, new value, and the source ( NameMap ) that emitted the event. Client events are the key building blocks for other Coherence functionality including Near Cache and Continuous Query Caches (CQC). See the Coherence Documentation links below for more information: Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches Table of Contents What You Will Build What You Need Building the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of client events including: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Tests The example code comprises the ClientEventsTest class, which runs a test showing various aspects of client events. The testMapListeners runs the following test code for various scenarios testStandardMapListener - standard MapListener implementation listening to all events testMultiplexingMapListener - MultiplexingMapListener listening to all events through the onMapEvent() method testSimpleMapListener - SimpleMapListener allows the use of lambdas to add event handlers to listen to events testListenOnQueries - listening for only for events on New York customers testEventTypes - listening for new or updated GOLD customers Review the Customer class All the tests use the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private int id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review the test boostrap and cleanup to start the cluster before all the tests and shutdown after the tests <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); coherence.start().join(); customers = coherence.getSession().getMap(\"customers\"); } <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Review the testStandardMapListener code This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. <markup lang=\"java\" >/** * Simple {@link MapListener} implementation for Customers. */ public static class CustomerMapListener implements MapListener&lt;Integer, Customer&gt; { private final AtomicInteger insertCount = new AtomicInteger(); private final AtomicInteger updateCount = new AtomicInteger(); private final AtomicInteger removeCount = new AtomicInteger(); private final AtomicInteger liteEvents = new AtomicInteger(); @Override public void entryInserted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"New customer: new key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getNewValue()); insertCount.incrementAndGet(); if (mapEvent.getNewValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryUpdated(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Updated customer key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); updateCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryDeleted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Deleted customer: old key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getOldValue()); removeCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } public int getInsertCount() { return insertCount.get(); } public int getUpdateCount() { return updateCount.get(); } public int getRemoveCount() { return removeCount.get(); } public int getLiteEvents() { return liteEvents.get(); } } Implements MapListener interface AtomicIntegers for test validation Respond to insert events with new value Respond to update events with old and new values Respond to delete events with old value <markup lang=\"java\" >Logger.info(\"*** testStandardMapListener\"); customers.clear(); CustomerMapListener mapListener = new CustomerMapListener(); customers.addMapListener(mapListener); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.invoke(1, Processors.update(Customer::setCreditLimit, 2000L)); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(1)); Eventually.assertThat(invoking(mapListener).getRemoveCount(), is(1)); customers.removeMapListener(mapListener); Create the MapListener Add the MapListener to listen for all events Add the customers Update the credit limit for customer 1 Remove customer 1 Wait for all events Review the testMultiplexingMapListener code This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"java\" >/** * Simple {@link MultiplexingMapListener} implementation for Customers. */ public static class MultiplexingCustomerMapListener extends MultiplexingMapListener&lt;Integer, Customer&gt; { private final AtomicInteger counter = new AtomicInteger(); @Override protected void onMapEvent(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"isInsert=\" + mapEvent.isInsert() + \", isDelete=\" + mapEvent.isDelete() + \", isUpdate=\" + mapEvent.isUpdate()); Logger.info(\"key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); Logger.info(mapEvent.toString()); counter.incrementAndGet(); } public int getCount() { return counter.get(); } } Extends abstract class MultiplexingMapListener AtomicInteger for test validation Respond to all events and use MapEvent methods to determine type of event <markup lang=\"java\" >Logger.info(\"*** testMultiplexingMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; multiplexingMapListener = new MultiplexingCustomerMapListener(); // Multiplexing MapListener listening on all entries customers.addMapListener(multiplexingMapListener); customer1 = new Customer(1, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.invoke(1, Processors.update(Customer::setAddress, \"Updated address\")); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking((MultiplexingCustomerMapListener) multiplexingMapListener).getCount(), is(3)); customers.removeMapListener(multiplexingMapListener); Create the MapListener Add the MapListener to listen for all events Mutate the customers Wait for all events Review the testSimpleMapListener code This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"java\" >Logger.info(\"*** testSimpleMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; simpleMapListener = new SimpleMapListener&lt;Integer, Customer&gt;() .addInsertHandler((e) -&gt; Logger.info(\"New Customer added with id=\" + e.getNewValue().getId())) .addDeleteHandler((e) -&gt; Logger.info(\"Deleted customer id =\" + e.getOldValue().getId())) .addInsertHandler((e) -&gt; insertCount.incrementAndGet()) .addDeleteHandler((e) -&gt; deleteCount.incrementAndGet()); customers.addMapListener(simpleMapListener, 1, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.clear(); // should only be 1 insert and 1 delete as we are listening on the key Eventually.assertThat(invoking(this).getInsertCount(), is(1)); Eventually.assertThat(invoking(this).getDeleteCount(), is(1)); Create the SimpleMapListener instance Add an insert handler to display new customers Add delete a handler to display deleted customers Add an insert handler to increment an atomic Add delete a handler to increment an atomic Register the listener on the key 1 (customer id 1) wait for all events Review the testListenOnQueries code This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"java\" >Logger.info(\"*** testListenOnQueries\"); customers.clear(); mapListener = new CustomerMapListener(); // MapListener listening only to new customers from NY Filter&lt;Customer&gt; filter = Filters.like(Customer::getAddress, \"%NY%\"); MapEventFilter&lt;Integer, Customer&gt; eventFilter = new MapEventFilter&lt;&gt;(filter); customer1 = new Customer(1, \"Tim\", \"123 James Street, Perth, Australia\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street, New York, NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customer4 = new Customer(4, \"James Stewart\", \"123 5th Ave, New York, NY\", Customer.SILVER, 200); // Listen only for events where address is in New York customers.addMapListener(mapListener, eventFilter, true); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); customers.put(customer4.getId(), customer4); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); // ensure we only receive lite events Eventually.assertThat(invoking(mapListener).getLiteEvents(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the MapListener instance Create a like filter to select only customers whose address contains NY Add the map listener and specify a MapEventFilter which takes the filter created above as well as specifying the event is lite event where the new and old values may not necessarily be present wait for all events Review the testEventTypes code This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"java\" >Logger.info(\"*** testEventTypes\"); customers.clear(); mapListener = new CustomerMapListener(); filter = Filters.equal(Customer::getCustomerType, Customer.GOLD); // listen only for events where customers has been inserted as GOLD or updated to GOLD status or were changed from GOLD int mask = MapEventFilter.E_INSERTED | MapEventFilter.E_UPDATED_ENTERED| MapEventFilter.E_UPDATED_LEFT; eventFilter = new MapEventFilter&lt;&gt;(mask, filter); customers.addMapListener(mapListener, eventFilter, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); // update customer 1 from BRONZE to GOLD customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); customers.invoke(2, Processors.update(Customer::setCustomerType, Customer.SILVER)); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(1)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the CustomerMapListener instance Create an equals filter to select only GOLD customers Create a mask for inserted events for when the filter is matched or events that are updated and now the filter matches Add the map listener and specify a MapEventFilter which takes the filter created above/ wait for all events Run the Examples Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.clientevents.ClientEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code testStandardMapListener Output This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. Output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testStandardMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer: old key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} Insert event from new customer id 1 Insert event from new customer id 2 Update event from updating of customer 1&#8217;s credit limit Delete event containing old version of deleted customer 1 testMultiplexingMapListener Output This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=true, isDelete=false, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=null, new=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=false, isUpdate=true &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=true, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000}, new=null Insert event from new customer id 1 Update event from an update of customer 1 address Delete event from customer 1 testSimpleMapListener Output This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testSimpleMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New Customer added with id=1 &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer id =1 testListenOnQueries Output This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/null &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=4/null Both above queries only return the key because they are lite events and only customer 2 and 4 are returned as they are the only ones with NY in the address. testEventTypes Output This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testEventTypes &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='GOLD', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=2, old=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='SILVER', balance=10000} Insert event from new GOLD customer id 2 Update event changing customer type from BRONZE to GOLD for customer id 1 Update event changing customer type from GOLD to BRONZE for customer id 2 Summary In this example you have seen how to use the following features of client events: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters See Also Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches ",
            "title": "Client Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": "",
            "title": "Coherence Java CDI gRPC Client"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. ",
            "title": "Configure gRPC Channels"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence gRPC client will attempt to connect to a default server endpoint on localhost:1409 if no other channel has been configured. This is fine for development and testing but in most real-world applications the client will need to know the endpoint to connect to. Most applications would only require a channel to connect to a single Coherence cluster but there are use-cases where clients connect to multiple clusters, and the Coherence gRPC Java client supports these use-cases too. The Coherence gRPC client has been built on top of the Helidon Microprofile gRPC library and uses it to provide gRPC channels. Configure gRPC Channels Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. ",
            "title": "Remote Connections"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. ",
            "title": "Session Types"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " Coherence uses the concept of a Session to manage a set of related Coherence resources, such as maps, caches, topics, etc. When using the Coherence Java gRPC client a Session connects to a specific gRPC channel (described above) and uses a specific serialization format to marshal requests and responses. This means that different sessions using different serializers may connect to the same server endpoint. Typically, for efficiency the client and server would be configured to use matching serialization formats to avoid deserialization of data on the server but this does not have to be the case. If the server is using a different serializer for the server side caches it must be able to deserialize the client&#8217;s requests, so there must be a serializer configured on the server to match that used by the client. As with gRPC channels above, Coherence Sessions can be configured using Helidon configuration. Coherence sessions are configured in the coherence.sessions section of the configuration. Each session has its own entry in the configuration hierarchy, as shown below: <markup lang=\"yaml\" >coherence: sessions: default: serializer: pof channel: default The example above shows configuration for the default Coherence session, this is the session that will be used to provide Coherence beans when no session name has been specified for an injection point. In this example, the default session will use POF serialization and connect to the server using the default gRPC channel. The default session, if not configured, will use the default channel and will use Java serialization. As with channels, multiple sessions can be configured: <markup lang=\"yaml\" >coherence: sessions: test: serializer: pof channel: test prod: serializer: pof channel: prod # Helidon gRPC configuration grpc: channels: - name: test host: test.storage.acme.com port: 1408 - name: prod host: storage.acme.com port: 1408 In the example above, there are two Coherence sessions configured and two corresponding gRPC channels. Session Types By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. ",
            "title": "Coherence gRPC Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " A number of commonly used Coherence objects can be injected when using Java gRPC client. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In order to inject an instance of a NamedMap into your gRPC client CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >@Inject private NamedMap&lt;Long, Person&gt; people; <markup lang=\"java\" >@Inject @SesionName(\"products\") private NamedMap&lt;Long, Product&gt; products; In this example the Coherence CDI extensions will use the products session to provide the client side NamedMap backed on the server by a NamedMap called products . Other remote resources, such a NamedCache can be injected the same way: <markup lang=\"java\" >@Inject private NamedCache&lt;Long, Product&gt; products; The Coherence CDI documentation covers the different types of resources supported by CDI. When using them with the gRPC Java client. ",
            "title": "Injecting NamedMap NamedCache and Related Objects"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " If an application bean requires multiple maps or caches where the names will only be known at runtime then a Coherence com.tangosol.net.Session can be injected instead of other specific named resources. The required maps or caches can then be obtained from the Session by calling methods such as Session.getMap or Session.getCache , etc. <markup lang=\"java\" >@Inject @Name(\"products\") private Session session; The @Name qualifier has the value products , so the Session injected here will be the pre-configured Session named products . ",
            "title": "Injecting Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. ",
            "title": "Observe Events for Maps and Caches from Specific Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. ",
            "title": "Observe Events for Maps and Caches from Specific Server-side Scopes"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence NamedMap and NamedCache APIs allow implementations of MapListener to be added that will then receive events as map/cache entries get inserted, updated or deleted. When using CDI it is possible to subscribe to the same events using CDI observer methods. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } The Observes qualifier is what makes this method a standard CDI observer. The MapName qualifier determines which map/cache to observer. If this qualifier is not present events from all caches will be observed. Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. Observe Events for Maps and Caches from Specific Sessions In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. Observe Events for Maps and Caches from Specific Server-side Scopes In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Using Asynchronous Observers All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Using CDI Observers to Handle MapEvents"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence gRPC Helidon client is a CDI enabled library that allows Java clients to connect via gRPC to a Coherence proxy server. This library has a dependency on Helidon for some services. In order to use Coherence gRPC Helidon client, you need to declare it as a dependency in your pom.xml <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-helidon-client&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Using the Coherence gRPC Helidon client assumes that there is a corresponding server process running which is using the Coherence gRPC proxy service to expose the required gRPC endpoints. Once the necessary dependency is in place, the simplest way to start using it is to just inject Coherence resources into the application&#8217;s beans. A lot of the annotations and qualifiers are identical to those described in the Coherence CDI documentation. The following sections describe different injection points in more detail. Configuring gRPC Connections Configuring Coherence Remote Sessions Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache , and related objects Session Injection Using CDI Observers to Handle Coherence Map or Cache Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches owned by a specific Session Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Remote Connections The Coherence gRPC client will attempt to connect to a default server endpoint on localhost:1409 if no other channel has been configured. This is fine for development and testing but in most real-world applications the client will need to know the endpoint to connect to. Most applications would only require a channel to connect to a single Coherence cluster but there are use-cases where clients connect to multiple clusters, and the Coherence gRPC Java client supports these use-cases too. The Coherence gRPC client has been built on top of the Helidon Microprofile gRPC library and uses it to provide gRPC channels. Configure gRPC Channels Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. Coherence gRPC Sessions Coherence uses the concept of a Session to manage a set of related Coherence resources, such as maps, caches, topics, etc. When using the Coherence Java gRPC client a Session connects to a specific gRPC channel (described above) and uses a specific serialization format to marshal requests and responses. This means that different sessions using different serializers may connect to the same server endpoint. Typically, for efficiency the client and server would be configured to use matching serialization formats to avoid deserialization of data on the server but this does not have to be the case. If the server is using a different serializer for the server side caches it must be able to deserialize the client&#8217;s requests, so there must be a serializer configured on the server to match that used by the client. As with gRPC channels above, Coherence Sessions can be configured using Helidon configuration. Coherence sessions are configured in the coherence.sessions section of the configuration. Each session has its own entry in the configuration hierarchy, as shown below: <markup lang=\"yaml\" >coherence: sessions: default: serializer: pof channel: default The example above shows configuration for the default Coherence session, this is the session that will be used to provide Coherence beans when no session name has been specified for an injection point. In this example, the default session will use POF serialization and connect to the server using the default gRPC channel. The default session, if not configured, will use the default channel and will use Java serialization. As with channels, multiple sessions can be configured: <markup lang=\"yaml\" >coherence: sessions: test: serializer: pof channel: test prod: serializer: pof channel: prod # Helidon gRPC configuration grpc: channels: - name: test host: test.storage.acme.com port: 1408 - name: prod host: storage.acme.com port: 1408 In the example above, there are two Coherence sessions configured and two corresponding gRPC channels. Session Types By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. Injecting Coherence Objects into CDI Beans A number of commonly used Coherence objects can be injected when using Java gRPC client. Injecting NamedMap NamedCache and Related Objects In order to inject an instance of a NamedMap into your gRPC client CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >@Inject private NamedMap&lt;Long, Person&gt; people; <markup lang=\"java\" >@Inject @SesionName(\"products\") private NamedMap&lt;Long, Product&gt; products; In this example the Coherence CDI extensions will use the products session to provide the client side NamedMap backed on the server by a NamedMap called products . Other remote resources, such a NamedCache can be injected the same way: <markup lang=\"java\" >@Inject private NamedCache&lt;Long, Product&gt; products; The Coherence CDI documentation covers the different types of resources supported by CDI. When using them with the gRPC Java client. Injecting Sessions If an application bean requires multiple maps or caches where the names will only be known at runtime then a Coherence com.tangosol.net.Session can be injected instead of other specific named resources. The required maps or caches can then be obtained from the Session by calling methods such as Session.getMap or Session.getCache , etc. <markup lang=\"java\" >@Inject @Name(\"products\") private Session session; The @Name qualifier has the value products , so the Session injected here will be the pre-configured Session named products . Using CDI Observers to Handle MapEvents The Coherence NamedMap and NamedCache APIs allow implementations of MapListener to be added that will then receive events as map/cache entries get inserted, updated or deleted. When using CDI it is possible to subscribe to the same events using CDI observer methods. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } The Observes qualifier is what makes this method a standard CDI observer. The MapName qualifier determines which map/cache to observer. If this qualifier is not present events from all caches will be observed. Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. Observe Events for Maps and Caches from Specific Sessions In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. Observe Events for Maps and Caches from Specific Server-side Scopes In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Using Asynchronous Observers All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Usage"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. ",
            "title": "Image Contents"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 21.12.6-SNAPSHOT the image produced will be oraclecoherence/coherence-ce:21.12.6-SNAPSHOT To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:21.12.6-SNAPSHOT ",
            "title": "Building the Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the Image in Kubernetes"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. ",
            "title": "Specifying Coherence System Properties"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. ",
            "title": "Specifying JVM Options"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Adding to the Classpath"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This module builds an example Coherence OCI compatible image. The image built in this module is a demo and example of how to build a Coherence image using the JIB Maven Plugin . The image is not intended to be used in production deployments or as a base image, it is specifically for demos, experimentation and learning purposes. Image Contents The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. Building the Image Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 21.12.6-SNAPSHOT the image produced will be oraclecoherence/coherence-ce:21.12.6-SNAPSHOT To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:21.12.6-SNAPSHOT Run the image Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. Specifying Coherence System Properties Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. Specifying JVM Options Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. Adding to the Classpath Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Coherence OCI Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Multiple containers can be started to form a cluster. By default, Coherence uses multi-cast for cluster discovery but in containers this either will not work, or is not reliable, so well-known-addressing can be used. This example is going to use basic Docker commands and links between containers. There are other ways to achieve the same sort of functionality depending on the network configurations you want to use in Docker. First, determine the name to be used for the first container, in this example it will be storage-1 . Next, create a ` Start the first container in the cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-1 \\ --hostname storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} The first container has been started with a container name of storage-1 , and the host name also set to storage-1 . The container sets the WKA host name to storage-1 using -e coherence.wka=storage-1 (this will be converted to the System property coherence.wka=storage-1 see Specifying Coherence System Properties above). The container sets the Coherence cluster name to testing using -e coherence.cluster=testing (this will be converted to the System property coherence.cluster=testing see Specifying Coherence System Properties above). The important part here is that the container has a name, and the --hostname option has also been set. This will allow the subsequent cluster members to find this container. Now, subsequent containers can be started using the same cluster name and WKA host name, but with different container names and a link to the first container, all the containers will form a single Coherence cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-2 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} docker run -d -P \\ --name storage-3 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} Two more containers, storage-2 and storage-3 will now be part of the cluster. All the members must have a --link option to the first container and have the same WKA and cluster name properties. ",
            "title": "Clustering"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image comes with tracing already configured, it just requires a suitable Jaeger server to send spans to. The simplest way to start is deploy the Jaeger all-in-one server, for example: <markup lang=\"bash\" >docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:latest The Jaeger UI will be available to browse to at http://127.0.0.1:16686 Jaeger has been started with a container name of jaeger , so it will be discoverable using that host name by the Coherence containers. Start the Coherence container with a link to the Jaeger container and set the JAEGER_AGENT_HOST environment variable to jaeger : <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ oraclecoherence/coherence-ce:{version-coherence-maven} Once the Coherence container is running perform some interactions with it using one of the exposed services, i.e Extend or gRPC, and spans will be sent to the Jaeger collector and will be visible in the UI by querying for the coherence service name. The service name used can be changed by setting the JAEGER_SERVICE_NAME environment variable when starting the container, for example: <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ -e JAEGER_SERVICE_NAME=coherence-test oraclecoherence/coherence-ce:{version-coherence-maven} Spans will now be sent to Jaeger with the service name coherence-test . Tracing is very useful to show what happens under the covers for a given Coherence API call. Traces are more interesting when they come from a Coherence cluster with multiple members, where the traces span different cluster members. This can easily be done by running multiple containers with tracing enabled and configuring Clustering as described above. ",
            "title": "Tracing"
        },
        {
            "location": "/docs/core/07_partition_events_logging",
            "text": " In order to preserve the integrity of data, when partition events occur such as partition movements between members, read or write access to data will be temporarily blocked. This happens when re-distribution takes place or indices are built. The amounts of time involved are usually extremely short, but can add up if the cache contains significant amounts of data. Event Type Description Redistribution When a server joins or leaves a cluster, a number of events occur on each member in the cluster, existing and new, which correspond to the movement of data and backup partitions according to the partitioning strategy. This scheme determines which members own which partitions, and which members own which backups. Restoring from backup After primary partitions are lost, the backups are moved into primary storage in the members where they are located. Naturally, the partitions in question are locked until the event finishes. Recovery from persistence Persistence maintenance, such as snapshot creation and recovery from persistence, will cause the affected partitions to be unavailable. Index building If an application needs to have data indexed, this is typically done by calling addIndex on a NamedCache . If the cache already contains a significant amount of data, or the cost of computing the index per entry (the ValueExtractor ) is high, this operation can take some time during which any submitted queries will be blocked waiting for the index data structures to be populated. Note that regular index maintenance, such as adding or deleting elements, does not incur the same unavailability penalty. ",
            "title": "Data Availability"
        },
        {
            "location": "/docs/core/07_partition_events_logging",
            "text": " By default, logging of times when partitions are unavailable is turned off as it generates a significant amount of logs. To enable logging of partition events, set the property coherence.distributed.partition.events to log and set log level to 8 or more. e.g.: <markup lang=\"text\" >-Dcoherence.distributed.partition.events=log ",
            "title": "Feature Usage"
        },
        {
            "location": "/docs/core/07_partition_events_logging",
            "text": " The events below are logged, one per partition except when partitions are initially assigned. Along with that, the owning member and possibly the time it made the partition unavailable are also logged. Event Description ASSIGN The partition is either initially, or as a result of losing primary and all backups, assigned to a cluster member. PRIMARY_TRANSFER_OUT The partition is being transferred to a different member. BACKUP_TRANSFER_OUT This primary is transferring a snapshot of the partition and all its content to the targeted member as it will be in the chain of backup replicas. PRIMARY_TRANSFER_IN This member is receiving a partition and all related data for primary ownership. This will be due to a PRIMARY_TRANSFER_OUT from the existing owner of the partition. RESTORE The loss of primary partitions results in backup owners (replicas) restoring data from backup storage to primary for the affected partitions. INDEX_BUILD Index data structures were populated for the relevant partitions. This will effect queries that use said indices but does not block key based data access or mutation. PERSISTENCE The relevant partitions were made unavailable due to persistence maintenance operations; this will minimally include recovery from persistence and snapshot creation. ",
            "title": "Events Logged"
        },
        {
            "location": "/docs/core/07_partition_events_logging",
            "text": " On Member 1: (at startup) 2021-06-11 09:26:10.159/5.522 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedTopicDedicated:0x000A:5, member=1): PartitionSet{0..256}, Owner: 1, Action: ASSIGN, UnavailableTime: 0 ... (application calls addIndex() on a cache) 2021-06-11 09:28:36.872/152.234 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000B:152, member=1): PartitionId: 43, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 3 ... (the partitions listed are being transferred to another member, along with backups; note how backups and partitions are not the same) 2021-06-11 09:28:45.573/160.935 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 132, Owner: 1, Action: BACKUP_TRANSFER_OUT, UnavailableTime: 1 2021-06-11 09:28:45.678/161.040 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 133, Owner: 1, Action: BACKUP_TRANSFER_OUT, UnavailableTime: 1 ... 2021-06-11 09:28:49.911/165.273 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 2, Owner: 1, Action: PRIMARY_TRANSFER_OUT, UnavailableTime: 5 2021-06-11 09:28:50.017/165.379 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 3, Owner: 1, Action: PRIMARY_TRANSFER_OUT, UnavailableTime: 3 ... On Member 2: (partitions are being received; if they have indices, they are rebuilt immediately after reception) 2021-06-11 09:28:49.805/8.033 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=2): PartitionId: 1, Owner: 2, Action: PRIMARY_TRANSFER_IN, UnavailableTime: 1 2021-06-11 09:28:49.806/8.034 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000B:8, member=2): PartitionId: 1, Owner: 2, Action: INDEX_BUILD, UnavailableTime: 0 Member 2 stops, back on Member 1: (partitions are being restored from backup, and the indices related to them rebuilt) 2021-06-11 10:29:19.041/3794.322 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 0, Owner: 1, Action: RESTORE, UnavailableTime: 109 2021-06-11 10:29:19.041/3794.322 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 1, Owner: 1, Action: RESTORE, UnavailableTime: 109 ... 2021-06-11 10:29:19.062/3794.343 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000E:3794, member=1): PartitionId: 1, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 12 2021-06-11 10:29:19.066/3794.347 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000D:3794, member=1): PartitionId: 0, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 16 2021-06-11 10:29:19.067/3794.349 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000E:3794, member=1): PartitionId: 2, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 5 ... ",
            "title": "Example"
        },
        {
            "location": "/docs/core/07_partition_events_logging",
            "text": " While logging gives valuable details on partition&#8217;s lifecycle, it is a simple means of providing this information. Ultimately, better forms of presenting it for consumption will be provided, such as a JMX MBean or a Coherence report. ",
            "title": "Future"
        },
        {
            "location": "/docs/core/07_partition_events_logging",
            "text": " The most commonly used service in Coherence to store and access data is the distributed / partitioned service. It offers partitioned access to store and retrieve data, thus provides scalability, in addition to redundancy by ensuring replicas are in sync. This concept of partitioning can be entirely opaque to an end user as Coherence will transparently map keys to partitions and partitions to members. As ownership members join and leave the partitioned service, the partitions are redistributed across the new/remaining members avoiding an entire rehash of the data. Coherence also designates replicas providing them an initial snapshot followed by a journal of updates as they occur on the primary. These partition lifecycle events (members joining and leaving the service) result in partitions being blocked and therefore Coherence attempts to reduce this time of unavailability as much as possible. Until now, there has been minimal means to track this partition unavailability. This feature provides insight into these partition lifecycle events, highlighting when they start and end allowing customers to correlate increased response times with said lifecycle events. Data Availability In order to preserve the integrity of data, when partition events occur such as partition movements between members, read or write access to data will be temporarily blocked. This happens when re-distribution takes place or indices are built. The amounts of time involved are usually extremely short, but can add up if the cache contains significant amounts of data. Event Type Description Redistribution When a server joins or leaves a cluster, a number of events occur on each member in the cluster, existing and new, which correspond to the movement of data and backup partitions according to the partitioning strategy. This scheme determines which members own which partitions, and which members own which backups. Restoring from backup After primary partitions are lost, the backups are moved into primary storage in the members where they are located. Naturally, the partitions in question are locked until the event finishes. Recovery from persistence Persistence maintenance, such as snapshot creation and recovery from persistence, will cause the affected partitions to be unavailable. Index building If an application needs to have data indexed, this is typically done by calling addIndex on a NamedCache . If the cache already contains a significant amount of data, or the cost of computing the index per entry (the ValueExtractor ) is high, this operation can take some time during which any submitted queries will be blocked waiting for the index data structures to be populated. Note that regular index maintenance, such as adding or deleting elements, does not incur the same unavailability penalty. Feature Usage By default, logging of times when partitions are unavailable is turned off as it generates a significant amount of logs. To enable logging of partition events, set the property coherence.distributed.partition.events to log and set log level to 8 or more. e.g.: <markup lang=\"text\" >-Dcoherence.distributed.partition.events=log Events Logged The events below are logged, one per partition except when partitions are initially assigned. Along with that, the owning member and possibly the time it made the partition unavailable are also logged. Event Description ASSIGN The partition is either initially, or as a result of losing primary and all backups, assigned to a cluster member. PRIMARY_TRANSFER_OUT The partition is being transferred to a different member. BACKUP_TRANSFER_OUT This primary is transferring a snapshot of the partition and all its content to the targeted member as it will be in the chain of backup replicas. PRIMARY_TRANSFER_IN This member is receiving a partition and all related data for primary ownership. This will be due to a PRIMARY_TRANSFER_OUT from the existing owner of the partition. RESTORE The loss of primary partitions results in backup owners (replicas) restoring data from backup storage to primary for the affected partitions. INDEX_BUILD Index data structures were populated for the relevant partitions. This will effect queries that use said indices but does not block key based data access or mutation. PERSISTENCE The relevant partitions were made unavailable due to persistence maintenance operations; this will minimally include recovery from persistence and snapshot creation. Example On Member 1: (at startup) 2021-06-11 09:26:10.159/5.522 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedTopicDedicated:0x000A:5, member=1): PartitionSet{0..256}, Owner: 1, Action: ASSIGN, UnavailableTime: 0 ... (application calls addIndex() on a cache) 2021-06-11 09:28:36.872/152.234 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000B:152, member=1): PartitionId: 43, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 3 ... (the partitions listed are being transferred to another member, along with backups; note how backups and partitions are not the same) 2021-06-11 09:28:45.573/160.935 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 132, Owner: 1, Action: BACKUP_TRANSFER_OUT, UnavailableTime: 1 2021-06-11 09:28:45.678/161.040 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 133, Owner: 1, Action: BACKUP_TRANSFER_OUT, UnavailableTime: 1 ... 2021-06-11 09:28:49.911/165.273 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 2, Owner: 1, Action: PRIMARY_TRANSFER_OUT, UnavailableTime: 5 2021-06-11 09:28:50.017/165.379 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 3, Owner: 1, Action: PRIMARY_TRANSFER_OUT, UnavailableTime: 3 ... On Member 2: (partitions are being received; if they have indices, they are rebuilt immediately after reception) 2021-06-11 09:28:49.805/8.033 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=2): PartitionId: 1, Owner: 2, Action: PRIMARY_TRANSFER_IN, UnavailableTime: 1 2021-06-11 09:28:49.806/8.034 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000B:8, member=2): PartitionId: 1, Owner: 2, Action: INDEX_BUILD, UnavailableTime: 0 Member 2 stops, back on Member 1: (partitions are being restored from backup, and the indices related to them rebuilt) 2021-06-11 10:29:19.041/3794.322 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 0, Owner: 1, Action: RESTORE, UnavailableTime: 109 2021-06-11 10:29:19.041/3794.322 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=DistributedCache:PartitionedCache, member=1): PartitionId: 1, Owner: 1, Action: RESTORE, UnavailableTime: 109 ... 2021-06-11 10:29:19.062/3794.343 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000E:3794, member=1): PartitionId: 1, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 12 2021-06-11 10:29:19.066/3794.347 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000D:3794, member=1): PartitionId: 0, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 16 2021-06-11 10:29:19.067/3794.349 Oracle Coherence GE 14.1.2.0.0 (dev-mycomputer) &lt;D8&gt; (thread=PartitionedCacheDedicated:0x000E:3794, member=1): PartitionId: 2, Owner: 1, Action: INDEX_BUILD, UnavailableTime: 5 ... Future While logging gives valuable details on partition&#8217;s lifecycle, it is a simple means of providing this information. Ultimately, better forms of presenting it for consumption will be provided, such as a JMX MBean or a Coherence report. ",
            "title": "Partition Events Logging"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The simplest way to create a Subscribers is from the Coherence Session API, by calling the createSubscriber method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); The code snippet above creates an anonymous Subscriber that subscribes to String messages from the topic named test-topic . Alternatively, a Subscriber can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Subscriber&lt;String&gt; subscriber = topic.createSubscriber(); Both the Session.createSubscriber() and NamedTopic.createSubscriber() methods also take a var-args array of Subscriber.Option instances to further configure the behaviour of the subscriber. Some of these options are described below. ",
            "title": "Creating Subscribers"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " To create a subscriber that is part of a subscriber group the Subscriber.Name option can be used. Subscriber groups have a unique name and a subscriber joins a group by specifying the group name. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import static com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", inGroup(\"group-one\")); The code above creates a subscriber that subscribes from the test-topic .The subscriber is part of the group named group-one .This is specified by adding a Subscriber.Name option using the static factory method Subscriber.Name.inGroup . ",
            "title": "Creating Group Subscribers"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Subscribers should ideally be closed when application code finishes with them so that any server side and client side resources associated with them are also closed and cleaned up.Orphaned subscribers, where the client application has gone away, will eventually be cleaned up by server side code.Subscriber groups that are durable will remain until manually removed. Subscribers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); try (Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\")) { // ... receive messages ... } In the above example, the subscriber is used to receive messages inside the try/catch block.Once the try/catch block exits the subscriber is closed. When a subscriber is closed, it can no longer be used.Calls to subscriber methods after closing will throw an IllegalStateException . ",
            "title": "Closing Subscribers"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " By default, the CompletableFuture returned from a call to receive will not complete until a message is received. If the topic is empty (or in the case of a group subscriber all the channels owned by the subscriber are empty) the future will not complete until a new message is published to the topic or channel. This behaviour can be changed so that is a topic or owned channels are empty, the future will complete with a null element value. This is controlled by creating the subscriber with the CompleteOnEmpty option. For example, to create a subscriber where calls to receive return even if the topic is empty: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); Element&lt;String&gt; element = future.get(); The subscriber is created using the CompleteOnEmpty.enabled() option, so it will complete futures even if the topic is empty. The call to future.get() may return null if the topic or owned channels are empty. ",
            "title": "Future Completion"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Because the subscriber API is asynchronous, multiple consecutive calls can be made to the receive methods, without waiting for the first call to complete.To maintain message delivery order, the subscriber will complete the futures in the order that the calls were made. Important Any use of the CompletableFuture async API (for example future.thenApplyAsync() , future.handleAsync() etc) to hand of completion handling to another thread will then remove any ordering guarantees for message processing. The same applies to application code that manually hands the returned elements off to other worker threads for processing. It is up to the application code to then handle the futures in such a way that ordering is maintained if that is important to the application&#8217;s use-case. The use of the synchronous CompletableFuture API (for example future.thenApply() , future.handle() etc.) will cause completion of other futures by the subscriber to block until the handler code is complete.To maintain order of completion, the subscribe queues up the futures to be completed by a single daemon thread. For examples: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Void&gt; futureOne = subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); CompletableFuture&lt;Void&gt; futureTwo = subscriber.receive() .thenAccept(element -&gt; { // handle second element... }); In the example above, the code that handles the first element must fully complete before the second future will complete. In use cases where order of processing on the client is not important the full async API can be used. Important Another important aspect of using the async API with subscribers is correct error handling. This is bad code: <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); If the call to receive() fails and the future completes exceptionally, or the handler code in the thenAccept call fails and throws an exception, those exceptions will be lost and not even logged. A better way is to always finish with a handle call or use one of the other methods of the CompletableFuture API to check for exceptions. <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // process second element... }).handle((_void, error) -&gt; { if (error != null) { // something went wrong!!! } return null; }); ",
            "title": "Multiple calls to Receive and Message Ordering"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The sole purpose of a subscriber is to receive messages from a topic.This is done by calling the Subscriber.receive() method to receive a single message or Subscriber.receive(int) to receive multiple messages in a batch.Both forms of the receive method are asynchronous, and return a CompletableFuture that will be completed with the result of polling the topic for messages. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); CompletableFuture&lt;List&lt;Element&lt;String&gt;&gt;&gt; futureBatch subscriber.receive(10); The first call to receive will return a CompletableFuture that will complete with a Subscriber.Element that will contain the message from the topic and meta-data about the element. The second call to receive will return a CompletableFuture that will complete with a batch of upto 10 elements.The int parameter is a hint to the subscriber to return a batch and is the maximum number of messages that should be returned, the subscriber could return fewer messages.At most, a subscriber will return a full page of messages in a batch, so calling receive with a value higher than a page size will not return more messages than the page contains. Future Completion By default, the CompletableFuture returned from a call to receive will not complete until a message is received. If the topic is empty (or in the case of a group subscriber all the channels owned by the subscriber are empty) the future will not complete until a new message is published to the topic or channel. This behaviour can be changed so that is a topic or owned channels are empty, the future will complete with a null element value. This is controlled by creating the subscriber with the CompleteOnEmpty option. For example, to create a subscriber where calls to receive return even if the topic is empty: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); Element&lt;String&gt; element = future.get(); The subscriber is created using the CompleteOnEmpty.enabled() option, so it will complete futures even if the topic is empty. The call to future.get() may return null if the topic or owned channels are empty. Multiple calls to Receive and Message Ordering Because the subscriber API is asynchronous, multiple consecutive calls can be made to the receive methods, without waiting for the first call to complete.To maintain message delivery order, the subscriber will complete the futures in the order that the calls were made. Important Any use of the CompletableFuture async API (for example future.thenApplyAsync() , future.handleAsync() etc) to hand of completion handling to another thread will then remove any ordering guarantees for message processing. The same applies to application code that manually hands the returned elements off to other worker threads for processing. It is up to the application code to then handle the futures in such a way that ordering is maintained if that is important to the application&#8217;s use-case. The use of the synchronous CompletableFuture API (for example future.thenApply() , future.handle() etc.) will cause completion of other futures by the subscriber to block until the handler code is complete.To maintain order of completion, the subscribe queues up the futures to be completed by a single daemon thread. For examples: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Void&gt; futureOne = subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); CompletableFuture&lt;Void&gt; futureTwo = subscriber.receive() .thenAccept(element -&gt; { // handle second element... }); In the example above, the code that handles the first element must fully complete before the second future will complete. In use cases where order of processing on the client is not important the full async API can be used. Important Another important aspect of using the async API with subscribers is correct error handling. This is bad code: <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); If the call to receive() fails and the future completes exceptionally, or the handler code in the thenAccept call fails and throws an exception, those exceptions will be lost and not even logged. A better way is to always finish with a handle call or use one of the other methods of the CompletableFuture API to check for exceptions. <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // process second element... }).handle((_void, error) -&gt; { if (error != null) { // something went wrong!!! } return null; }); ",
            "title": "Receiving Messages"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The Element returned from a receive call has a commit() method that can be used to commit the element&#8217;s channel and position. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value CommitResult result = element.commit(); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The commit method is called to commit the position of the element. By committing the element directly, application code does not need to track the channel or positions of received elements. ",
            "title": "Commit a Received Element"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " To commit a Position in a channel directly the Subscriber.commit(int, Position) method can be used. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value int channel = element.getChannel(); Position position = element.commit(); CommitResult result = subscriber.commit(channel, position); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The channel and Position can be obtained for the element The channel and Position can then be committed later by calling commit on the subscriber ",
            "title": "Commit a Position"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " In order to provide at least once delivery guarantees, the subscriber API has methods that allow messages to be committed, so that the server knows they have been processed and will not re-deliver them in the case where a group subscriber fails over or is closed, and a new subscriber in the group takes over the channel ownership. When a subscriber does a commit, it is actually committing a position in a channel of a topic.It effectively says that a specific position in a channel and all earlier positions have been processed.For example if a subscriber reads 10 messages from positions 0 - 9 and commits position 9, then positions 0 - 8 are also committed. There are two ways to commit a position; either using the commit method on an Element returned from a call to receive() , or by calling the commit method on a Subscriber that takes a channel and Position argument. Commit a Received Element The Element returned from a receive call has a commit() method that can be used to commit the element&#8217;s channel and position. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value CommitResult result = element.commit(); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The commit method is called to commit the position of the element. By committing the element directly, application code does not need to track the channel or positions of received elements. Commit a Position To commit a Position in a channel directly the Subscriber.commit(int, Position) method can be used. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value int channel = element.getChannel(); Position position = element.commit(); CommitResult result = subscriber.commit(channel, position); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The channel and Position can be obtained for the element The channel and Position can then be committed later by calling commit on the subscriber ",
            "title": "Committing (Message Acknowledgement)"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The subscriber has a seek method that takes a channel, and a Position that moves the subscriber to the specified position in the channel. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Element&lt;String&gt; firstElement = subscriber.receive().get(); for (int i = 0; i &lt; 10; i++) { Element&lt;String&gt; element = subscriber.receive().get(); // process element... } subscriber.seek(firstElement.getChannel(), firstElement.getPosition()); The example above is a bit contrived, but shows how seek can be used.The first element is received from the topic. Another 10 elements are then processed from the subscriber. The seek method is then used to move the subscriber back to the position of the first message. When seeking, the next message received is the message after the seek position.In the example above, after the seek call the next message received wil not be the same as first element, it will be the next message, so the first message received in the for loop. The subscriber also has methods to seek to the head (re-read the first message) or tail (read the next message published) for a channel without needing to know the head or tail positions. ",
            "title": "Seek to a Position"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Subscribers can also be repositioned to the next message based on a timestamp that the message was published. All messages have a timestamp based on the Coherence cluster time in the storage member that accepted the published message. When seeking using a timestamp, the subscriber is repositioned such that the next message received is the first message after the specified timestamp. The timestamp is specified as a java.time.Instant when seeking to a timestamp. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Instant timestamp = LocalDateTime.of(LocalDate.now(), LocalTime.of(20, 30)) .toInstant(ZoneOffset.UTC); Position position = subscriber.seek(1, timestamp); A java.time.Instant is created for 20:30 today. Seek is called to reposition the subscriber so that the next message received from channel 1 will be the first message published after 20:30. Repositioning to a timestamp in the future will reposition the subscriber at the tail, so the next message received will be the next published message, regardless of the time. It is not possible to seek to a timestamp in the future so that messages are ignored until the time is reached. ",
            "title": "Seek to a Timestamp"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The common behaviour for a subscriber is to connect and then receive messages in order until all the messages are processed.Sometimes though it is desirable to rewind a subscriber to reprocess previously consumed messages, or to move a subscriber forwards to skip messages. When rewinding a position, whether the action is successful or not depends on how the topic has been configured.If the topic is configured to retain messages (not the default) then previously received messages are still available and can be re-received.For topics that do not retain messages, then messages are removed once all connected subscribers, or subscriber groups, have read the message.In the case of non-retained topics therefore, it may not be possible to rewind as the messages may have been removed.Even in topics that retain consumed messages, the messages may have been removed if the topic is configured with message expiry. If an attempt is made to rewind further back than the first message in the topic, the seek will reposition the subscriber just before the first available message.If an attempt is made to reposition a subscriber much further ahead than the current tail of the topic, the subscriber will be positioned at the tail, so that it receives the next published message. Seek to a Position The subscriber has a seek method that takes a channel, and a Position that moves the subscriber to the specified position in the channel. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Element&lt;String&gt; firstElement = subscriber.receive().get(); for (int i = 0; i &lt; 10; i++) { Element&lt;String&gt; element = subscriber.receive().get(); // process element... } subscriber.seek(firstElement.getChannel(), firstElement.getPosition()); The example above is a bit contrived, but shows how seek can be used.The first element is received from the topic. Another 10 elements are then processed from the subscriber. The seek method is then used to move the subscriber back to the position of the first message. When seeking, the next message received is the message after the seek position.In the example above, after the seek call the next message received wil not be the same as first element, it will be the next message, so the first message received in the for loop. The subscriber also has methods to seek to the head (re-read the first message) or tail (read the next message published) for a channel without needing to know the head or tail positions. Seek to a Timestamp Subscribers can also be repositioned to the next message based on a timestamp that the message was published. All messages have a timestamp based on the Coherence cluster time in the storage member that accepted the published message. When seeking using a timestamp, the subscriber is repositioned such that the next message received is the first message after the specified timestamp. The timestamp is specified as a java.time.Instant when seeking to a timestamp. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Instant timestamp = LocalDateTime.of(LocalDate.now(), LocalTime.of(20, 30)) .toInstant(ZoneOffset.UTC); Position position = subscriber.seek(1, timestamp); A java.time.Instant is created for 20:30 today. Seek is called to reposition the subscriber so that the next message received from channel 1 will be the first message published after 20:30. Repositioning to a timestamp in the future will reposition the subscriber at the tail, so the next message received will be the next published message, regardless of the time. It is not possible to seek to a timestamp in the future so that messages are ignored until the time is reached. ",
            "title": "Seeking - Reposition a Subscriber"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Subscribers are used to receive messages to a Coherence topic, a subscriber receives messages from a single topic. Creating Subscribers Creating Subscriber Groups Closing Subscribers Receiving Messages Committing Seek to a Position Creating Subscribers The simplest way to create a Subscribers is from the Coherence Session API, by calling the createSubscriber method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); The code snippet above creates an anonymous Subscriber that subscribes to String messages from the topic named test-topic . Alternatively, a Subscriber can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Subscriber&lt;String&gt; subscriber = topic.createSubscriber(); Both the Session.createSubscriber() and NamedTopic.createSubscriber() methods also take a var-args array of Subscriber.Option instances to further configure the behaviour of the subscriber. Some of these options are described below. Creating Group Subscribers To create a subscriber that is part of a subscriber group the Subscriber.Name option can be used. Subscriber groups have a unique name and a subscriber joins a group by specifying the group name. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import static com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", inGroup(\"group-one\")); The code above creates a subscriber that subscribes from the test-topic .The subscriber is part of the group named group-one .This is specified by adding a Subscriber.Name option using the static factory method Subscriber.Name.inGroup . Closing Subscribers Subscribers should ideally be closed when application code finishes with them so that any server side and client side resources associated with them are also closed and cleaned up.Orphaned subscribers, where the client application has gone away, will eventually be cleaned up by server side code.Subscriber groups that are durable will remain until manually removed. Subscribers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); try (Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\")) { // ... receive messages ... } In the above example, the subscriber is used to receive messages inside the try/catch block.Once the try/catch block exits the subscriber is closed. When a subscriber is closed, it can no longer be used.Calls to subscriber methods after closing will throw an IllegalStateException . Receiving Messages The sole purpose of a subscriber is to receive messages from a topic.This is done by calling the Subscriber.receive() method to receive a single message or Subscriber.receive(int) to receive multiple messages in a batch.Both forms of the receive method are asynchronous, and return a CompletableFuture that will be completed with the result of polling the topic for messages. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); CompletableFuture&lt;List&lt;Element&lt;String&gt;&gt;&gt; futureBatch subscriber.receive(10); The first call to receive will return a CompletableFuture that will complete with a Subscriber.Element that will contain the message from the topic and meta-data about the element. The second call to receive will return a CompletableFuture that will complete with a batch of upto 10 elements.The int parameter is a hint to the subscriber to return a batch and is the maximum number of messages that should be returned, the subscriber could return fewer messages.At most, a subscriber will return a full page of messages in a batch, so calling receive with a value higher than a page size will not return more messages than the page contains. Future Completion By default, the CompletableFuture returned from a call to receive will not complete until a message is received. If the topic is empty (or in the case of a group subscriber all the channels owned by the subscriber are empty) the future will not complete until a new message is published to the topic or channel. This behaviour can be changed so that is a topic or owned channels are empty, the future will complete with a null element value. This is controlled by creating the subscriber with the CompleteOnEmpty option. For example, to create a subscriber where calls to receive return even if the topic is empty: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); Element&lt;String&gt; element = future.get(); The subscriber is created using the CompleteOnEmpty.enabled() option, so it will complete futures even if the topic is empty. The call to future.get() may return null if the topic or owned channels are empty. Multiple calls to Receive and Message Ordering Because the subscriber API is asynchronous, multiple consecutive calls can be made to the receive methods, without waiting for the first call to complete.To maintain message delivery order, the subscriber will complete the futures in the order that the calls were made. Important Any use of the CompletableFuture async API (for example future.thenApplyAsync() , future.handleAsync() etc) to hand of completion handling to another thread will then remove any ordering guarantees for message processing. The same applies to application code that manually hands the returned elements off to other worker threads for processing. It is up to the application code to then handle the futures in such a way that ordering is maintained if that is important to the application&#8217;s use-case. The use of the synchronous CompletableFuture API (for example future.thenApply() , future.handle() etc.) will cause completion of other futures by the subscriber to block until the handler code is complete.To maintain order of completion, the subscribe queues up the futures to be completed by a single daemon thread. For examples: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Void&gt; futureOne = subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); CompletableFuture&lt;Void&gt; futureTwo = subscriber.receive() .thenAccept(element -&gt; { // handle second element... }); In the example above, the code that handles the first element must fully complete before the second future will complete. In use cases where order of processing on the client is not important the full async API can be used. Important Another important aspect of using the async API with subscribers is correct error handling. This is bad code: <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); If the call to receive() fails and the future completes exceptionally, or the handler code in the thenAccept call fails and throws an exception, those exceptions will be lost and not even logged. A better way is to always finish with a handle call or use one of the other methods of the CompletableFuture API to check for exceptions. <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // process second element... }).handle((_void, error) -&gt; { if (error != null) { // something went wrong!!! } return null; }); Committing (Message Acknowledgement) In order to provide at least once delivery guarantees, the subscriber API has methods that allow messages to be committed, so that the server knows they have been processed and will not re-deliver them in the case where a group subscriber fails over or is closed, and a new subscriber in the group takes over the channel ownership. When a subscriber does a commit, it is actually committing a position in a channel of a topic.It effectively says that a specific position in a channel and all earlier positions have been processed.For example if a subscriber reads 10 messages from positions 0 - 9 and commits position 9, then positions 0 - 8 are also committed. There are two ways to commit a position; either using the commit method on an Element returned from a call to receive() , or by calling the commit method on a Subscriber that takes a channel and Position argument. Commit a Received Element The Element returned from a receive call has a commit() method that can be used to commit the element&#8217;s channel and position. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value CommitResult result = element.commit(); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The commit method is called to commit the position of the element. By committing the element directly, application code does not need to track the channel or positions of received elements. Commit a Position To commit a Position in a channel directly the Subscriber.commit(int, Position) method can be used. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value int channel = element.getChannel(); Position position = element.commit(); CommitResult result = subscriber.commit(channel, position); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The channel and Position can be obtained for the element The channel and Position can then be committed later by calling commit on the subscriber Seeking - Reposition a Subscriber The common behaviour for a subscriber is to connect and then receive messages in order until all the messages are processed.Sometimes though it is desirable to rewind a subscriber to reprocess previously consumed messages, or to move a subscriber forwards to skip messages. When rewinding a position, whether the action is successful or not depends on how the topic has been configured.If the topic is configured to retain messages (not the default) then previously received messages are still available and can be re-received.For topics that do not retain messages, then messages are removed once all connected subscribers, or subscriber groups, have read the message.In the case of non-retained topics therefore, it may not be possible to rewind as the messages may have been removed.Even in topics that retain consumed messages, the messages may have been removed if the topic is configured with message expiry. If an attempt is made to rewind further back than the first message in the topic, the seek will reposition the subscriber just before the first available message.If an attempt is made to reposition a subscriber much further ahead than the current tail of the topic, the subscriber will be positioned at the tail, so that it receives the next published message. Seek to a Position The subscriber has a seek method that takes a channel, and a Position that moves the subscriber to the specified position in the channel. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Element&lt;String&gt; firstElement = subscriber.receive().get(); for (int i = 0; i &lt; 10; i++) { Element&lt;String&gt; element = subscriber.receive().get(); // process element... } subscriber.seek(firstElement.getChannel(), firstElement.getPosition()); The example above is a bit contrived, but shows how seek can be used.The first element is received from the topic. Another 10 elements are then processed from the subscriber. The seek method is then used to move the subscriber back to the position of the first message. When seeking, the next message received is the message after the seek position.In the example above, after the seek call the next message received wil not be the same as first element, it will be the next message, so the first message received in the for loop. The subscriber also has methods to seek to the head (re-read the first message) or tail (read the next message published) for a channel without needing to know the head or tail positions. Seek to a Timestamp Subscribers can also be repositioned to the next message based on a timestamp that the message was published. All messages have a timestamp based on the Coherence cluster time in the storage member that accepted the published message. When seeking using a timestamp, the subscriber is repositioned such that the next message received is the first message after the specified timestamp. The timestamp is specified as a java.time.Instant when seeking to a timestamp. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Instant timestamp = LocalDateTime.of(LocalDate.now(), LocalTime.of(20, 30)) .toInstant(ZoneOffset.UTC); Position position = subscriber.seek(1, timestamp); A java.time.Instant is created for 20:30 today. Seek is called to reposition the subscriber so that the next message received from channel 1 will be the first message published after 20:30. Repositioning to a timestamp in the future will reposition the subscriber at the tail, so the next message received will be the next published message, regardless of the time. It is not possible to seek to a timestamp in the future so that messages are ignored until the time is reached. ",
            "title": "Subscribers"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Classes Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " In this example you will run a test that will demonstrate using Durable Events. The test does the following: Starts 2 Cache Servers using Oracle Bedrock Creates and registers a version aware MapListener Inserts, updates and deletes cache entries Simulates the client being disconnected Issues cache mutations remotely while the client is disconnected Reconnects the client and validate that events generated while the client was disconnected are received To enable Durable Events you must have the following system properties set for cache servers: Enable active persistence by using -Dcoherence.distributed.persistence.mode=active Set the directory to store Durable Events using -Dcoherence.distributed.persistence.events.dir=/my/events/dir Optionally set the directory to store active persistence using -Dcoherence.distributed.persistence.base.dir=/my/persistence/dir Register a versioned MapListener on a NamedMap If you do not set the directory to store active persistence the default directory coherence off the users home directory will be chosen. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Review the Customer class This example uses the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private long id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review how the 2 cache servers are started by Oracle Bedrock <markup lang=\"java\" >/** * Startup 2 cache servers using Oracle Bedrock. * * @throws IOException if any errors creating temporary directory */ @BeforeAll public static void startup() throws IOException { persistenceDir = FileHelper.createTempDir(); String path = persistenceDir.getAbsolutePath(); LocalPlatform platform = LocalPlatform.get(); props = new Properties(); props.put(\"coherence.distributed.partitions\", \"23\"); props.put(\"coherence.distributed.persistence.mode\", \"active\"); props.put(\"coherence.distributed.persistence.base.dir\", path); props.put(\"coherence.distributed.persistence.events.dir\", path + FILE_SEP + \"events\"); OptionsByType optionsByType = OptionsByType.empty(); optionsByType.addAll(LocalStorage.enabled(), Multicast.ttl(0), Logging.at(2)); // add the properties to the Bedrock startup props.forEach((k,v) -&gt; optionsByType.add(SystemProperty.of((String) k, (String) v))); OptionsByType optionsByTypeMember1 = OptionsByType.of(optionsByType).add(RoleName.of(\"member1\")); OptionsByType optionsByTypeMember2 = OptionsByType.of(optionsByType).add(RoleName.of(\"member2\")); member1 = platform.launch(CoherenceCacheServer.class, optionsByTypeMember1.asArray()); member2 = platform.launch(CoherenceCacheServer.class, optionsByTypeMember2.asArray()); Eventually.assertThat(invoking(member1).getClusterSize(), CoreMatchers.is(2)); } Set the partition count to 23 to reduce the startup time Set active persistence mode Set the base directory to store persistence files Set the base directory to store persistence events Review the DurableEventsTest class <markup lang=\"java\" >/** * Runs a test to simulate a client registering a versioned {@link MapListener}, * being disconnected, reconnecting, and then receiving all the events that were * missed while the client was disconnected. */ @Test public void testDurableEvents() { try { final AtomicInteger eventCount = new AtomicInteger(); final String CACHE_NAME = \"customers\"; System.getProperties().putAll(props); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"3\"); Coherence coherence = Coherence.clusterMember(); coherence.start().join(); NamedMap&lt;Long, Customer&gt; customers = coherence.getSession().getMap(CACHE_NAME); MapListener&lt;Long, Customer&gt; mapListener = new SimpleMapListener&lt;Long, Customer&gt;() .addEventHandler(System.out::println) .addEventHandler((e) -&gt; eventCount.incrementAndGet()) .versioned(); customers.addMapListener(mapListener); Logger.info(\"Added Map Listener, generating 3 events\"); // generate 3 events, insert, update and delete Customer customer = new Customer(100L, \"Customer 100\", \"Address\", Customer.GOLD, 5000); customers.put(customer.getId(), customer); customers.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customers.remove(100L); // wait until we receive first three events Eventually.assertDeferred(eventCount::get, is(3)); // cause a service distribution for PartitionedCache service to simulate disc Logger.info(\"Disconnecting client\"); causeServiceDisruption(customers); Logger.info(\"Remotely insert, update and delete a new customer\"); // do a remote invocation to insert, update and delete a customer. This is done // remotely via Oracle Bedrock as not to reconnect the client member2.invoke(() -&gt; { NamedMap&lt;Long, Customer&gt; customerMap = CacheFactory.getCache(CACHE_NAME); Customer newCustomer = new Customer(100L, \"Customer 101\", \"Customer address\", Customer.SILVER, 100); customerMap.put(newCustomer.getId(), newCustomer); customerMap.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customerMap.remove(100L); return null; }); // Events should still only be 3 as client has not yet reconnected Eventually.assertDeferred(eventCount::get, is(3)); Logger.info(\"Issuing size to reconnect client\"); // issue an operation that will cause a service restart and listener to be re-registered customers.size(); // we should now see the 3 events we missed because we were disconnected Eventually.assertDeferred(eventCount::get, is(6)); } finally { Coherence coherence = Coherence.getInstance(); coherence.close(); } } Set system properties for the client Create a new SimpleMapListener Add an event handler to output the events received Add an event handler to increment the number of events received Indicate that this MapListener is versioned Add the MapListener to the NamedMap Simulate the client being disconnected by stopping the service for the NamedMap Generate 3 new events remotely on one of the members Issue an operator that will cause the client to restart and re-register the listener Assert that we now see the additional 3 events that were generated while the client was disconnected ",
            "title": "Review the Classes"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " You can run the test in one of three ways: Using your IDE to run DurableEventsTest class Using Maven via ./mvnw clean verify Using Gradle via ./gradlew test After initial cache server startup, you will see output similar to the following: Timestamps have been removed and output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=3): Added Map Listener, generating 3 events ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, partition=20, version=1} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, new value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=2} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=3} &lt;Info&gt; (thread=main, member=3): Disconnecting client &lt;Info&gt; (thread=main, member=3): Remotely insert, update and delete a new customer &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache left the cluster &lt;Info&gt; (thread=main, member=3): Issuing size to reconnect client &lt;Info&gt; (thread=main, member=3): Restarting NamedCache: customers &lt;Info&gt; (thread=main, member=3): Restarting Service: PartitionedCache &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache joined the cluster with senior service member 1 ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, partition=20, version=4} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, new value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=5} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=6} Adding the versioned SimpleMapListener Output of three events while the client is connected Message indicating we are disconnecting client Service for the client leaving as it is disconnected Restarting the cache and service due to size() request which will also automatically re-register the MapListener Client now receives the events it missed during disconnect ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " In this example you ran a test that demonstrated using Durable Events by: Starting 2 Cache Servers using Oracle Bedrock Creating and registering a version aware MapListener Inserting, updating and deleting cache entries Simulating the client being disconnected Issuing cache mutations remotely while the client is disconnected Reconnecting the client and validate that events generated while the client was disconnected are received ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Durable Events Overview Client Events Develop Applications using Map Events ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Coherence provides the MapListener interface as described in Client Events , where clients can sign up for events from any Coherence NamedMap . With traditional client events, if a client disconnects for any reason and then reconnects and automatically re-registers a MapListener , it will miss any events that were sent during that disconnected time. Durable Events is a new (experimental) feature that allows clients to create a versioned listener which will allow a client, if disconnected, to receive events missed while they were in a disconnected state. As for standard `MapListener`s you are able to register for all events, events based upon a filter or events for a specific key. More advanced use cases for Durable Events include the ability to replay all events for a NamedMap . Please see Durable Events Documentation for more information on Durable Events. Durable events are an experimental feature only and should not be used in product as yet. Durable Events are not yet supported for Coherence*Extend clients. Table of Contents What You Will Build What You Need Building the Example Code Review the Classes Run the Examples Summary See Also What You Will Build In this example you will run a test that will demonstrate using Durable Events. The test does the following: Starts 2 Cache Servers using Oracle Bedrock Creates and registers a version aware MapListener Inserts, updates and deletes cache entries Simulates the client being disconnected Issues cache mutations remotely while the client is disconnected Reconnects the client and validate that events generated while the client was disconnected are received To enable Durable Events you must have the following system properties set for cache servers: Enable active persistence by using -Dcoherence.distributed.persistence.mode=active Set the directory to store Durable Events using -Dcoherence.distributed.persistence.events.dir=/my/events/dir Optionally set the directory to store active persistence using -Dcoherence.distributed.persistence.base.dir=/my/persistence/dir Register a versioned MapListener on a NamedMap If you do not set the directory to store active persistence the default directory coherence off the users home directory will be chosen. What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. Review the Classes Review the Customer class This example uses the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private long id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review how the 2 cache servers are started by Oracle Bedrock <markup lang=\"java\" >/** * Startup 2 cache servers using Oracle Bedrock. * * @throws IOException if any errors creating temporary directory */ @BeforeAll public static void startup() throws IOException { persistenceDir = FileHelper.createTempDir(); String path = persistenceDir.getAbsolutePath(); LocalPlatform platform = LocalPlatform.get(); props = new Properties(); props.put(\"coherence.distributed.partitions\", \"23\"); props.put(\"coherence.distributed.persistence.mode\", \"active\"); props.put(\"coherence.distributed.persistence.base.dir\", path); props.put(\"coherence.distributed.persistence.events.dir\", path + FILE_SEP + \"events\"); OptionsByType optionsByType = OptionsByType.empty(); optionsByType.addAll(LocalStorage.enabled(), Multicast.ttl(0), Logging.at(2)); // add the properties to the Bedrock startup props.forEach((k,v) -&gt; optionsByType.add(SystemProperty.of((String) k, (String) v))); OptionsByType optionsByTypeMember1 = OptionsByType.of(optionsByType).add(RoleName.of(\"member1\")); OptionsByType optionsByTypeMember2 = OptionsByType.of(optionsByType).add(RoleName.of(\"member2\")); member1 = platform.launch(CoherenceCacheServer.class, optionsByTypeMember1.asArray()); member2 = platform.launch(CoherenceCacheServer.class, optionsByTypeMember2.asArray()); Eventually.assertThat(invoking(member1).getClusterSize(), CoreMatchers.is(2)); } Set the partition count to 23 to reduce the startup time Set active persistence mode Set the base directory to store persistence files Set the base directory to store persistence events Review the DurableEventsTest class <markup lang=\"java\" >/** * Runs a test to simulate a client registering a versioned {@link MapListener}, * being disconnected, reconnecting, and then receiving all the events that were * missed while the client was disconnected. */ @Test public void testDurableEvents() { try { final AtomicInteger eventCount = new AtomicInteger(); final String CACHE_NAME = \"customers\"; System.getProperties().putAll(props); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"3\"); Coherence coherence = Coherence.clusterMember(); coherence.start().join(); NamedMap&lt;Long, Customer&gt; customers = coherence.getSession().getMap(CACHE_NAME); MapListener&lt;Long, Customer&gt; mapListener = new SimpleMapListener&lt;Long, Customer&gt;() .addEventHandler(System.out::println) .addEventHandler((e) -&gt; eventCount.incrementAndGet()) .versioned(); customers.addMapListener(mapListener); Logger.info(\"Added Map Listener, generating 3 events\"); // generate 3 events, insert, update and delete Customer customer = new Customer(100L, \"Customer 100\", \"Address\", Customer.GOLD, 5000); customers.put(customer.getId(), customer); customers.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customers.remove(100L); // wait until we receive first three events Eventually.assertDeferred(eventCount::get, is(3)); // cause a service distribution for PartitionedCache service to simulate disc Logger.info(\"Disconnecting client\"); causeServiceDisruption(customers); Logger.info(\"Remotely insert, update and delete a new customer\"); // do a remote invocation to insert, update and delete a customer. This is done // remotely via Oracle Bedrock as not to reconnect the client member2.invoke(() -&gt; { NamedMap&lt;Long, Customer&gt; customerMap = CacheFactory.getCache(CACHE_NAME); Customer newCustomer = new Customer(100L, \"Customer 101\", \"Customer address\", Customer.SILVER, 100); customerMap.put(newCustomer.getId(), newCustomer); customerMap.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customerMap.remove(100L); return null; }); // Events should still only be 3 as client has not yet reconnected Eventually.assertDeferred(eventCount::get, is(3)); Logger.info(\"Issuing size to reconnect client\"); // issue an operation that will cause a service restart and listener to be re-registered customers.size(); // we should now see the 3 events we missed because we were disconnected Eventually.assertDeferred(eventCount::get, is(6)); } finally { Coherence coherence = Coherence.getInstance(); coherence.close(); } } Set system properties for the client Create a new SimpleMapListener Add an event handler to output the events received Add an event handler to increment the number of events received Indicate that this MapListener is versioned Add the MapListener to the NamedMap Simulate the client being disconnected by stopping the service for the NamedMap Generate 3 new events remotely on one of the members Issue an operator that will cause the client to restart and re-register the listener Assert that we now see the additional 3 events that were generated while the client was disconnected Run the Examples You can run the test in one of three ways: Using your IDE to run DurableEventsTest class Using Maven via ./mvnw clean verify Using Gradle via ./gradlew test After initial cache server startup, you will see output similar to the following: Timestamps have been removed and output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=3): Added Map Listener, generating 3 events ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, partition=20, version=1} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, new value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=2} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=3} &lt;Info&gt; (thread=main, member=3): Disconnecting client &lt;Info&gt; (thread=main, member=3): Remotely insert, update and delete a new customer &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache left the cluster &lt;Info&gt; (thread=main, member=3): Issuing size to reconnect client &lt;Info&gt; (thread=main, member=3): Restarting NamedCache: customers &lt;Info&gt; (thread=main, member=3): Restarting Service: PartitionedCache &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache joined the cluster with senior service member 1 ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, partition=20, version=4} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, new value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=5} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=6} Adding the versioned SimpleMapListener Output of three events while the client is connected Message indicating we are disconnecting client Service for the client leaving as it is disconnected Restarting the cache and service due to size() request which will also automatically re-register the MapListener Client now receives the events it missed during disconnect Summary In this example you ran a test that demonstrated using Durable Events by: Starting 2 Cache Servers using Oracle Bedrock Creating and registering a version aware MapListener Inserting, updating and deleting cache entries Simulating the client being disconnected Issuing cache mutations remotely while the client is disconnected Reconnecting the client and validate that events generated while the client was disconnected are received See Also Durable Events Overview Client Events Develop Applications using Map Events ",
            "title": "Durable Events"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units and the back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays CacheMBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays CacheMBean metrics for the front cache to show cache pruning happening Displays StorageManagerMBean metrics to show listener registrations There are two tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { final int MAX = 100; // Create the Coherence instance from the configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.create(\"near-cache-config.xml\")) .build(); Coherence coherence = Coherence.clusterMember(cfg); coherence.start().join(); // retrieve a session Session session = coherence.getSession(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up Base.sleep(3000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 3 seconds to ensure JMX stats are up to date Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Run the examples using one of the test classes below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured eviction policies Explored MBeans related to near caching ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This guide walks you through how to use near caching within Coherence by providing various examples and configurations that showcase the different features available. A near cache is a hybrid cache; it typically fronts a distributed cache or a remote cache with a local cache. Near cache invalidates front cache entries, using a configured invalidation strategy, and provides excellent performance and synchronization. Near cache backed by a partitioned cache offers zero-millisecond local access for repeat data access, while enabling concurrency and ensuring coherency and fail over, effectively combining the best attributes of replicated and partitioned caches. See the Coherence Documentation for detailed information on near caches. Table of Contents What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Example Code The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units and the back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays CacheMBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays CacheMBean metrics for the front cache to show cache pruning happening Displays StorageManagerMBean metrics to show listener registrations There are two tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { final int MAX = 100; // Create the Coherence instance from the configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.create(\"near-cache-config.xml\")) .build(); Coherence coherence = Coherence.clusterMember(cfg); coherence.start().join(); // retrieve a session Session session = coherence.getSession(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up Base.sleep(3000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 3 seconds to ensure JMX stats are up to date Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics Review the Tests The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. Run the Examples Run the examples using one of the test classes below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations Summary In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured eviction policies Explored MBeans related to near caching See Also Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "Near Caching"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Classes Run the Example Show Usage Start 4 cache servers Start example Kill a cache server Kill more cache servers Experiment with other connection options Packaging the example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build This example can be built via Maven only. It is not supported to be run via Gradle. You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " In this example you will build and run a utility allowing you to monitor StatusHA values for Coherence services. At its core, this example uses the ServiceMBean as described in the Coherence documentation . In particular the following attributes are queried: StatusHA - The High Availability (HA) status for this service. A value of MACHINE-SAFE indicates that all the cluster members running on any given computer could be stopped without data loss. A value of NODE-SAFE indicates that a cluster member could be stopped without data loss. A value of ENDANGERED indicates that abnormal termination of any cluster member that runs this service may cause data loss. A value of N/A indicates that the service has no high availability impact. StorageEnabledCount - Specifies the total number of cluster members running this service for which local storage is enabled PartitionsUnbalanced - The total number of primary and backup partitions that remain to be transferred until the partition distribution across the storage enabled service members is fully balanced PartitionsVulnerable - The total number of partitions that are backed up on the same machine where the primary partition owner resides PartitionsEndangered - The total number of partitions that are not currently backed up PartitionsAll - The total number of partitions that every cache storage is divided into The utility can connect to a Coherence cluster and query the MBeans using the following methods: Use MBeanServer from a cluster. Requires correct cluster config to join cluster. (Default) Use JMX URL to connect to a cluster Use a host and port to connect to a remote JMX process Use Management over REST to connect to a cluster via HTTP Continue on to review the example code or go directly here to run the example. What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build This example can be built via Maven only. It is not supported to be run via Gradle. You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " The example consists of the following main classes: StatusHAWatcher - Main entrypoint to parse arguments and run the example ServiceData - Data structure to hold the service information queried from the cluster DataFetcher - Interface implemented by various classes to retrieve ServiceMBean details MBeanServerProxyDataFetcher - Implementation to retrieve data from MBeanServerProxy JMXDataFetcher - Implementation to retrieve data from a remote or local JMX connection HTTPDataFetcher - Implementation to retrieve data from a Management over REST connection from either standalone cluster or WebLogic Server Review the ServiceData class This class contains the following fields: <markup lang=\"java\" >/** * The service name. */ private final String serviceName; /** * The number of storage-enabled members for this service. */ private final int storageCount; /** * The StatusHA value for the service. */ private final String statusHA; /** * The total number of partitions in this service. */ private final int partitionCount; /** * The number of partitions that are vulnerable, e.g. backed up on the same machine. */ private final int partitionsVulnerable; /** * The number of partitions yet to be balanced. */ private final int partitionsUnbalanced; /** * The number of partitions that do not have a backup. */ private final int partitionsEndangered; Review the DataFetcher interface This interface defines the following methods which are used to retrieve ServiceMBean attributes via different methods <markup lang=\"java\" >public interface DataFetcher { /** * Returns the cluster name. * * @return the cluster name */ String getClusterName(); /** * Returns the cluster version. * * @return the cluster version. */ String getClusterVersion(); /** * Returns the {@link ServiceData}. * * @return the {@link ServiceData} */ Set&lt;ServiceData&gt; getStatusHaData(); /** * Returns the {@link Set} of service names that are partitioned services. * * @return the {@link Set} of service names that are partitioned services */ Set&lt;String&gt; getServiceNames(); } Review the MBeanServerProxyDataFetcher class This class is an implementation of the DataFetcher interface to retrieve data from MBeanServerProxy . Constructor <markup lang=\"java\" >public MBeanServerProxyDataFetcher(String serviceName) { super(serviceName); // be as quiet as we can System.setProperty(\"coherence.log.level\", \"1\"); Registry registry = CacheFactory.ensureCluster().getManagement(); if (registry == null) { throw new RuntimeException(\"Unable to get registry from cluster\"); } proxy = registry.getMBeanServerProxy(); if (proxy == null) { throw new RuntimeException(\"Unable to get MBeanServerProxy\"); } Join the cluster and retrieve Management . The correct cluster operational override must be supplied to connect to the cluster. Retrieve the MBeanServerProxy instance getStatusHaData method <markup lang=\"java\" >@Override public Set&lt;ServiceData&gt; getStatusHaData() { Set&lt;ServiceData&gt; setData = new HashSet&lt;&gt;(); getMBeans().forEach(bean -&gt; { String sServiceName = extractService(bean); // retrieve values from one node as all of them will have the same values Optional&lt;String&gt; serviceMBean = proxy.queryNames(COHERENCE + Registry.SERVICE_TYPE + \",name=\" + sServiceName + \",*\", null) .stream().findAny(); if (!serviceMBean.isPresent()) { throw new RuntimeException(\"Unable to find ServiceMBean for service \" + sServiceName); } String sServiceMbean = serviceMBean.get(); Map&lt;String, Object&gt; mapServiceAttr = proxy.getAttributes(sServiceMbean, Filters.always()); String sStatusHA = getSafeStatusHA((String) mapServiceAttr.get(ATTR_STATUS_HA)); int nPartitionCount = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITION_COUNT).toString()); int nStorageCount = Integer.parseInt(mapServiceAttr.get(ATTR_STORAGE_ENABLED_COUNT).toString()); int nVulnerable = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITIONS_VULNERABLE).toString()); int nUnbalanced = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITIONS_UNBALANCED).toString()); int nEndangered = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITIONS_ENDANGERED).toString()); setData.add(new ServiceData(sServiceName, nStorageCount, sStatusHA, nPartitionCount, nVulnerable, nUnbalanced, nEndangered)); }); return setData; } Call getMBeans() to query the distribution coordinator MBean to retrieve all partitioned services Use MBeanServerProxy to get any ServiceMBean for the given service Retrieve all attributes from the ServiceMBean Add a new ServiceData instance to the set to return Review the JMXDataFetcher class This class is an implementation of the DataFetcher interface to retrieve data from a JMX connection. Constructor <markup lang=\"java\" >public JMXDataFetcher(String jmxConnectionURL, String serviceName) { super(serviceName); try { JMXConnector connect = JMXConnectorFactory.connect(new JMXServiceURL(jmxConnectionURL)); mbs = connect.getMBeanServerConnection(); } catch (Exception e) { throw new RuntimeException(\"Unable to connect to JMX Url \" + jmxConnectionURL, e); } Retrieve an JMXConnector from the given JMX URL Retrieve an MBeanServerConnection from the JMXConnector getStatusHaData method <markup lang=\"java\" >@Override public Set&lt;ServiceData&gt; getStatusHaData() { Set&lt;ServiceData&gt; setData = new HashSet&lt;&gt;(); getMBeans().forEach(bean -&gt; { String serviceName = extractService(bean); AttributeList listServiceAttrs; String sQuery = COHERENCE + Registry.SERVICE_TYPE + \",name=\" + serviceName + \",*\"; try { Set&lt;ObjectName&gt; setServices = mbs.queryNames(new ObjectName(sQuery), null); if (setServices.size() == 0) { throw new RuntimeException(\"Cannot query for service \" + serviceName); } String mbean = setServices.stream().findAny().map(ObjectName::toString).get(); listServiceAttrs = mbs.getAttributes(new ObjectName(mbean), new String[]{ ATTR_PARTITIONS_VULNERABLE, ATTR_PARTITIONS_UNBALANCED, ATTR_PARTITIONS_ENDANGERED, ATTR_STATUS_HA, ATTR_STORAGE_ENABLED_COUNT, ATTR_PARTITION_COUNT }); } catch (Exception e) { throw new RuntimeException(\"Unable to find attributes for \" + sQuery, e); } int partitionCount = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITION_COUNT)); int storageCount = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_STORAGE_ENABLED_COUNT)); String statusHA = getSafeStatusHA(getAttributeValue(listServiceAttrs, ATTR_STATUS_HA)); int vulnerable = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITIONS_VULNERABLE)); int unbalanced = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITIONS_UNBALANCED)); int endangered = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITIONS_ENDANGERED)); setData.add(new ServiceData(serviceName, storageCount, statusHA, partitionCount, vulnerable, unbalanced, endangered)); }); return setData; } Query the MBeanServerConnection to get any ServiceMBean for the given service Retrieve all required attributes Review the HTTPDataFetcher class This class is an implementation of the DataFetcher interface to retrieve data from a Management over REST connection. Constructor <markup lang=\"java\" >public HTTPDataFetcher(String url, String serviceName) { super(serviceName); if (url == null) { throw new IllegalArgumentException(\"Http URL must not be null\"); } httpUrl = url; // Managed Coherence Servers URL http://&lt;admin-host&gt;:&lt;admin-port&gt;/management/coherence/&lt;version&gt;/clusters isWebLogic = httpUrl.contains(\"/management/coherence/\") &amp;&amp; httpUrl.contains(\"clusters\"); if (isWebLogic) { System.out.println(\"Enter basic authentication information for WebLogic Server connection\"); System.out.print(\"Enter username: \"); Console console = System.console(); String username = console.readLine(); System.out.print(\"Enter password. (will not be displayed): \"); char[] password= console.readPassword(); if (username == null || password.length == 0) { throw new RuntimeException(\"Please enter username and password\"); } String authentication = username + \":\" + new String(password); byte[] encodedData = Base64.getEncoder().encode(authentication.getBytes(StandardCharsets.UTF_8)); basicAuth = \"Basic \" + new String(encodedData); } If the URL is for a WebLogic Server connection, prompt for username/ password getMBeans method This method constructs a HTTP Request to retrieve the data from Management over REST endpoint. <markup lang=\"java\" >private JsonNode getMBeans(String serviceName) { try { URLBuilder builder = new URLBuilder(httpUrl); if (isWebLogic) { // WebLogic Server requires the cluster name as a path segement builder.addPathSegment(getClusterName()); } builder.addPathSegment(\"services\"); if (serviceName != null) { builder = builder.addPathSegment(serviceName.replaceAll(\"\\\"\", \"\")); } builder = builder .addPathSegment(\"members\") .addQueryParameter(\"fields\", \"name,type,statusHA,partitionsAll,partitionsEndangered,\" + \"partitionsVulnerable,partitionsUnbalanced,storageEnabledCount,requestPendingCount,outgoingTransferCount\") .addQueryParameter(\"links\", \"\"); JsonNode rootNode = getResponse(builder); return isWebLogic ? rootNode : rootNode.get(\"items\"); } catch (Exception e) { throw new RuntimeException(\"Unable to get service info from \" + httpUrl, e); } } ",
            "title": "Review the Classes"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " The supported way to run this example is to build using Maven as described here and running using java -jar target/status-ha-{version}.jar from a terminal in the base directory of this example: examples/guides/500-status-ha . Firstly, issue the command with the -u option which displays the usage. <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h Usage: StatusHAWatcher [options] Connection options: -m Use MBeanServer from cluster. Requires correct cluster config to join cluster. (Default) -h url Use Management over REST to connect to cluster -j url Use JMX URL to connect to cluster -hp host:port Connect to a JMX process via host:port Other Options: -d delay Delay between each check in seconds -s service Service name to monitor or all services if not specified -u Display usage StatusHA meanings: ENDANGERED - abnormal termination of any cluster node that runs this service may cause data loss NODE-SAFE - any single cluster node could be stopped without data loss MACHINE-SAFE - all the cluster nodes running on any given machine could be stopped at once without data loss RACK-SAFE - all the cluster nodes running on any given rack could be stopped at once without data loss SITE-SAFE - all the cluster nodes running on any given rack could be stopped at once without data loss Partition meanings: Endangered - The total number of partitions that are not currently backed up Vulnerable - The total number of partitions that are backed up on the same machine where the primary partition owner resides Unbalanced - The total number of primary and backup partitions which remain to be transferred until the partition distribution across the storage enabled service members is fully balanced Remaining - The number of partition transfers that remain to be completed before the service achieves the goals set by this strategy To test the utility we will start some cache servers. ",
            "title": "Show Usage"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " If you want to connect using the default option, MBeanServer connection, you must ensure you build the example with the same Coherence version of the cluster you are going to connect to. Change to the `examples/guides/500-status-ha/target/libs directory Issue the following command in four terminals to start 4 DefaultCacheServer processes. On two of the processes use -Dcoherence.machine=machine1 and on the other two use -Dcoherence.machine=machine2 to simulate processes running on separate physical servers. <markup lang=\"bash\" >java -Dcoherence.machine=machine1 -jar coherence-{version}.jar ",
            "title": "Start 4 cache servers"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " When the cache servers have started, ensure you are in the examples/guides/500-status-ha directory and run the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar Notes: By default, the MBeans will be queried every 5 seconds. You can change this by using the -d option and specify a delay in seconds. All services are queried. You can select only a single service to be monitored by using -s option and specifying a service name. You will see output similar to the following showing the status HA values for the cluster services. <markup lang=\"bash\" >Connection: Cluster MBean Server Service: all Delay: 5 seconds Oracle Coherence Version 21.06.1 Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. Cluster Name: timmiddleton's cluster (21.06.1) Press CTRL-C to quit Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:27:13 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:13 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:18 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:18 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:23 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:23 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe You will notice that the StatusHA of all services is MACHINE-SAFE as there are an even number of cache servers on each \"machine\". ",
            "title": "Start the example"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " Kill one of the cache server processes using CTRL-C and note the change in the output of the example: <markup lang=\"bash\" >Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:29:39 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 257 170 170 partitions are unbalanced Tue Aug 03 11:29:39 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:45 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:45 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:50 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:50 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable You will notice that the StatusHA values are now node safe as there are not enough servers on each machine to provide a higher level of safety. ",
            "title": "Kill a cache server"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " Kill all but one cache server, and you will notice the StatusHA value is ENDANGERED as there is only one cache server with no backups available on other cache servers. <markup lang=\"bash\" >Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:33:14 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:14 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:19 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:19 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:24 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:24 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Start the remaining cache servers, and you will see the StatusHA return to MACHINE-SAFE. ",
            "title": "Kill more cache servers"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " Other connection options are available which do not require you to have the same Coherence version as the example. Connect via JMX to a Host/Port If you have a Coherence MBean server running on a host/port you can connect to the cluster using the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -hp host:port Connect via JMX to a JMX URL <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -j service:jmx:rmi:///jndi/rmi://localhost:8888/jmxrmi Connect via Management over REST If you have a stand-alone Coherence cluster with Management over REST enabled, use the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h http://host:management-port/management/coherence/cluster Connect via Management over REST to WebLogic Server If you have a stand-alone Coherence cluster within WebLogic Server, use the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h http://host:admin-port/management/coherence/latest/clusters ",
            "title": "Experiment with other connection options"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " When the example is built, the following artifacts are created: target/status-ha-{version}.jar - executable jar with META-INF/MANIFEST.MF adding libs directory contents to classpath target/libs/coherence-{version}.jar - Coherence version the example was built with target/libs/jackson-annotations-2.12.0.jar - required dependencies target/libs/jackson-core-2.12.0.jar - required dependencies target/libs/jackson-databind-2.12.0.jar - required dependencies If you wish to take the example and run it on a separate machine, create a temporary directory and carry out the following: We are using a temporary directory /tmp/build as our example. Modify as you need. <markup lang=\"bash\" >cp target/status-ha-{version}.jar /tmp/build mkdir /tmp/build/libs cp target/libs/jackson-annotations-2.12.0.jar /tmp/build/libs cp target/libs/jackson-core-2.12.0.jar /tmp/build/libs cp target/libs/jackson-databind-2.12.0.jar /tmp/build/libs You can then change to the /tmp/build directory and run the example using: <markup lang=\"bash\" >cd /tmp/build java -jar status-ha-{version}.jar Zip or Tar the directory up and transfer to your target machine. If you wish to change the Coherence version used to build the example you can set the following system properties: -Dcoherence.version - the coherence version -Dcoherence.group.id - defaults to com.oracle.coherence.ce . Change to com.oracle.coherence for commercial edition. <markup lang=\"bash\" >mvn clean install -DskipTests -Dcoherence.version=14.1.1-0-0 -Dcoherence.groupid=com.oracle.coherence ",
            "title": "Packaging the example"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " Show Usage The supported way to run this example is to build using Maven as described here and running using java -jar target/status-ha-{version}.jar from a terminal in the base directory of this example: examples/guides/500-status-ha . Firstly, issue the command with the -u option which displays the usage. <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h Usage: StatusHAWatcher [options] Connection options: -m Use MBeanServer from cluster. Requires correct cluster config to join cluster. (Default) -h url Use Management over REST to connect to cluster -j url Use JMX URL to connect to cluster -hp host:port Connect to a JMX process via host:port Other Options: -d delay Delay between each check in seconds -s service Service name to monitor or all services if not specified -u Display usage StatusHA meanings: ENDANGERED - abnormal termination of any cluster node that runs this service may cause data loss NODE-SAFE - any single cluster node could be stopped without data loss MACHINE-SAFE - all the cluster nodes running on any given machine could be stopped at once without data loss RACK-SAFE - all the cluster nodes running on any given rack could be stopped at once without data loss SITE-SAFE - all the cluster nodes running on any given rack could be stopped at once without data loss Partition meanings: Endangered - The total number of partitions that are not currently backed up Vulnerable - The total number of partitions that are backed up on the same machine where the primary partition owner resides Unbalanced - The total number of primary and backup partitions which remain to be transferred until the partition distribution across the storage enabled service members is fully balanced Remaining - The number of partition transfers that remain to be completed before the service achieves the goals set by this strategy To test the utility we will start some cache servers. Start 4 cache servers If you want to connect using the default option, MBeanServer connection, you must ensure you build the example with the same Coherence version of the cluster you are going to connect to. Change to the `examples/guides/500-status-ha/target/libs directory Issue the following command in four terminals to start 4 DefaultCacheServer processes. On two of the processes use -Dcoherence.machine=machine1 and on the other two use -Dcoherence.machine=machine2 to simulate processes running on separate physical servers. <markup lang=\"bash\" >java -Dcoherence.machine=machine1 -jar coherence-{version}.jar Start the example When the cache servers have started, ensure you are in the examples/guides/500-status-ha directory and run the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar Notes: By default, the MBeans will be queried every 5 seconds. You can change this by using the -d option and specify a delay in seconds. All services are queried. You can select only a single service to be monitored by using -s option and specifying a service name. You will see output similar to the following showing the status HA values for the cluster services. <markup lang=\"bash\" >Connection: Cluster MBean Server Service: all Delay: 5 seconds Oracle Coherence Version 21.06.1 Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. Cluster Name: timmiddleton's cluster (21.06.1) Press CTRL-C to quit Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:27:13 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:13 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:18 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:18 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:23 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:23 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe You will notice that the StatusHA of all services is MACHINE-SAFE as there are an even number of cache servers on each \"machine\". Kill a cache server Kill one of the cache server processes using CTRL-C and note the change in the output of the example: <markup lang=\"bash\" >Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:29:39 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 257 170 170 partitions are unbalanced Tue Aug 03 11:29:39 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:45 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:45 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:50 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:50 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable You will notice that the StatusHA values are now node safe as there are not enough servers on each machine to provide a higher level of safety. Kill more cache servers Kill all but one cache server, and you will notice the StatusHA value is ENDANGERED as there is only one cache server with no backups available on other cache servers. <markup lang=\"bash\" >Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:33:14 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:14 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:19 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:19 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:24 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:24 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Start the remaining cache servers, and you will see the StatusHA return to MACHINE-SAFE. Experiment with other connection options Other connection options are available which do not require you to have the same Coherence version as the example. Connect via JMX to a Host/Port If you have a Coherence MBean server running on a host/port you can connect to the cluster using the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -hp host:port Connect via JMX to a JMX URL <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -j service:jmx:rmi:///jndi/rmi://localhost:8888/jmxrmi Connect via Management over REST If you have a stand-alone Coherence cluster with Management over REST enabled, use the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h http://host:management-port/management/coherence/cluster Connect via Management over REST to WebLogic Server If you have a stand-alone Coherence cluster within WebLogic Server, use the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h http://host:admin-port/management/coherence/latest/clusters Packaging the example When the example is built, the following artifacts are created: target/status-ha-{version}.jar - executable jar with META-INF/MANIFEST.MF adding libs directory contents to classpath target/libs/coherence-{version}.jar - Coherence version the example was built with target/libs/jackson-annotations-2.12.0.jar - required dependencies target/libs/jackson-core-2.12.0.jar - required dependencies target/libs/jackson-databind-2.12.0.jar - required dependencies If you wish to take the example and run it on a separate machine, create a temporary directory and carry out the following: We are using a temporary directory /tmp/build as our example. Modify as you need. <markup lang=\"bash\" >cp target/status-ha-{version}.jar /tmp/build mkdir /tmp/build/libs cp target/libs/jackson-annotations-2.12.0.jar /tmp/build/libs cp target/libs/jackson-core-2.12.0.jar /tmp/build/libs cp target/libs/jackson-databind-2.12.0.jar /tmp/build/libs You can then change to the /tmp/build directory and run the example using: <markup lang=\"bash\" >cd /tmp/build java -jar status-ha-{version}.jar Zip or Tar the directory up and transfer to your target machine. If you wish to change the Coherence version used to build the example you can set the following system properties: -Dcoherence.version - the coherence version -Dcoherence.group.id - defaults to com.oracle.coherence.ce . Change to com.oracle.coherence for commercial edition. <markup lang=\"bash\" >mvn clean install -DskipTests -Dcoherence.version=14.1.1-0-0 -Dcoherence.groupid=com.oracle.coherence ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " In this example you built and ran a utility allowing you to monitor StatusHA values for Coherence services. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " Coherence MBean Reference Starting and Stopping Cluster Members Using JMX to Manage Oracle Coherence ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/500-status-ha/README",
            "text": " This guide walks you through how to monitor the High Available (HA) Status or StatusHA value for Coherence Partitioned Services within a cluster. StatusHA is most commonly used to ensure services are in a safe state between restarting cache servers during a rolling restart. See the Coherence documentation on Starting and Stopping Cluster Members for more information on rolling redeploys. Table of Contents What You Will Build What You Need Building the Example Code Review the Classes Run the Example Show Usage Start 4 cache servers Start example Kill a cache server Kill more cache servers Experiment with other connection options Packaging the example Summary See Also What You Will Build In this example you will build and run a utility allowing you to monitor StatusHA values for Coherence services. At its core, this example uses the ServiceMBean as described in the Coherence documentation . In particular the following attributes are queried: StatusHA - The High Availability (HA) status for this service. A value of MACHINE-SAFE indicates that all the cluster members running on any given computer could be stopped without data loss. A value of NODE-SAFE indicates that a cluster member could be stopped without data loss. A value of ENDANGERED indicates that abnormal termination of any cluster member that runs this service may cause data loss. A value of N/A indicates that the service has no high availability impact. StorageEnabledCount - Specifies the total number of cluster members running this service for which local storage is enabled PartitionsUnbalanced - The total number of primary and backup partitions that remain to be transferred until the partition distribution across the storage enabled service members is fully balanced PartitionsVulnerable - The total number of partitions that are backed up on the same machine where the primary partition owner resides PartitionsEndangered - The total number of partitions that are not currently backed up PartitionsAll - The total number of partitions that every cache storage is divided into The utility can connect to a Coherence cluster and query the MBeans using the following methods: Use MBeanServer from a cluster. Requires correct cluster config to join cluster. (Default) Use JMX URL to connect to a cluster Use a host and port to connect to a remote JMX process Use Management over REST to connect to a cluster via HTTP Continue on to review the example code or go directly here to run the example. What You Need About 20 minutes A favorite text editor or IDE JDK 1.8 or later Maven 3.5+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build This example can be built via Maven only. It is not supported to be run via Gradle. You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. Review the Classes The example consists of the following main classes: StatusHAWatcher - Main entrypoint to parse arguments and run the example ServiceData - Data structure to hold the service information queried from the cluster DataFetcher - Interface implemented by various classes to retrieve ServiceMBean details MBeanServerProxyDataFetcher - Implementation to retrieve data from MBeanServerProxy JMXDataFetcher - Implementation to retrieve data from a remote or local JMX connection HTTPDataFetcher - Implementation to retrieve data from a Management over REST connection from either standalone cluster or WebLogic Server Review the ServiceData class This class contains the following fields: <markup lang=\"java\" >/** * The service name. */ private final String serviceName; /** * The number of storage-enabled members for this service. */ private final int storageCount; /** * The StatusHA value for the service. */ private final String statusHA; /** * The total number of partitions in this service. */ private final int partitionCount; /** * The number of partitions that are vulnerable, e.g. backed up on the same machine. */ private final int partitionsVulnerable; /** * The number of partitions yet to be balanced. */ private final int partitionsUnbalanced; /** * The number of partitions that do not have a backup. */ private final int partitionsEndangered; Review the DataFetcher interface This interface defines the following methods which are used to retrieve ServiceMBean attributes via different methods <markup lang=\"java\" >public interface DataFetcher { /** * Returns the cluster name. * * @return the cluster name */ String getClusterName(); /** * Returns the cluster version. * * @return the cluster version. */ String getClusterVersion(); /** * Returns the {@link ServiceData}. * * @return the {@link ServiceData} */ Set&lt;ServiceData&gt; getStatusHaData(); /** * Returns the {@link Set} of service names that are partitioned services. * * @return the {@link Set} of service names that are partitioned services */ Set&lt;String&gt; getServiceNames(); } Review the MBeanServerProxyDataFetcher class This class is an implementation of the DataFetcher interface to retrieve data from MBeanServerProxy . Constructor <markup lang=\"java\" >public MBeanServerProxyDataFetcher(String serviceName) { super(serviceName); // be as quiet as we can System.setProperty(\"coherence.log.level\", \"1\"); Registry registry = CacheFactory.ensureCluster().getManagement(); if (registry == null) { throw new RuntimeException(\"Unable to get registry from cluster\"); } proxy = registry.getMBeanServerProxy(); if (proxy == null) { throw new RuntimeException(\"Unable to get MBeanServerProxy\"); } Join the cluster and retrieve Management . The correct cluster operational override must be supplied to connect to the cluster. Retrieve the MBeanServerProxy instance getStatusHaData method <markup lang=\"java\" >@Override public Set&lt;ServiceData&gt; getStatusHaData() { Set&lt;ServiceData&gt; setData = new HashSet&lt;&gt;(); getMBeans().forEach(bean -&gt; { String sServiceName = extractService(bean); // retrieve values from one node as all of them will have the same values Optional&lt;String&gt; serviceMBean = proxy.queryNames(COHERENCE + Registry.SERVICE_TYPE + \",name=\" + sServiceName + \",*\", null) .stream().findAny(); if (!serviceMBean.isPresent()) { throw new RuntimeException(\"Unable to find ServiceMBean for service \" + sServiceName); } String sServiceMbean = serviceMBean.get(); Map&lt;String, Object&gt; mapServiceAttr = proxy.getAttributes(sServiceMbean, Filters.always()); String sStatusHA = getSafeStatusHA((String) mapServiceAttr.get(ATTR_STATUS_HA)); int nPartitionCount = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITION_COUNT).toString()); int nStorageCount = Integer.parseInt(mapServiceAttr.get(ATTR_STORAGE_ENABLED_COUNT).toString()); int nVulnerable = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITIONS_VULNERABLE).toString()); int nUnbalanced = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITIONS_UNBALANCED).toString()); int nEndangered = Integer.parseInt(mapServiceAttr.get(ATTR_PARTITIONS_ENDANGERED).toString()); setData.add(new ServiceData(sServiceName, nStorageCount, sStatusHA, nPartitionCount, nVulnerable, nUnbalanced, nEndangered)); }); return setData; } Call getMBeans() to query the distribution coordinator MBean to retrieve all partitioned services Use MBeanServerProxy to get any ServiceMBean for the given service Retrieve all attributes from the ServiceMBean Add a new ServiceData instance to the set to return Review the JMXDataFetcher class This class is an implementation of the DataFetcher interface to retrieve data from a JMX connection. Constructor <markup lang=\"java\" >public JMXDataFetcher(String jmxConnectionURL, String serviceName) { super(serviceName); try { JMXConnector connect = JMXConnectorFactory.connect(new JMXServiceURL(jmxConnectionURL)); mbs = connect.getMBeanServerConnection(); } catch (Exception e) { throw new RuntimeException(\"Unable to connect to JMX Url \" + jmxConnectionURL, e); } Retrieve an JMXConnector from the given JMX URL Retrieve an MBeanServerConnection from the JMXConnector getStatusHaData method <markup lang=\"java\" >@Override public Set&lt;ServiceData&gt; getStatusHaData() { Set&lt;ServiceData&gt; setData = new HashSet&lt;&gt;(); getMBeans().forEach(bean -&gt; { String serviceName = extractService(bean); AttributeList listServiceAttrs; String sQuery = COHERENCE + Registry.SERVICE_TYPE + \",name=\" + serviceName + \",*\"; try { Set&lt;ObjectName&gt; setServices = mbs.queryNames(new ObjectName(sQuery), null); if (setServices.size() == 0) { throw new RuntimeException(\"Cannot query for service \" + serviceName); } String mbean = setServices.stream().findAny().map(ObjectName::toString).get(); listServiceAttrs = mbs.getAttributes(new ObjectName(mbean), new String[]{ ATTR_PARTITIONS_VULNERABLE, ATTR_PARTITIONS_UNBALANCED, ATTR_PARTITIONS_ENDANGERED, ATTR_STATUS_HA, ATTR_STORAGE_ENABLED_COUNT, ATTR_PARTITION_COUNT }); } catch (Exception e) { throw new RuntimeException(\"Unable to find attributes for \" + sQuery, e); } int partitionCount = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITION_COUNT)); int storageCount = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_STORAGE_ENABLED_COUNT)); String statusHA = getSafeStatusHA(getAttributeValue(listServiceAttrs, ATTR_STATUS_HA)); int vulnerable = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITIONS_VULNERABLE)); int unbalanced = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITIONS_UNBALANCED)); int endangered = Integer.parseInt(getAttributeValue(listServiceAttrs, ATTR_PARTITIONS_ENDANGERED)); setData.add(new ServiceData(serviceName, storageCount, statusHA, partitionCount, vulnerable, unbalanced, endangered)); }); return setData; } Query the MBeanServerConnection to get any ServiceMBean for the given service Retrieve all required attributes Review the HTTPDataFetcher class This class is an implementation of the DataFetcher interface to retrieve data from a Management over REST connection. Constructor <markup lang=\"java\" >public HTTPDataFetcher(String url, String serviceName) { super(serviceName); if (url == null) { throw new IllegalArgumentException(\"Http URL must not be null\"); } httpUrl = url; // Managed Coherence Servers URL http://&lt;admin-host&gt;:&lt;admin-port&gt;/management/coherence/&lt;version&gt;/clusters isWebLogic = httpUrl.contains(\"/management/coherence/\") &amp;&amp; httpUrl.contains(\"clusters\"); if (isWebLogic) { System.out.println(\"Enter basic authentication information for WebLogic Server connection\"); System.out.print(\"Enter username: \"); Console console = System.console(); String username = console.readLine(); System.out.print(\"Enter password. (will not be displayed): \"); char[] password= console.readPassword(); if (username == null || password.length == 0) { throw new RuntimeException(\"Please enter username and password\"); } String authentication = username + \":\" + new String(password); byte[] encodedData = Base64.getEncoder().encode(authentication.getBytes(StandardCharsets.UTF_8)); basicAuth = \"Basic \" + new String(encodedData); } If the URL is for a WebLogic Server connection, prompt for username/ password getMBeans method This method constructs a HTTP Request to retrieve the data from Management over REST endpoint. <markup lang=\"java\" >private JsonNode getMBeans(String serviceName) { try { URLBuilder builder = new URLBuilder(httpUrl); if (isWebLogic) { // WebLogic Server requires the cluster name as a path segement builder.addPathSegment(getClusterName()); } builder.addPathSegment(\"services\"); if (serviceName != null) { builder = builder.addPathSegment(serviceName.replaceAll(\"\\\"\", \"\")); } builder = builder .addPathSegment(\"members\") .addQueryParameter(\"fields\", \"name,type,statusHA,partitionsAll,partitionsEndangered,\" + \"partitionsVulnerable,partitionsUnbalanced,storageEnabledCount,requestPendingCount,outgoingTransferCount\") .addQueryParameter(\"links\", \"\"); JsonNode rootNode = getResponse(builder); return isWebLogic ? rootNode : rootNode.get(\"items\"); } catch (Exception e) { throw new RuntimeException(\"Unable to get service info from \" + httpUrl, e); } } Run the Example Show Usage The supported way to run this example is to build using Maven as described here and running using java -jar target/status-ha-{version}.jar from a terminal in the base directory of this example: examples/guides/500-status-ha . Firstly, issue the command with the -u option which displays the usage. <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h Usage: StatusHAWatcher [options] Connection options: -m Use MBeanServer from cluster. Requires correct cluster config to join cluster. (Default) -h url Use Management over REST to connect to cluster -j url Use JMX URL to connect to cluster -hp host:port Connect to a JMX process via host:port Other Options: -d delay Delay between each check in seconds -s service Service name to monitor or all services if not specified -u Display usage StatusHA meanings: ENDANGERED - abnormal termination of any cluster node that runs this service may cause data loss NODE-SAFE - any single cluster node could be stopped without data loss MACHINE-SAFE - all the cluster nodes running on any given machine could be stopped at once without data loss RACK-SAFE - all the cluster nodes running on any given rack could be stopped at once without data loss SITE-SAFE - all the cluster nodes running on any given rack could be stopped at once without data loss Partition meanings: Endangered - The total number of partitions that are not currently backed up Vulnerable - The total number of partitions that are backed up on the same machine where the primary partition owner resides Unbalanced - The total number of primary and backup partitions which remain to be transferred until the partition distribution across the storage enabled service members is fully balanced Remaining - The number of partition transfers that remain to be completed before the service achieves the goals set by this strategy To test the utility we will start some cache servers. Start 4 cache servers If you want to connect using the default option, MBeanServer connection, you must ensure you build the example with the same Coherence version of the cluster you are going to connect to. Change to the `examples/guides/500-status-ha/target/libs directory Issue the following command in four terminals to start 4 DefaultCacheServer processes. On two of the processes use -Dcoherence.machine=machine1 and on the other two use -Dcoherence.machine=machine2 to simulate processes running on separate physical servers. <markup lang=\"bash\" >java -Dcoherence.machine=machine1 -jar coherence-{version}.jar Start the example When the cache servers have started, ensure you are in the examples/guides/500-status-ha directory and run the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar Notes: By default, the MBeans will be queried every 5 seconds. You can change this by using the -d option and specify a delay in seconds. All services are queried. You can select only a single service to be monitored by using -s option and specifying a service name. You will see output similar to the following showing the status HA values for the cluster services. <markup lang=\"bash\" >Connection: Cluster MBean Server Service: all Delay: 5 seconds Oracle Coherence Version 21.06.1 Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. Cluster Name: timmiddleton's cluster (21.06.1) Press CTRL-C to quit Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:27:13 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:13 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:18 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:18 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:23 AWST 2021 PartitionedCache 4 MACHINE-SAFE 257 0 0 0 Safe Tue Aug 03 11:27:23 AWST 2021 PartitionedTopic 4 MACHINE-SAFE 257 0 0 0 Safe You will notice that the StatusHA of all services is MACHINE-SAFE as there are an even number of cache servers on each \"machine\". Kill a cache server Kill one of the cache server processes using CTRL-C and note the change in the output of the example: <markup lang=\"bash\" >Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:29:39 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 257 170 170 partitions are unbalanced Tue Aug 03 11:29:39 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:45 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:45 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:50 AWST 2021 PartitionedTopic 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable Tue Aug 03 11:29:50 AWST 2021 PartitionedCache 3 NODE-SAFE 257 0 86 0 86 partitions are vulnerable You will notice that the StatusHA values are now node safe as there are not enough servers on each machine to provide a higher level of safety. Kill more cache servers Kill all but one cache server, and you will notice the StatusHA value is ENDANGERED as there is only one cache server with no backups available on other cache servers. <markup lang=\"bash\" >Date/Time Service Name Storage Count StatusHA Partitions Endangered Vulnerable Unbalanced Status ---------------------------- ------------ -------------- ------------ ----------- ----------- ----------- ----------- ------------- Tue Aug 03 11:33:14 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:14 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:19 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:19 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:24 AWST 2021 PartitionedTopic 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Tue Aug 03 11:33:24 AWST 2021 PartitionedCache 1 ENDANGERED 257 257 257 0 StatusHA is ENDANGERED Start the remaining cache servers, and you will see the StatusHA return to MACHINE-SAFE. Experiment with other connection options Other connection options are available which do not require you to have the same Coherence version as the example. Connect via JMX to a Host/Port If you have a Coherence MBean server running on a host/port you can connect to the cluster using the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -hp host:port Connect via JMX to a JMX URL <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -j service:jmx:rmi:///jndi/rmi://localhost:8888/jmxrmi Connect via Management over REST If you have a stand-alone Coherence cluster with Management over REST enabled, use the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h http://host:management-port/management/coherence/cluster Connect via Management over REST to WebLogic Server If you have a stand-alone Coherence cluster within WebLogic Server, use the following: <markup lang=\"bash\" >java -jar target/status-ha-{version}.jar -h http://host:admin-port/management/coherence/latest/clusters Packaging the example When the example is built, the following artifacts are created: target/status-ha-{version}.jar - executable jar with META-INF/MANIFEST.MF adding libs directory contents to classpath target/libs/coherence-{version}.jar - Coherence version the example was built with target/libs/jackson-annotations-2.12.0.jar - required dependencies target/libs/jackson-core-2.12.0.jar - required dependencies target/libs/jackson-databind-2.12.0.jar - required dependencies If you wish to take the example and run it on a separate machine, create a temporary directory and carry out the following: We are using a temporary directory /tmp/build as our example. Modify as you need. <markup lang=\"bash\" >cp target/status-ha-{version}.jar /tmp/build mkdir /tmp/build/libs cp target/libs/jackson-annotations-2.12.0.jar /tmp/build/libs cp target/libs/jackson-core-2.12.0.jar /tmp/build/libs cp target/libs/jackson-databind-2.12.0.jar /tmp/build/libs You can then change to the /tmp/build directory and run the example using: <markup lang=\"bash\" >cd /tmp/build java -jar status-ha-{version}.jar Zip or Tar the directory up and transfer to your target machine. If you wish to change the Coherence version used to build the example you can set the following system properties: -Dcoherence.version - the coherence version -Dcoherence.group.id - defaults to com.oracle.coherence.ce . Change to com.oracle.coherence for commercial edition. <markup lang=\"bash\" >mvn clean install -DskipTests -Dcoherence.version=14.1.1-0-0 -Dcoherence.groupid=com.oracle.coherence Summary In this example you built and ran a utility allowing you to monitor StatusHA values for Coherence services. See Also Coherence MBean Reference Starting and Stopping Cluster Members Using JMX to Manage Oracle Coherence ",
            "title": "Monitoring StatusHA"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Portable Types provide a way to add support for POF serialization to your classes via annotations and without the need to implement serialization code by hand, just like POF Annotations did. However, unlike POF Annotations, Portable Types: Implement serialization code at compile-time using byte code instrumentation, and do not rely on Java reflection at runtime at all. This makes them just as fast, but less error-prone, as manually implemented serialization code. Support, but do not require explicit registration via POF config file, as all the metadata required for POF type registration, such as type identifier, and the serializer class to use, are already available in the @PortableType annotation. Fully support class evolution. As a matter of fact, Portable Types provide a better and more complete evolution support than if you implemented Evolvable interface by hand. One of the limitations of Evolvable is that it only supports evolution of the leaf classes in the class hierarchy. Portable Types do not have this limitation, and allow you not only to evolve any class in the hierarchy, but also to evolve the class hierarchy itself, by adding new classes to any level of the class hierarchy. When we first introduced POF back in 2006, it was never the goal to require manual implementation of the serialization code&#8201;&#8212;&#8201;we always wanted to provide the tooling that would do the heavy lifting and allow users to simply express their intent via annotations. It may have taken us almost 15 years, but we feel that with the release of Portable Types, we are finally there. ",
            "title": "Features and Benefits"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " There are only two basic requirements for Portable Types: The class must be annotated with @PortableType annotation, and The fields that should be serialized must be annotated with @Portable or one of related annotations ( @PortableDate , @PortableArray , @PortableSet , @PortableList , or @PortableMap ) <markup lang=\"java\" >@PortableType(id = 1) public class Pet { @Portable protected String name; // constructors, accessors, etc. } @PortableType(id = 2) public class Dog extends Pet { @Portable private String breed; // constructors, accessors, etc. } Additional attribute-level annotations allow you to control certain serialization behaviors that are specific to the type of the attribute. For example, @PortableDate allows you to control whether you want to serialize date, time, or both when serializing java.util.Date instances (via mode property), and whether time zone information should be included (via includeTimezone property). If you are using Java 8 (or later) java.time classes, that information can be derived from the class itself, so you can (and should) simply use @Portable annotation instead. For example, LocalTime will be serialized as time only, with no time zone information, while the OffsetDateTime will be serialized as both date and time, with time zone information. Similarly, when serializing arrays, collections and maps, POF allows you to use uniform encoding , where the element type (or key and/or value type, in case of maps) is written into the POF stream only once, instead of once for each element of the collection, resulting in a more compact serialized form. <markup lang=\"java\" >public class MyClass { @PortableArray(elementClass = String.class) private String[] m_stringArray; @PortableSet(elementClass = String.class, clazz = LinkedHashSet.class) private Set&lt;String&gt; m_setOfStrings; @PortableList(elementClass = String.class) private List&lt;String&gt; m_listOfStrings; @PortableMap(keyClass = Integer.class, valueClass = String.class, clazz = TreeMap.class) private Map&lt;Integer, String&gt; m_uniformMap; } As you can see from the examples above, these annotations also allow you to specify the concrete class that should be created during deserialization for a given attribute. If the clazz property is not specified, HashSet will be used as a default set type, ArrayList as a default list type, and HashMap as a default map type. ",
            "title": "Usage Basics"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Coherence is a distributed system, and there is no guarantee that every cluster member, and every client process that connects to the cluster, will have the same version of each and every class. As a matter of fact, for systems that use rolling upgrades in order to avoid any downtime, it is pretty much guaranteed that they won&#8217;t! It is also neither safe nor practical for most Coherence customers to upgrade the cluster and all the clients at the same time, so being able to tolerate different versions of the same class across cluster members and clients is not only nice to have, but a necessity for many Coherence users. The issue is that when a process that has an older version of the class reads serialized data created from the newer version of the same class, it may encounter some attributes that it knows nothing about. Ideally, it should be able to ignore them and read the attributes it needs and knows about, instead of crashing, but that only solves part of the problem. If it ignores the unknown attributes completely, what will happen when it writes the same data back, by serializing an older version of the class that is only aware of some attributes? Unfortunately, the most likely answer is that it will lose the data it previously received but knows nothing about. Obviously, this is not a desirable scenario for a system that is intended for long-term data storage, so POF supports class evolution in a way that ensures that no data will be lost, regardless of how many versions of the same class are present across the various cluster and client processes, and regardless of which of those processes read or write the data. The support for class evolution has been in POF from the very beginning, via the Evolvable interface, but Portable Types remove some of the limitations and make the whole process significantly simpler. Both the class annotation ( @PortableType ) and the attribute annotations ( @Portable and related annotations) provide a way to specify versioning information necessary for class evolution. At the class level, whenever you modify a class by introducing a new attribute, you should increment the version property of the @PortableType annotation. At the same time, you should specify since attribute that matches the new class version number for any new class attribute. For example, to add age attribute to the Pet class, and color attribute to the Dog class, we would change the code above to: <markup lang=\"java\" >@PortableType(id = 1, version = 1) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; // constructors, accessors, etc. } @PortableType(id = 2, version = 1) public class Dog extends Pet { @Portable private String breed; @Portable(since = 1) private Color color; // constructors, accessors, etc. } Notice that both version and since properties are zero-based, which allows you to omit them completely in the initial implementation. It also means that for the first subsequent revision they should be set to 1 . Of course, those are just the defaults. You can certainly set the class and attribute version explicitly to any value even for the initial implementation, if you are so inclined. The only thing that matters is that you bump the version and set the since property to the latest version number whenever you make changes to the class in the future. For example, if in the future we decide to add height and weight attributes to the Pet class, we would simply increment the version to 2 and set the since property for the new attributes accordingly: <markup lang=\"java\" >@PortableType(id = 1, version = 2) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; @Portable(since = 2) protected int height; @Portable(since = 2) protected int weight; // constructors, accessors, etc. } Warning It may be obvious by now, but it&#8217;s probably worth calling out explicitly: class evolution allows you to add attributes to the new version of the class, but you should never remove existing attributes, as that will break serialization across class versions. You can certainly remove or deprecate attribute accessors from the class, but you should leave the field itself as-is, in order to preserve backwards compatibility of the serialized form. Along the same lines, you should avoid renaming the fields, as the default serialization order of fields is determined based on the alphabetical order of field names within a given class version (all fields with the same since value). ",
            "title": "Class Versioning and Evolution"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Annotating the classes is the first step in the implementation of Portable Types, but it is not sufficient on its own. In order to implement the necessary serialization logic, the classes also need to be instrumented at compile time. This is accomplished using the pof-maven-plugin , which should be configured in your POM file: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;20.12&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The configuration above will discover and instrument all project classes annotated with @PortableType annotation, including test classes. If you don&#8217;t need to instrument test classes you can omit the instrument-tests execution from the plugin configuration. The pof-maven-plugin uses Schema support to define the type system that contains all reachable portable types. This type system includes not only project classes that need to be instrumented, but also all portable types that exist in project dependencies. This is necessary because those dependent types may be used as attributes within the project classes, and need to be serialized appropriately. In some cases it may be necessary to expand the type system with the types that are not annotated with @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, assuming the Color class from the code examples above is an enum type, you would need to create the following META-INF/schema.xml file to register it and allow pof-maven-plugin to instrument Dog class correctly: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; Once all these bits and pieces are in place, you can simply run your build as usual: <markup lang=\"text\" >$ mvn clean install You can verify that the classes were instrumented successfully by checking the Maven output log. You should see something similar to the following: <markup lang=\"text\" >[INFO] --- pof-maven-plugin:20.12:instrument (instrument) @ petstore --- [INFO] Running PortableTypeGenerator for classes in /projects/petstore/target/classes [INFO] Instrumenting type petstore.Pet [INFO] Instrumenting type petstore.Dog Once the classes are successfully instrumented, they are ready to be registered and used. ",
            "title": "Compile-time Instrumentation"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Portable Object Format is not a self-describing serialization format: it replaces platform-specific class names with integer-based type identifiers , so it needs a way of mapping those type identifiers back to the platform-specific classes. This enables portability across platforms, which was, as the name clearly says, the main objective of POF. To manage the mappings between the type identifiers and concrete types, POF uses com.tangosol.io.pof.PofContext : <markup lang=\"java\" >public interface PofContext extends Serializer { PofSerializer getPofSerializer(int nTypeId); int getUserTypeIdentifier(Object o); int getUserTypeIdentifier(Class&lt;?&gt; clz); int getUserTypeIdentifier(String sClass); String getClassName(int nTypeId); Class&lt;?&gt; getClass(int nTypeId); boolean isUserType(Object o); boolean isUserType(Class&lt;?&gt; clz); boolean isUserType(String sClass); } It is worth noting that PofContext extends com.tangosol.io.Serializer interface, which means that any PofContext implementation can be used wherever Coherence expects a Serializer to be specified: within cache services as a storage-level serializer for data classes, as a transport-level serializer between thin clients and the proxy servers, etc. The PofContext performs the actual serialization by delegating to the appropriate PofSerializer , which is obtained via the PofContext.getPofSerializer method, based on a type identifier. There are several built-in implementations of PofContext . The SimplePofContext allows you to programmatically register type mappings by providing all the metadata needed for serialization, such as type identifier, class, and the PofSerializer to use: <markup lang=\"java\" >SimplePofContext ctx = new SimplePofContext(); ctx.registerUserType(1, Pet.class, new PortableTypeSerializer&lt;&gt;(1, Pet.class)); ctx.registerUserType(2, Dog.class, new PortableTypeSerializer&lt;&gt;(2, Dog.class)); ctx.registerUserType(3, Color.class, new EnumPofSerializer()); Notice that a lot of this information is somewhat repetitive and unnecessary when working with portable types, as all the metadata you need can be obtained from the class itself or the @PortableType annotation. Because of that, SimplePofContext also provides several convenience methods, specifically for portable types: <markup lang=\"java\" >ctx.registerPortableType(Pet.class); ctx.registerPortableType(Dog.class); Or even simpler: <markup lang=\"java\" >ctx.registerPortableTypes(Pet.class, Dog.class); While the SimplePofContext is useful for testing and quick prototyping, a PofContext implementation that is much more widely used within Coherence applications is ConfigurablePofContext . The ConfigurablePofContext allows you to provide type mappings via an external XML file: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;1&lt;/type-id&gt; &lt;class-name&gt;petstore.Pet&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;2&lt;/type-id&gt; &lt;class-name&gt;petstore.Dog&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;/pof-config&gt; You may notice that we didn&#8217;t have to specify serializer explicitly for Pet and Dog classes. This is because ConfigurablePofContext has the logic to determine which of the built-in PofSerializer implementations to use depending on the interfaces implemented by, or the annotations present on the specified class. In this case, it will automatically use PortableTypeSerializer because the classes have @PortableType annotation. However, we can make the configuration even simpler by enabling portable type discovery: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Once you set the enable-type-discovery flag to true , the ConfigurablePofContext will discover all the classes annotated with @PortableType and register them automatically, based on the annotation metadata. If we didn&#8217;t have the Color enum that has to be registered explicitly, we could even omit the configuration file completely, as the default pof-config.xml file that is built into Coherence looks like this: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;!-- by default just include coherence POF user types --&gt; &lt;include&gt;coherence-pof-config.xml&lt;/include&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Note The portable type discovery feature depends on the availability of a Jandex index within the modules that provide portable types that need to be registered. Make sure that you configure Jandex Maven Plugin to index classes in your modules at build time: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;process-classes&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; ",
            "title": "Registration and Discovery"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Once you have annotated, instrumented and registered portable types as described in the sections above, you can use them with Coherence just as easily as you would use plain Java Serializable classes, by configuring Coherence services to use pof serializer instead of the default java serializer. However, there is still one problem: serialization code is implemented by the pof-maven-plugin at compile-time, and only if you run Maven build, which can make it a bit cumbersome to run unit and integration tests within your IDE. In order to solve that problem, we have implemented IDE plugins for IntelliJ IDEA and Eclipse, which can instrument your classes during incremental or full compilation performed by your IDE. This allows you to test both the serialization of your classes and the code that depends on it without having to run Maven build or leave your IDE. Please follow the documentation for the Coherence IntelliJ Plugin or Coherence Eclipse Plugin for detailed instructions on how to install and use the plugin for your favorite IDE. Note We&#8217;ve used 1, 2, and 3 as type identifiers in the code and configuration examples above for simplicity, but it is worth noting that Coherence reserves type identifiers from 0 to 999 for internal use. That means that you should only use type identifiers of 1000 or higher for your own classes. ",
            "title": "IDE Support"
        },
        {
            "location": "/docs/core/04_portable_types",
            "text": " Portable Object Format (POF) was first introduced in Coherence 3.2 (2006), as a way to serialize classes in a platform and language independent format, and is the only serialization format supported by the legacy non-Java Extend clients, such as .NET and C++ Extend client implementations. As soon as it was released, POF became the preferred serialization format even for customers writing pure Java applications, for several reasons: It is significantly faster than other supported serialization formats, such as Java serialization and ExternalizableLite . It is significantly more compact that other supported serialization formats, allowing you to store more data in a cluster of a given size, and to move less data over the wire. It supports seamless evolution of data classes, allowing you to upgrade various parts of the application (both storage members and clients) independently of one another, without the risk of losing data in the process. Over the years POF remained largely unchanged, even though it did receive a number of additional features that simplified its use: POF Reflection was introduced in Coherence 3.5 (2009), allowing users to extract individual attributes from the POF stream via PofNavigator . POF Annotations were introduced in Coherence 3.7.1 (2011), as a way to eliminate the need for the manual implementation of the serialization-related code. Unfortunately, the latter fell a bit short. The implementation was heavily dependent on Java reflection, which sacrificed some performance benefits of POF. More importantly, they provide no support for class evolution, thus sacrificing another important POF benefit. As such, POF Annotations were deemed somewhat inadequate, and we started working on their replacement in 2013. Some supporting features, such as schema support , were included in Coherence 12.2.1 (2015) and 14.1.1 (2020), and the remaining work was completed and released as part of the Coherence CE 20.12 release and will be available in the next commercial release. Features and Benefits Portable Types provide a way to add support for POF serialization to your classes via annotations and without the need to implement serialization code by hand, just like POF Annotations did. However, unlike POF Annotations, Portable Types: Implement serialization code at compile-time using byte code instrumentation, and do not rely on Java reflection at runtime at all. This makes them just as fast, but less error-prone, as manually implemented serialization code. Support, but do not require explicit registration via POF config file, as all the metadata required for POF type registration, such as type identifier, and the serializer class to use, are already available in the @PortableType annotation. Fully support class evolution. As a matter of fact, Portable Types provide a better and more complete evolution support than if you implemented Evolvable interface by hand. One of the limitations of Evolvable is that it only supports evolution of the leaf classes in the class hierarchy. Portable Types do not have this limitation, and allow you not only to evolve any class in the hierarchy, but also to evolve the class hierarchy itself, by adding new classes to any level of the class hierarchy. When we first introduced POF back in 2006, it was never the goal to require manual implementation of the serialization code&#8201;&#8212;&#8201;we always wanted to provide the tooling that would do the heavy lifting and allow users to simply express their intent via annotations. It may have taken us almost 15 years, but we feel that with the release of Portable Types, we are finally there. Usage Basics There are only two basic requirements for Portable Types: The class must be annotated with @PortableType annotation, and The fields that should be serialized must be annotated with @Portable or one of related annotations ( @PortableDate , @PortableArray , @PortableSet , @PortableList , or @PortableMap ) <markup lang=\"java\" >@PortableType(id = 1) public class Pet { @Portable protected String name; // constructors, accessors, etc. } @PortableType(id = 2) public class Dog extends Pet { @Portable private String breed; // constructors, accessors, etc. } Additional attribute-level annotations allow you to control certain serialization behaviors that are specific to the type of the attribute. For example, @PortableDate allows you to control whether you want to serialize date, time, or both when serializing java.util.Date instances (via mode property), and whether time zone information should be included (via includeTimezone property). If you are using Java 8 (or later) java.time classes, that information can be derived from the class itself, so you can (and should) simply use @Portable annotation instead. For example, LocalTime will be serialized as time only, with no time zone information, while the OffsetDateTime will be serialized as both date and time, with time zone information. Similarly, when serializing arrays, collections and maps, POF allows you to use uniform encoding , where the element type (or key and/or value type, in case of maps) is written into the POF stream only once, instead of once for each element of the collection, resulting in a more compact serialized form. <markup lang=\"java\" >public class MyClass { @PortableArray(elementClass = String.class) private String[] m_stringArray; @PortableSet(elementClass = String.class, clazz = LinkedHashSet.class) private Set&lt;String&gt; m_setOfStrings; @PortableList(elementClass = String.class) private List&lt;String&gt; m_listOfStrings; @PortableMap(keyClass = Integer.class, valueClass = String.class, clazz = TreeMap.class) private Map&lt;Integer, String&gt; m_uniformMap; } As you can see from the examples above, these annotations also allow you to specify the concrete class that should be created during deserialization for a given attribute. If the clazz property is not specified, HashSet will be used as a default set type, ArrayList as a default list type, and HashMap as a default map type. Class Versioning and Evolution Coherence is a distributed system, and there is no guarantee that every cluster member, and every client process that connects to the cluster, will have the same version of each and every class. As a matter of fact, for systems that use rolling upgrades in order to avoid any downtime, it is pretty much guaranteed that they won&#8217;t! It is also neither safe nor practical for most Coherence customers to upgrade the cluster and all the clients at the same time, so being able to tolerate different versions of the same class across cluster members and clients is not only nice to have, but a necessity for many Coherence users. The issue is that when a process that has an older version of the class reads serialized data created from the newer version of the same class, it may encounter some attributes that it knows nothing about. Ideally, it should be able to ignore them and read the attributes it needs and knows about, instead of crashing, but that only solves part of the problem. If it ignores the unknown attributes completely, what will happen when it writes the same data back, by serializing an older version of the class that is only aware of some attributes? Unfortunately, the most likely answer is that it will lose the data it previously received but knows nothing about. Obviously, this is not a desirable scenario for a system that is intended for long-term data storage, so POF supports class evolution in a way that ensures that no data will be lost, regardless of how many versions of the same class are present across the various cluster and client processes, and regardless of which of those processes read or write the data. The support for class evolution has been in POF from the very beginning, via the Evolvable interface, but Portable Types remove some of the limitations and make the whole process significantly simpler. Both the class annotation ( @PortableType ) and the attribute annotations ( @Portable and related annotations) provide a way to specify versioning information necessary for class evolution. At the class level, whenever you modify a class by introducing a new attribute, you should increment the version property of the @PortableType annotation. At the same time, you should specify since attribute that matches the new class version number for any new class attribute. For example, to add age attribute to the Pet class, and color attribute to the Dog class, we would change the code above to: <markup lang=\"java\" >@PortableType(id = 1, version = 1) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; // constructors, accessors, etc. } @PortableType(id = 2, version = 1) public class Dog extends Pet { @Portable private String breed; @Portable(since = 1) private Color color; // constructors, accessors, etc. } Notice that both version and since properties are zero-based, which allows you to omit them completely in the initial implementation. It also means that for the first subsequent revision they should be set to 1 . Of course, those are just the defaults. You can certainly set the class and attribute version explicitly to any value even for the initial implementation, if you are so inclined. The only thing that matters is that you bump the version and set the since property to the latest version number whenever you make changes to the class in the future. For example, if in the future we decide to add height and weight attributes to the Pet class, we would simply increment the version to 2 and set the since property for the new attributes accordingly: <markup lang=\"java\" >@PortableType(id = 1, version = 2) public class Pet { @Portable protected String name; @Portable(since = 1) protected int age; @Portable(since = 2) protected int height; @Portable(since = 2) protected int weight; // constructors, accessors, etc. } Warning It may be obvious by now, but it&#8217;s probably worth calling out explicitly: class evolution allows you to add attributes to the new version of the class, but you should never remove existing attributes, as that will break serialization across class versions. You can certainly remove or deprecate attribute accessors from the class, but you should leave the field itself as-is, in order to preserve backwards compatibility of the serialized form. Along the same lines, you should avoid renaming the fields, as the default serialization order of fields is determined based on the alphabetical order of field names within a given class version (all fields with the same since value). Compile-time Instrumentation Annotating the classes is the first step in the implementation of Portable Types, but it is not sufficient on its own. In order to implement the necessary serialization logic, the classes also need to be instrumented at compile time. This is accomplished using the pof-maven-plugin , which should be configured in your POM file: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;20.12&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; The configuration above will discover and instrument all project classes annotated with @PortableType annotation, including test classes. If you don&#8217;t need to instrument test classes you can omit the instrument-tests execution from the plugin configuration. The pof-maven-plugin uses Schema support to define the type system that contains all reachable portable types. This type system includes not only project classes that need to be instrumented, but also all portable types that exist in project dependencies. This is necessary because those dependent types may be used as attributes within the project classes, and need to be serialized appropriately. In some cases it may be necessary to expand the type system with the types that are not annotated with @PortableType annotation, and are not discovered automatically. This is typically the case when some of your portable types have enum values, or existing classes that implement PortableObject interface explicitly as attributes. You can add those types to the schema by creating a META-INF/schema.xml file and specifying them explicitly. For example, assuming the Color class from the code examples above is an enum type, you would need to create the following META-INF/schema.xml file to register it and allow pof-maven-plugin to instrument Dog class correctly: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;schema xmlns=\"http://xmlns.oracle.com/coherence/schema\" xmlns:java=\"http://xmlns.oracle.com/coherence/schema/java\" external=\"true\"&gt; &lt;type name=\"Color\"&gt; &lt;java:type name=\"petstore.Color\"/&gt; &lt;/type&gt; &lt;/schema&gt; Once all these bits and pieces are in place, you can simply run your build as usual: <markup lang=\"text\" >$ mvn clean install You can verify that the classes were instrumented successfully by checking the Maven output log. You should see something similar to the following: <markup lang=\"text\" >[INFO] --- pof-maven-plugin:20.12:instrument (instrument) @ petstore --- [INFO] Running PortableTypeGenerator for classes in /projects/petstore/target/classes [INFO] Instrumenting type petstore.Pet [INFO] Instrumenting type petstore.Dog Once the classes are successfully instrumented, they are ready to be registered and used. Registration and Discovery Portable Object Format is not a self-describing serialization format: it replaces platform-specific class names with integer-based type identifiers , so it needs a way of mapping those type identifiers back to the platform-specific classes. This enables portability across platforms, which was, as the name clearly says, the main objective of POF. To manage the mappings between the type identifiers and concrete types, POF uses com.tangosol.io.pof.PofContext : <markup lang=\"java\" >public interface PofContext extends Serializer { PofSerializer getPofSerializer(int nTypeId); int getUserTypeIdentifier(Object o); int getUserTypeIdentifier(Class&lt;?&gt; clz); int getUserTypeIdentifier(String sClass); String getClassName(int nTypeId); Class&lt;?&gt; getClass(int nTypeId); boolean isUserType(Object o); boolean isUserType(Class&lt;?&gt; clz); boolean isUserType(String sClass); } It is worth noting that PofContext extends com.tangosol.io.Serializer interface, which means that any PofContext implementation can be used wherever Coherence expects a Serializer to be specified: within cache services as a storage-level serializer for data classes, as a transport-level serializer between thin clients and the proxy servers, etc. The PofContext performs the actual serialization by delegating to the appropriate PofSerializer , which is obtained via the PofContext.getPofSerializer method, based on a type identifier. There are several built-in implementations of PofContext . The SimplePofContext allows you to programmatically register type mappings by providing all the metadata needed for serialization, such as type identifier, class, and the PofSerializer to use: <markup lang=\"java\" >SimplePofContext ctx = new SimplePofContext(); ctx.registerUserType(1, Pet.class, new PortableTypeSerializer&lt;&gt;(1, Pet.class)); ctx.registerUserType(2, Dog.class, new PortableTypeSerializer&lt;&gt;(2, Dog.class)); ctx.registerUserType(3, Color.class, new EnumPofSerializer()); Notice that a lot of this information is somewhat repetitive and unnecessary when working with portable types, as all the metadata you need can be obtained from the class itself or the @PortableType annotation. Because of that, SimplePofContext also provides several convenience methods, specifically for portable types: <markup lang=\"java\" >ctx.registerPortableType(Pet.class); ctx.registerPortableType(Dog.class); Or even simpler: <markup lang=\"java\" >ctx.registerPortableTypes(Pet.class, Dog.class); While the SimplePofContext is useful for testing and quick prototyping, a PofContext implementation that is much more widely used within Coherence applications is ConfigurablePofContext . The ConfigurablePofContext allows you to provide type mappings via an external XML file: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;1&lt;/type-id&gt; &lt;class-name&gt;petstore.Pet&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;2&lt;/type-id&gt; &lt;class-name&gt;petstore.Dog&lt;/class-name&gt; &lt;/user-type&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;/pof-config&gt; You may notice that we didn&#8217;t have to specify serializer explicitly for Pet and Dog classes. This is because ConfigurablePofContext has the logic to determine which of the built-in PofSerializer implementations to use depending on the interfaces implemented by, or the annotations present on the specified class. In this case, it will automatically use PortableTypeSerializer because the classes have @PortableType annotation. However, we can make the configuration even simpler by enabling portable type discovery: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;user-type&gt; &lt;type-id&gt;3&lt;/type-id&gt; &lt;class-name&gt;petstore.Color&lt;/class-name&gt; &lt;serializer&gt; &lt;class-name&gt;com.tangosol.io.pof.EnumPofSerializer&lt;/class-name&gt; &lt;/serializer&gt; &lt;/user-type&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Once you set the enable-type-discovery flag to true , the ConfigurablePofContext will discover all the classes annotated with @PortableType and register them automatically, based on the annotation metadata. If we didn&#8217;t have the Color enum that has to be registered explicitly, we could even omit the configuration file completely, as the default pof-config.xml file that is built into Coherence looks like this: <markup lang=\"xml\" >&lt;pof-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-pof-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-pof-config coherence-pof-config.xsd\"&gt; &lt;user-type-list&gt; &lt;!-- by default just include coherence POF user types --&gt; &lt;include&gt;coherence-pof-config.xml&lt;/include&gt; &lt;/user-type-list&gt; &lt;enable-type-discovery&gt;true&lt;/enable-type-discovery&gt; &lt;/pof-config&gt; Note The portable type discovery feature depends on the availability of a Jandex index within the modules that provide portable types that need to be registered. Make sure that you configure Jandex Maven Plugin to index classes in your modules at build time: <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.0.8&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;process-classes&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; IDE Support Once you have annotated, instrumented and registered portable types as described in the sections above, you can use them with Coherence just as easily as you would use plain Java Serializable classes, by configuring Coherence services to use pof serializer instead of the default java serializer. However, there is still one problem: serialization code is implemented by the pof-maven-plugin at compile-time, and only if you run Maven build, which can make it a bit cumbersome to run unit and integration tests within your IDE. In order to solve that problem, we have implemented IDE plugins for IntelliJ IDEA and Eclipse, which can instrument your classes during incremental or full compilation performed by your IDE. This allows you to test both the serialization of your classes and the code that depends on it without having to run Maven build or leave your IDE. Please follow the documentation for the Coherence IntelliJ Plugin or Coherence Eclipse Plugin for detailed instructions on how to install and use the plugin for your favorite IDE. Note We&#8217;ve used 1, 2, and 3 as type identifiers in the code and configuration examples above for simplicity, but it is worth noting that Coherence reserves type identifiers from 0 to 999 for internal use. That means that you should only use type identifiers of 1000 or higher for your own classes. ",
            "title": "Portable Types"
        },
        {
            "location": "/docs/topics/05_persistence",
            "text": " Coherence topics store data in caches, and as such the Coherence persistence feature can be used to persist that data to disc. As well as the actual topic message data, the metadata associated with a topic will also be stored in caches. This data tracks information such as the head and tail of a topic, it also tracks subscriber groups and subscriber lifetimes. When persistence has been enabled, the metadata caches will also be persisted. Both active and on-demand persistence will work with topics, and in fact, a recommendation is to use active persistence, as data loss can severely impact topic functionality. Warning Care must be taken if recovering cache data from a persistence snapshot. A snapshot of topics caches will also contain metadata for the topic, this includes the commits, heads and tails for subscribers and subscriber groups. When recovering a snapshot for a Coherence topics cache service, all the metadata will also be recovered. This will reset the state of subscribers and subscriber groups, as well as topic heads, and tails, which may affect publishers. When recovering a snapshot it is important that no subscribers and publishers have been connected. Publishers and subscribers have their own state and on recovery this state will be out of date with the actual topic state in the topic metadata. This can cause subscribers to read the wrong data, or a publisher to attempt to publish to the wrong position. This may not immediately be apparent as it may not cause an exception to be thrown so data could be corrupted. ",
            "title": "Topics and Persistence"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Note Topics was first introduced in 14.1.1.0 and was considerable enhanced to improve message delivery guarantees in CE 21.06. Whilst the changes made in 21.06 are API compatible, the underlying data-structures and entry processors used are not backwards compatible, meaning that a rolling upgrade of a cluster using Topics from 14.1.1.0 (or CE version prior to 21.06) upgrading to CE 21.06 will not work. ",
            "title": "Version Compatibility"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " The Topics API enables the building of data pipelines between loosely coupled publishers and consumers. One or more publishers publish a stream of values to a topic. One or more subscribers consume the stream of values from a topic. The topic values are spread evenly across all Oracle Coherence data servers, enabling high throughput processing in a distributed and fault tolerant manner. Version Compatibility Note Topics was first introduced in 14.1.1.0 and was considerable enhanced to improve message delivery guarantees in CE 21.06. Whilst the changes made in 21.06 are API compatible, the underlying data-structures and entry processors used are not backwards compatible, meaning that a rolling upgrade of a cluster using Topics from 14.1.1.0 (or CE version prior to 21.06) upgrading to CE 21.06 will not work. ",
            "title": "Overview"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " In order to scale publishers and subscribers whilst still guaranteeing message ordering, Coherence Topics introduces the concept of channels. A channel is similar in idea to how Coherence partitions data in distributed NamedMap but to avoid confusion the name partition was not reused. Channels are an important part of the operation of both publishers and subscribers. A publisher is configured to control what ordering guarantees exist for the messages it publishes when they are received by subscribers. This is achieved by publishing messages to a specific channel. All messages published to a channel will be received by subscribers in the same order. Messages published to different channels may be interleaved as they are received by subscribers. The number of channels that a topic has allows publishers to scale better as they avoid contention that may occur with many publishers publishing to a single channel. Message consumption can be scaled because multiple subscribers (in a group) will subscribe to different channels, so scaling up receiving of messages, whilst maintaining order. The channel count for a topic is configurable, ideally a small prime (the default is 17). There are pros and cons with very small or very large channel counts, depending on the application use case and what sort of scaling or ordering guarantees it requires. ",
            "title": "Channels"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Every element published to a topic has a position. A position is an opaque data structure used by the underlying NamedTopic implementation to track the position of an element in a channel and maintain ordering of messages. Positions are then used by subscribers to track the elements that they have received and when committing a position to determine which preceding elements are also committed and to then recover to the correct position in the topic when subscribers reconnect or recover from failure. Whilst a position data structure is opaque, they are serializable, meaning that they can be stored into a separate data store by application code that wants to manually track message element processing. The combination of channel and position should be unique for each message element published and received. ",
            "title": "Position"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " A NamedTopic is the name of the Coherence data structure that stores data as an ordered stream of messages. Generally most application code does not need to interact directly with a NamedTopic , but instead with a Publisher or Subscriber to either publish to or subscribe to a NamedTopic . See: Configuring NamedTopics ",
            "title": "NamedTopic"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Some core concepts of Coherence Topics are described below. Channels In order to scale publishers and subscribers whilst still guaranteeing message ordering, Coherence Topics introduces the concept of channels. A channel is similar in idea to how Coherence partitions data in distributed NamedMap but to avoid confusion the name partition was not reused. Channels are an important part of the operation of both publishers and subscribers. A publisher is configured to control what ordering guarantees exist for the messages it publishes when they are received by subscribers. This is achieved by publishing messages to a specific channel. All messages published to a channel will be received by subscribers in the same order. Messages published to different channels may be interleaved as they are received by subscribers. The number of channels that a topic has allows publishers to scale better as they avoid contention that may occur with many publishers publishing to a single channel. Message consumption can be scaled because multiple subscribers (in a group) will subscribe to different channels, so scaling up receiving of messages, whilst maintaining order. The channel count for a topic is configurable, ideally a small prime (the default is 17). There are pros and cons with very small or very large channel counts, depending on the application use case and what sort of scaling or ordering guarantees it requires. Position Every element published to a topic has a position. A position is an opaque data structure used by the underlying NamedTopic implementation to track the position of an element in a channel and maintain ordering of messages. Positions are then used by subscribers to track the elements that they have received and when committing a position to determine which preceding elements are also committed and to then recover to the correct position in the topic when subscribers reconnect or recover from failure. Whilst a position data structure is opaque, they are serializable, meaning that they can be stored into a separate data store by application code that wants to manually track message element processing. The combination of channel and position should be unique for each message element published and received. NamedTopic A NamedTopic is the name of the Coherence data structure that stores data as an ordered stream of messages. Generally most application code does not need to interact directly with a NamedTopic , but instead with a Publisher or Subscriber to either publish to or subscribe to a NamedTopic . See: Configuring NamedTopics ",
            "title": "Concepts"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " A Publisher publishes messages to topic. Each Publisher is created with a configurable ordering option that determines the ordering guarantees for how those messages are then received by Subscribers . The topic values are spread evenly across all Coherence storage enabled cluster members, enabling high throughput processing in a distributed and fault tolerant manner. Multiple publishers can be created to publish messages to the same topic. See: Publishers ",
            "title": "Publishers"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " A Subscriber subscribes to a NamedTopic and receives messages from that topic. The subscriber mechanism is based on polling, as Subscriber calls its receive method to fetch the next message, messages are not pushed to a subscriber. The Subscriber API is asynchronous so applications can receive and process messages in a non-blocking \"push-like\" fashion using the CompletableFuture API. There are two types of Subscriber , anonymous subscribers and group subscribers. An anonymous subscriber connects to a topic and receives all messages published to that topic. Anonymous subscribers are not durable, if they are closed (or fail) then when they reconnect they are effectively seen as new subscribers and will restart processing messages from the topic&#8217;s tail (or from the head if the topic is configured to retain messages). A group subscriber is part of a group of one or more subscribers that all belong to the same named subscriber group. In a subscriber group each message is only delivered to one of the members of the group. This allows message processing to be scaled up by using multiple subscribers that will process messages from the same topic, whilst maintaining the publishers ordering. Message delivery in a subscriber group is controlled by assigning ownership of the channels of the topic to each subscriber in the group, so that each subscriber is polling a sub-set of channels. As subscribers in a group are created, or closed, or fail, their channel ownership is reallocated across the remaining subscribers. When new subscribers are created in a group, channels from the existing subscribers are redistributed to the new subscriber. This all happens automatically to try to maintain an evenly balanced distribution of channels to group subscribers. Obviously if more subscribers are created in a group than there are channels in the topic then some of those subscribers cannot be allocated a channel as there are not enough channels to go around. A subscriber group is durable, if all the subscribers in a group are closed, next time a subscriber in the group re-subscribes it will start processing messages from next message after the last committed position. See: Subscribers ",
            "title": "Subscribers"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Coherence Topics introduces publish and subscribe messaging, also often referred to as streaming, functionality in Oracle Coherence. Overview The Topics API enables the building of data pipelines between loosely coupled publishers and consumers. One or more publishers publish a stream of values to a topic. One or more subscribers consume the stream of values from a topic. The topic values are spread evenly across all Oracle Coherence data servers, enabling high throughput processing in a distributed and fault tolerant manner. Version Compatibility Note Topics was first introduced in 14.1.1.0 and was considerable enhanced to improve message delivery guarantees in CE 21.06. Whilst the changes made in 21.06 are API compatible, the underlying data-structures and entry processors used are not backwards compatible, meaning that a rolling upgrade of a cluster using Topics from 14.1.1.0 (or CE version prior to 21.06) upgrading to CE 21.06 will not work. Concepts Some core concepts of Coherence Topics are described below. Channels In order to scale publishers and subscribers whilst still guaranteeing message ordering, Coherence Topics introduces the concept of channels. A channel is similar in idea to how Coherence partitions data in distributed NamedMap but to avoid confusion the name partition was not reused. Channels are an important part of the operation of both publishers and subscribers. A publisher is configured to control what ordering guarantees exist for the messages it publishes when they are received by subscribers. This is achieved by publishing messages to a specific channel. All messages published to a channel will be received by subscribers in the same order. Messages published to different channels may be interleaved as they are received by subscribers. The number of channels that a topic has allows publishers to scale better as they avoid contention that may occur with many publishers publishing to a single channel. Message consumption can be scaled because multiple subscribers (in a group) will subscribe to different channels, so scaling up receiving of messages, whilst maintaining order. The channel count for a topic is configurable, ideally a small prime (the default is 17). There are pros and cons with very small or very large channel counts, depending on the application use case and what sort of scaling or ordering guarantees it requires. Position Every element published to a topic has a position. A position is an opaque data structure used by the underlying NamedTopic implementation to track the position of an element in a channel and maintain ordering of messages. Positions are then used by subscribers to track the elements that they have received and when committing a position to determine which preceding elements are also committed and to then recover to the correct position in the topic when subscribers reconnect or recover from failure. Whilst a position data structure is opaque, they are serializable, meaning that they can be stored into a separate data store by application code that wants to manually track message element processing. The combination of channel and position should be unique for each message element published and received. NamedTopic A NamedTopic is the name of the Coherence data structure that stores data as an ordered stream of messages. Generally most application code does not need to interact directly with a NamedTopic , but instead with a Publisher or Subscriber to either publish to or subscribe to a NamedTopic . See: Configuring NamedTopics Publishers A Publisher publishes messages to topic. Each Publisher is created with a configurable ordering option that determines the ordering guarantees for how those messages are then received by Subscribers . The topic values are spread evenly across all Coherence storage enabled cluster members, enabling high throughput processing in a distributed and fault tolerant manner. Multiple publishers can be created to publish messages to the same topic. See: Publishers Subscribers A Subscriber subscribes to a NamedTopic and receives messages from that topic. The subscriber mechanism is based on polling, as Subscriber calls its receive method to fetch the next message, messages are not pushed to a subscriber. The Subscriber API is asynchronous so applications can receive and process messages in a non-blocking \"push-like\" fashion using the CompletableFuture API. There are two types of Subscriber , anonymous subscribers and group subscribers. An anonymous subscriber connects to a topic and receives all messages published to that topic. Anonymous subscribers are not durable, if they are closed (or fail) then when they reconnect they are effectively seen as new subscribers and will restart processing messages from the topic&#8217;s tail (or from the head if the topic is configured to retain messages). A group subscriber is part of a group of one or more subscribers that all belong to the same named subscriber group. In a subscriber group each message is only delivered to one of the members of the group. This allows message processing to be scaled up by using multiple subscribers that will process messages from the same topic, whilst maintaining the publishers ordering. Message delivery in a subscriber group is controlled by assigning ownership of the channels of the topic to each subscriber in the group, so that each subscriber is polling a sub-set of channels. As subscribers in a group are created, or closed, or fail, their channel ownership is reallocated across the remaining subscribers. When new subscribers are created in a group, channels from the existing subscribers are redistributed to the new subscriber. This all happens automatically to try to maintain an evenly balanced distribution of channels to group subscribers. Obviously if more subscribers are created in a group than there are channels in the topic then some of those subscribers cannot be allocated a channel as there are not enough channels to go around. A subscriber group is durable, if all the subscribers in a group are closed, next time a subscriber in the group re-subscribes it will start processing messages from next message after the last committed position. See: Subscribers ",
            "title": "Introduction to Coherence Topics"
        },
        {
            "location": "/docs/README",
            "text": " To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs ",
            "title": "Build the Docs"
        },
        {
            "location": "/docs/README",
            "text": " To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "View the Docs"
        },
        {
            "location": "/docs/README",
            "text": " This is the module that builds the Coherence documentation. The module is not part of the default build and must be built separately. Build the Docs To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs View the Docs To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "Coherence Documentation Module"
        },
        {
            "location": "/docs/README",
            "text": " When putting version numbers in .adoc files, we use attribute substitutions. Attributes are set in the sitegen.yaml file, for example <markup lang=\"yaml\" >engine: asciidoctor: images-dir: \"docs/images\" libraries: - \"asciidoctor-diagram\" attributes: plantumlconfig: \"_plantuml-config.txt\" coherence-maven-group-id: \"${coherence.group.id}\" version-coherence: \"${revision}\" version-commercial-docs: \"14.1.1.0\" version-helidon: \"${helidon.version}\" The format of an attribute is name followed by a colon, and the attribute value in quotes, so above the value of the version-commercial-docs attribute is 14.1.1.0 . Attributes can be taken from Maven build properties by using the normal Maven property replacement string as the value. For example the version-coherence attribute&#8217;s value will be the Maven revision property value. In the .adoc files the attributes are then substituted by putting the attribute name in curly brackets. For example: The current commercial Coherence version is 14.1.1.0. would become The current commercial Coherence version is 14.1.1.0. ",
            "title": "Version Numbers"
        },
        {
            "location": "/docs/core/08_non_blocking",
            "text": " Certain data source libraries have APIs that do not necessitate the caller to wait for the result to come back before doing something else. For example, making HTTP calls can lead to relatively long waits between the time a request to store data is sent and the response comes back. By implementing non-blocking APIs, the caller can immediately do other work without having to wait for the actual store operation to complete. By implementing the NonBlockingEntryStore interface, the store implementer will be able to use non-blocking APIs in a more natural way. NonBlockingEntryStore is being provided in the context of pluggable data stores : in order to use it, an implementation class needs to be provided and configured. This class will either load, store or remove data from the data source by way of a ReadWriteBackingMap . This backing map provides two elements: an internal map to cache the data, and a data source access portion to interact with the data base. The NonBlockingEntryStore interface is provided the BinaryEntry that represents the load , store or erase operation. This provides an opportunity for implementers to avoid deserialization if desired; this is similar to BinaryEntryStore . Avoiding deserialization generally is possible if the raw binary is stored in the downstream system, or the binary can be navigated to extract relevant parts, as opposed to deserializing the entire key or value. Note: getKey , getValue and getOriginalValue will induce deserialization for the first call. ",
            "title": "NonBlockingEntryStore"
        },
        {
            "location": "/docs/core/08_non_blocking",
            "text": " To specify a non-blocking cache store implementation, provide the implementation class name within the read-write-backing-map-scheme as shown below. <markup lang=\"xml\" >... &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;cache-mapping&gt; ... &lt;cache-name&gt;myCache&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-rwbm-nonblocking&lt;/scheme-name&gt; ... &lt;/cache-mapping&gt; &lt;distributed-scheme&gt; ... &lt;scheme-name&gt;distributed-rwbm-nonblocking&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.company.NonBlockingStoreImpl&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; ... &lt;/distributed-scheme&gt; &lt;/cache-config&gt; ",
            "title": "Configuration"
        },
        {
            "location": "/docs/core/08_non_blocking",
            "text": " Once configured, a class implementing the NonBlockingEntryStore interface needs to be added to the added to the classpath of the storage enabled members. See below for example code. With the class in place, the equivalency below is established: get() -invokes&#8594; load() Note: If data is already in the cache, load() does not get called. Also, calling get() will wait for onNext() / onError() to complete before returning. getAll() -invokes&#8594; loadAll() put() -invokes&#8594; store() putAll() -invokes&#8594; storeAll() remove() -invokes&#8594; erase() removeAll() -invokes&#8594; eraseAll() The code below contains portions of code is using a reactive API to access a data source. <markup lang=\"java\" >... /** * An example NonBlockingEntryStore implementation */ public class ExampleNonBlockingEntryStore&lt;K, V&gt; { @Override public void load(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer) { K key = binEntry.getKey(); Flux.from(getConnection()) .flatMap(connection -&gt; connection.createStatement(LOAD_STMT) .bind(\"$1\", key) .execute()) .flatMap(result -&gt; result.map((row, meta) -&gt; { return new Student( (String) row.get(\"name\"), (String) row.get(\"address\")); } )) .collectList() .doOnNext(s -&gt; { binEntry.setValue((V) s.get(0)); observer.onNext(binEntry); }) .doOnError(t -&gt; { if (t instanceof IndexOutOfBoundsException) { CacheFactory.log(\"Could not find row for key: \" + key); } else { CacheFactory.log(\"Error: \" + t); } observer.onError(binEntry, new Exception(t)); }) .subscribe(); } ... @Override public void store(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer) { K key = binEntry.getKey(); Student oStudent = (Student) binEntry.getValue(); Flux.from(getConnection()) .flatMap(connection -&gt; connection.createStatement(STORE_STMT) .bind(\"$1\", key) .bind(\"$2\", oStudent.getName()) .bind(\"$3\", oStudent.getAddress()) .execute()) .flatMap(Result::getRowsUpdated) .doOnNext((s) -&gt; { CacheFactory.log(\"store done, rows updated: \" + s); observer.onNext(binEntry); }) .doOnError(t -&gt; new Exception(t)) .subscribe(); } ... private static final String STORE_STMT = \"INSERT INTO student VALUES ($1, $2, $3) ON conflict (id) DO UPDATE SET name=$2, address=$2\"; private static final String LOAD_STMT = \"SELECT NAME, ADDRESS FROM student WHERE id=$1\"; Be sure to consult these best practices when implementing an entry store for your data sources. ",
            "title": "Implementation"
        },
        {
            "location": "/docs/core/08_non_blocking",
            "text": " Below is a summary of the tasks required in order to use the feature. To get started, please check the guide for non blocking stores . Configuration To specify a non-blocking cache store implementation, provide the implementation class name within the read-write-backing-map-scheme as shown below. <markup lang=\"xml\" >... &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;cache-mapping&gt; ... &lt;cache-name&gt;myCache&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-rwbm-nonblocking&lt;/scheme-name&gt; ... &lt;/cache-mapping&gt; &lt;distributed-scheme&gt; ... &lt;scheme-name&gt;distributed-rwbm-nonblocking&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.company.NonBlockingStoreImpl&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; ... &lt;/distributed-scheme&gt; &lt;/cache-config&gt; Implementation Once configured, a class implementing the NonBlockingEntryStore interface needs to be added to the added to the classpath of the storage enabled members. See below for example code. With the class in place, the equivalency below is established: get() -invokes&#8594; load() Note: If data is already in the cache, load() does not get called. Also, calling get() will wait for onNext() / onError() to complete before returning. getAll() -invokes&#8594; loadAll() put() -invokes&#8594; store() putAll() -invokes&#8594; storeAll() remove() -invokes&#8594; erase() removeAll() -invokes&#8594; eraseAll() The code below contains portions of code is using a reactive API to access a data source. <markup lang=\"java\" >... /** * An example NonBlockingEntryStore implementation */ public class ExampleNonBlockingEntryStore&lt;K, V&gt; { @Override public void load(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer) { K key = binEntry.getKey(); Flux.from(getConnection()) .flatMap(connection -&gt; connection.createStatement(LOAD_STMT) .bind(\"$1\", key) .execute()) .flatMap(result -&gt; result.map((row, meta) -&gt; { return new Student( (String) row.get(\"name\"), (String) row.get(\"address\")); } )) .collectList() .doOnNext(s -&gt; { binEntry.setValue((V) s.get(0)); observer.onNext(binEntry); }) .doOnError(t -&gt; { if (t instanceof IndexOutOfBoundsException) { CacheFactory.log(\"Could not find row for key: \" + key); } else { CacheFactory.log(\"Error: \" + t); } observer.onError(binEntry, new Exception(t)); }) .subscribe(); } ... @Override public void store(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer) { K key = binEntry.getKey(); Student oStudent = (Student) binEntry.getValue(); Flux.from(getConnection()) .flatMap(connection -&gt; connection.createStatement(STORE_STMT) .bind(\"$1\", key) .bind(\"$2\", oStudent.getName()) .bind(\"$3\", oStudent.getAddress()) .execute()) .flatMap(Result::getRowsUpdated) .doOnNext((s) -&gt; { CacheFactory.log(\"store done, rows updated: \" + s); observer.onNext(binEntry); }) .doOnError(t -&gt; new Exception(t)) .subscribe(); } ... private static final String STORE_STMT = \"INSERT INTO student VALUES ($1, $2, $3) ON conflict (id) DO UPDATE SET name=$2, address=$2\"; private static final String LOAD_STMT = \"SELECT NAME, ADDRESS FROM student WHERE id=$1\"; Be sure to consult these best practices when implementing an entry store for your data sources. ",
            "title": "How to Use"
        },
        {
            "location": "/docs/core/08_non_blocking",
            "text": " Coherence provides a means of integrating with underlying data sources using a number of existing strategies; namely, read-through, write-through, write-behind and refresh-ahead. Coherence now also provides a NonBlockingEntryStore interface for integrating with data sources that provide non-blocking APIs. This new strategy is similar in nature to write-behind, as it is asynchronous to the original mutation, however does not require a queue to defer the call to the store and immediately passes the intent to store to the implementer. The implementer can in-turn immediately call the non-blocking api of the data source and on success or failure a future can pass that information to the provided StoreObserver via onNext or onError respectively. The primary methods of the NonBlockingEntryStore are highlighted below: <markup lang=\"java\" >public interface NonBlockingEntryStore&lt;K, V&gt; { public void load(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer); public void store(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer); public void erase(BinaryEntry&lt;K, V&gt; binEntry); } There are similar methods to the above in existing CacheStore and BinaryEntryStore interfaces, however with the NonBlockingEntryStore interface the calls are non-blocking thus Coherence does not expect the operation to be completed when control is returned. To allow the implementer to notify Coherence of operation completion, the StoreObserver is provided and must be called upon success or failure. This allows Coherence to process the result of the operation. It is worth pointing out that similar to the write-behind strategy upon failure, and therefore restore of primary partitions, Coherence will call NonBlockingEntryStore.store for the entries it did not receive a success or error notification for. This provides at least once semantics allowing implementers to call the non-blocking data source if deemed necessary. The diagrams below illustrate the flow from the initial request, to the invocation of the NonBlockingEntryStore on the storage enabled nodes: for a get() operation inducing a load: the application calls get() on entry A that is not in the cache yet. a request goes to the storage member that owns the entry, in this instance JVM2. Entry ownership, and thus partition ownership, is determined algorithmically based on the raw (or binary) value of the key and the number of partitions the associated partitioned service is configured with. Since it has not been accessed yet or has expired, a miss takes place and the call is relayed to the configure entry store. the load() operation for the entry store that implements NonBlockingEntryStore is called; custom logic is provided a BinaryEntry with a null initial value and a StoreObserver . The implementer performs the datastore operation(s) necessary to populate the cache entry. When the operation on the underlying data source completes, the implementation will call either observer.onNext or observer.onError , whether the value was successfully loaded or not. The implementer will update the BinaryEntry via setValue or updateBinaryValue , prior to calling onNext . This will allow Coherence to ensure data is inserted in the primary partition owner (JVM2) and backed up accordingly. the primary partition owner sends the value to another storage member in the cluster for backup purposes. the entry value is sent back to the calling application where a transient reference is kept. Note that although the data source operation can be performed asynchronously and the call to load() does not need to wait for its completion to return, the get() invocation is synchronous from the caller&#8217;s perspective. for a put() operation: the application calls put() on entry A with value A . the entry is stored on the owning member. since the cache is configured with a NonBlockingEntryStore , the store() operation is called. store() is provided a BinaryEntry and a a StoreObserver . The implementer performs the datastore operation(s) necessary to save the cache entry into a datastore. at this point, the store() call of the NonBlockingEntryStore can return, and put() will then give control back to the calling application. the datastore asynchronously performs the datastore operation(s) necessary to save the cache entry into a datastore, then calls the observer.onNext() method for normal operations (or observer.onError() in case of a problem). If necessary (for example, the value of the BinaryEntry has been updated), the value is put back into the cache. the value is then sent to the backup owning member for safekeeping. getAll() functions comparably to get() , except it processes a set of entries. This provides an opportunity for an implementer to optimize batch operations (multi-entry) against the datasource thus reduce the communication overhead with the datasource. Once the associated entry has been successfully written the implementer must call StoreObserver.onNext passing the relevant entry (or onError() if an error occurred processing this particular entry). Note Coherence expects all entries to be processed before concluding. putAll() also functions comparably to put() , except on a set of entries. The same expectation is in effect here: either all entries are processed using onNext()/onError() , or onComplete() can be used to interrupt the operation. The difference with putAll() is that the caller will not wait for completion, thus any exception will not be thrown but printed out in the log. the remove() operation functions in the same way as CacheStore or BinaryEntryStore from the application standpoint. Besides providing a natural way of integrating with non-blocking data stores, this model takes advantage of the benefits of such stores in terms of performance and scalability. NonBlockingEntryStore Certain data source libraries have APIs that do not necessitate the caller to wait for the result to come back before doing something else. For example, making HTTP calls can lead to relatively long waits between the time a request to store data is sent and the response comes back. By implementing non-blocking APIs, the caller can immediately do other work without having to wait for the actual store operation to complete. By implementing the NonBlockingEntryStore interface, the store implementer will be able to use non-blocking APIs in a more natural way. NonBlockingEntryStore is being provided in the context of pluggable data stores : in order to use it, an implementation class needs to be provided and configured. This class will either load, store or remove data from the data source by way of a ReadWriteBackingMap . This backing map provides two elements: an internal map to cache the data, and a data source access portion to interact with the data base. The NonBlockingEntryStore interface is provided the BinaryEntry that represents the load , store or erase operation. This provides an opportunity for implementers to avoid deserialization if desired; this is similar to BinaryEntryStore . Avoiding deserialization generally is possible if the raw binary is stored in the downstream system, or the binary can be navigated to extract relevant parts, as opposed to deserializing the entire key or value. Note: getKey , getValue and getOriginalValue will induce deserialization for the first call. How to Use Below is a summary of the tasks required in order to use the feature. To get started, please check the guide for non blocking stores . Configuration To specify a non-blocking cache store implementation, provide the implementation class name within the read-write-backing-map-scheme as shown below. <markup lang=\"xml\" >... &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;cache-mapping&gt; ... &lt;cache-name&gt;myCache&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-rwbm-nonblocking&lt;/scheme-name&gt; ... &lt;/cache-mapping&gt; &lt;distributed-scheme&gt; ... &lt;scheme-name&gt;distributed-rwbm-nonblocking&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.company.NonBlockingStoreImpl&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; ... &lt;/distributed-scheme&gt; &lt;/cache-config&gt; Implementation Once configured, a class implementing the NonBlockingEntryStore interface needs to be added to the added to the classpath of the storage enabled members. See below for example code. With the class in place, the equivalency below is established: get() -invokes&#8594; load() Note: If data is already in the cache, load() does not get called. Also, calling get() will wait for onNext() / onError() to complete before returning. getAll() -invokes&#8594; loadAll() put() -invokes&#8594; store() putAll() -invokes&#8594; storeAll() remove() -invokes&#8594; erase() removeAll() -invokes&#8594; eraseAll() The code below contains portions of code is using a reactive API to access a data source. <markup lang=\"java\" >... /** * An example NonBlockingEntryStore implementation */ public class ExampleNonBlockingEntryStore&lt;K, V&gt; { @Override public void load(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer) { K key = binEntry.getKey(); Flux.from(getConnection()) .flatMap(connection -&gt; connection.createStatement(LOAD_STMT) .bind(\"$1\", key) .execute()) .flatMap(result -&gt; result.map((row, meta) -&gt; { return new Student( (String) row.get(\"name\"), (String) row.get(\"address\")); } )) .collectList() .doOnNext(s -&gt; { binEntry.setValue((V) s.get(0)); observer.onNext(binEntry); }) .doOnError(t -&gt; { if (t instanceof IndexOutOfBoundsException) { CacheFactory.log(\"Could not find row for key: \" + key); } else { CacheFactory.log(\"Error: \" + t); } observer.onError(binEntry, new Exception(t)); }) .subscribe(); } ... @Override public void store(BinaryEntry&lt;K, V&gt; binEntry, StoreObserver&lt;K, V&gt; observer) { K key = binEntry.getKey(); Student oStudent = (Student) binEntry.getValue(); Flux.from(getConnection()) .flatMap(connection -&gt; connection.createStatement(STORE_STMT) .bind(\"$1\", key) .bind(\"$2\", oStudent.getName()) .bind(\"$3\", oStudent.getAddress()) .execute()) .flatMap(Result::getRowsUpdated) .doOnNext((s) -&gt; { CacheFactory.log(\"store done, rows updated: \" + s); observer.onNext(binEntry); }) .doOnError(t -&gt; new Exception(t)) .subscribe(); } ... private static final String STORE_STMT = \"INSERT INTO student VALUES ($1, $2, $3) ON conflict (id) DO UPDATE SET name=$2, address=$2\"; private static final String LOAD_STMT = \"SELECT NAME, ADDRESS FROM student WHERE id=$1\"; Be sure to consult these best practices when implementing an entry store for your data sources. ",
            "title": "Non Blocking Data Sources"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " First and foremost, Coherence provides a fundamental service that is responsible for all facets of clustering and is a common denominator / building block for all other Coherence services. This service, referred to as 'service 0' internally, ensures the mesh of members is maintained and responsive, taking action to collaboratively evict, shun, or in some cases voluntarily depart the cluster when deemed necessary. As members join and leave the cluster, other Coherence services are notified thus allows those services to react accordingly. This part of the Coherence product has been in production for 10+ years, being the subject of some extensive and imaginative testing. While it has been discussed here it certainly is not something that customers, generally, interact with directly but is valuable to be aware of. Coherence services build on top of the clustering service, with the key implementations to be aware of are PartitionedService, InvocationService, and ProxyService. In the majority of cases customers will deal with caches; a cache will be represented by an implementation of NamedCache&lt;K,V&gt; . Cache is an unfortunate name, as many Coherence customers use Coherence as a system-of-record rather than a lossy store of data. A cache is hosted by a service, generally the PartitionedService, and is the entry point to storing, retrieving, aggregating, querying, and streaming data. There are a number of features that caches provide: Fundamental key-based access : get/put getAll/putAll Client-side and storage-side events MapListeners to asynchronously notify clients of changes to data EventInterceptors (either sync or async) to be notified storage level events, including mutations, partition transfer, failover, etc NearCaches - locally cached data based on previous requests with local content invalidated upon changes in storage tier ViewCaches - locally stored view of remote data that can be a subset based on a predicate and is kept in sync real time Queries - distributed, parallel query evaluation to return matching key, values or entries with potential to optimize performance with indices Aggregations - a map/reduce style aggregation where data is aggregated in parallel on all storage nodes and results streamed back to the client for aggregation of those results to produce a final result Data local processing - an ability to send a function to the relevant storage node to execute processing logic for the appropriate entries with exclusive access Partition local transactions - an ability to perform scalable transactions by associating data (thus being on the same partition) and manipulating other entries on the same partition potentially across caches Non-blocking / async NamedCache API C&#43;&#43; and .NET clients - access the same NamedCache API from either C&#43;&#43; or .NET Portable Object Format - optimized serialization format, with the ability to navigate the serialized form for optimized queries, aggregations, or data processing Integration with Databases - Database &amp; third party data integration with CacheStores including both synchronous or asynchronous writes CohQL - ansi-style query language with a console for adhoc queries Topics - distributed topics implementation offering pub/sub messaging with the storage capacity the cluster and parallelizable subscribers There are also a number of non-functional features that Coherence provides: Rock solid clustering - highly tuned and robust clustering stack that allows Coherence to scale to thousands of members in a cluster with thousands of partitions and terabytes of data being accessed, mutated, queried and aggregated concurrently Safety first - resilient data management that ensures backup copies are on distinct machines, racks, or sites and the ability to maintain multiple backups 24/7 Availability - zero down time with rolling redeploy of cluster members to upgrade application or product versions Backwards and forwards compatibility of product upgrades, including major versions Persistent Caches - with the ability to use local file system persistence (thus avoid extra network hops) and leverage Coherence consensus protocols to perform distributed disk recovery when appropriate Distributed State Snapshot - ability to perform distributed point-in-time snapshot of cluster state, and recover snapshot in this or a different cluster (leverages persistence feature) Lossy redundancy - ability to reduce the redundancy guarantee by making backups and/or persistence asynchronous from a client perspective Single Mangement View - provides insight into the cluster with a single JMX server that provides a view of all members of the cluster Management over REST - all JMX data and operations can be performed over REST, including cluster wide thread dumps and heapdumps Non-cluster Access - access to the cluster from the outside via proxies, for distant (high latency) clients and for non-java languages such as C&#43;&#43; and .NET Kubernetes friendly - seamlessly and safely deploy applications to k8s with our own operator ",
            "title": "Introduction"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Coherence Community Edition does not include the following Oracle Coherence commercial edition functionality Management of Coherence via the Oracle WebLogic Management Framework WebLogic Server Multi-tenancy support Deployment of Grid Archives (GARs) HTTP session management for application servers (Coherence*Web) GoldenGate HotCache TopLink-based CacheLoaders and CacheStores Elastic Data Federation and WAN (wide area network) support Transaction Framework CommonJ work manager ",
            "title": "Coherence Community Edition Disabled and Excluded Functionality"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Java - jdk8 or higher Maven - 3.6.3 or higher ",
            "title": "Prerequisites"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "How to Get Coherence Community Edition"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Prerequisites Java - jdk8 or higher Maven - 3.6.3 or higher How to Get Coherence Community Edition As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "Quick Start"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.12.6-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.12.6-SNAPSHOT/coherence-21.12.6-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 ",
            "title": " CohQL Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.12.6-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.12.6-SNAPSHOT/coherence-21.12.6-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": " Coherence Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates the procedure to start a storage enabled Coherence Server, followed by a storage disabled Coherence Console. Using the console, data is inserted, retrieved, and then the console is terminated. The console is restarted and data is once again retrieved to illustrate the permanence of the data. This example uses the out-of-the-box cache configuration and therefore explicitly specifying the console is storage disabled is unnecessary. Coherence cluster members discover each other via one of two mechanisms; multicast (default) or Well Known Addressing (deterministic broadcast). If your system does not support multicast, enable WKA by specifying -Dcoherence.wka=localhost for both processes started in the following console examples. CohQL Console To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.12.6-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.12.6-SNAPSHOT/coherence-21.12.6-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 Coherence Console To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=21.12.6-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/21.12.6-SNAPSHOT/coherence-21.12.6-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": "CLI Hello Coherence"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": "Build HelloCoherence "
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates starting a storage enabled Coherence server, followed by running the HelloCoherence application. The HelloCoherence application inserts and retrieves data from the Coherence server. Build HelloCoherence Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": " Programmatic Hello Coherence Example"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": "<markup lang=\"shell\" >$&gt; git clone git@github.com:oracle/coherence.git $&gt; cd coherence/prj # build all modules $&gt; mvn clean install # build all modules skipping tests $&gt; mvn clean install -DskipTests # build a specific module, including all dependent modules and run tests $&gt; mvn -am -pl test/functional/persistence clean verify # build only coherence.jar without running tests $&gt; mvn -am -pl coherence clean install -DskipTests # build only coherence.jar and skip compilation of CDBs and tests $&gt; mvn -am -pl coherence clean install -DskipTests -Dtde.compile.not.required ",
            "title": " Building"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI provides support for CDI (Contexts and Dependency Injection) within Coherence cluster members. It allows you both to inject Coherence-managed resources, such as NamedMap , NamedCache and Session instances into CDI managed beans, to inject CDI beans into Coherence-managed resources, such as event interceptors and cache stores, and to handle Coherence server-side events using CDI observer methods. In addition, Coherence CDI provides support for automatic injection of transient objects upon deserialization. This allows you to inject CDI managed beans such as services and repositories (to use DDD nomenclature) into transient objects, such as entry processor and even data class instances, greatly simplifying implementation of true Domain Driven applications. ",
            "title": "Coherence CDI"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Inject Views"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import javax.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject @Name(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Injecting NamedMap , NamedCache and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"payments-cluster.xml\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import javax.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; ",
            "title": "Injecting NamedTopic and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; ",
            "title": " Cluster and OperationalContext Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; ",
            "title": "Named Session Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; ",
            "title": " Serializer Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Inject a POF Serializer With a Specific POF Configuration"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Other Supported Injection Points"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import javax.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject @Name(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"payments-cluster.xml\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import javax.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } ",
            "title": "Observe Events for Maps and Caches in Specific Services and Scopes"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using CDI Observers to Handle Coherence Server-Side Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Injecting CDI Beans into Coherence-managed Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Making transient classes Injectable "
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Injecting CDI Beans into Transient Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . ",
            "title": "Create the Custom Filter Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. ",
            "title": "Create the Custom Filter Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "FilterBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ",
            "title": "PropertyExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); ",
            "title": "ChainedExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "PofExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "Built-In ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. ",
            "title": "Custom ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "MapEventTransformerBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to use Coherence CDI, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using CDI to inject Coherence objects into managed CDI beans, and vice versa, as the following sections describe. Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache`, and related objects Injecting NamedMap or NamedCache Views Injecting NamedTopic and related objects Other Supported Injection Points Cluster and OperationalContext Injection Named Session Injection Serializer Injection Injecting CDI Beans into Coherence-managed Objects Using CDI Observers to Handle Coherence Server-Side Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Injecting CDI Beans into Transient Objects Making transient classes Injectable Filter Binding Annotations Extractor Binding Annotations Built-In Extractor Binding Annotations @PropertyExtractor @ChainedExtractor @PofExtractor Custom Extractor Binding Annotations MapEventTransformer Binding Annotations Injecting Coherence Objects into CDI Beans CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import javax.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject @Name(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import javax.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import javax.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import javax.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import javax.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"payments-cluster.xml\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import javax.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import javax.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import javax.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import javax.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import javax.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import javax.enterprise.context.ApplicationScoped; import javax.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import javax.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. Injecting CDI Beans into Coherence-managed Objects Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. Injecting CDI Beans into Transient Objects Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. FilterBinding Annotations As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ExtractorBinding Annotations Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { MapEventTransformerBinding Annotations Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Usage"
        },
        {
            "location": "/docs/core/01_overview",
            "text": " Coherence has a number of core improvements documented in this section. fa-rocket Bootstrap API Bootstrap Coherence application. library_books Parallel Recovery Recover data from Coherence&#8217;s Persistence mechanism in parallel. settings_ethernet Portable Types Implement versioned data classes that can evolve over time. fa-sitemap Repository API Higher level, DDD-friendly data access API. fa-backward Durable Events On demand event replay. import_contacts Partition Events Logging Logging of events that render partitions unavailable. extension Non-Blocking Data Sources Integration with data sources with non-blocking APIs. fa-cubes Partition Backup Enhancements Changes to partition backup management. ",
            "title": "Overview"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " In addition to the basic CRUD functionality, the Repository API provides many features that simplify common data management tasks: Powerful projection features Flexible in-place entity updates First-class data aggregation support Stream API support Asynchronous API support Event listener support Declarative acceleration and index creation CDI Support ",
            "title": "Features and Benefits"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Coherence provides an abstract base class com.oracle.coherence.repository.AbstractRepository , which your custom repository implementation needs to extend and provide implementation of three abstract methods: <markup lang=\"java\" > /** * Return the identifier of the specified entity instance. * * @param entity the entity to get the identifier from * * @return the identifier of the specified entity instance */ protected abstract ID getId(T entity); /** * Return the type of entities in this repository. * * @return the type of entities in this repository */ protected abstract Class&lt;? extends T&gt; getEntityType(); /** * Return the map that is used as the underlying entity store. * * @return the map that is used as the underlying entity store */ protected abstract M getMap(); For example, a repository implementation that can be used to store Person entities, with String identifiers, can be as simple as: <markup lang=\"java\" >public class PeopleRepository extends AbstractRepository&lt;String, Person&gt; { private NamedMap&lt;String, Person&gt; people; public PeopleRepository(NamedMap&lt;String, Person&gt; people) { this.people = people; } protected NamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person person) { return person.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the NamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository That is it in a nutshell: a trivial repository implementation above will allow you to access all the Repository API features described in the remaining sections, which are provided by the AbstractRepository class you extended. However, you are free (and encouraged) to add additional business methods to the repository class above that will make it easier to use within your application. The most common example of such methods would be various \"finder\" methods that your application needs. For example, if your application needs to frequently query the repository to find people based on their name, you may want to add a method for that purpose: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return getAll(filter); } You can then invoke findByName method directly within the application to find all the people whose first or last name starts with a letter A , for example: <markup lang=\"java\" >for (Person p : people.findByName(\"A%\")) { // processing } ",
            "title": "Implementing a Repository"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " We&#8217;ve already seen one read operation, getAll , in the example above, but let&#8217;s start from the beginning and look into how we can add, remove, update and query our repository. To add new entities to the repository, or replace the existing ones, you can use either the save or the saveAll method. The former takes a single entity as an argument and stores it in the backing NamedMap : <markup lang=\"java\" >people.save(new Person(\"555-11-2222\", \"Aleks\", 46)); The latter allows you to store a batch of entities at once by passing either a collection or a stream of entities as an argument. Once you have some entities stored in a repository, you can query the repository using get and getAll methods. <markup lang=\"java\" >Person person = people.get(\"555-11-2222\"); assert person.getName().equals(\"Aleks\"); assert person.getAge() == 46; Collection&lt;Person&gt; allPeople = people.getAll(); Collection&lt;Person&gt; allAdults = people.getAll(Filters.greaterOrEqual(Person::getAge, 18)); get a single Person by identifier get all the people from the repository get all the people from the repository that are 18 or older You can retrieve sorted results by calling getAllOrderedBy method and specifying a Comparable property via a method reference: <markup lang=\"java\" >Collection&lt;Person&gt; peopleOrderedByAge = people.getAllOrderedBy(Person::getAge) the result will contain all people from the repository, sorted by age from the youngest to the oldest For more complex use cases, you can specify a Comparator to use instead. For example, if we wanted to always sort the results of the findByName method defined above first by last name and then by first name, we could re-implement it as: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return getAllOrderedBy(filter, Remote.comparator(Person::getLastName) .thenComparing(Person::getFirstName)); } the results will be sorted by last name, and then by first name; note that we are using Coherence Remote.comparator instead of standard Java Comparator in order to ensure that the specified comparator is serializable and can be sent to remote cluster members Finally, to remove entities from a repository you can use one of the several remove methods: <markup lang=\"java\" >boolean fRemoved = people.remove(person); boolean fRemoved = people.removeById(\"111-22-3333\"); removes specified entity from the repository removes entity with the specified identifier from the repository In both examples above the result will be a boolean indicating whether the entity was actually removed from the backing NamedMap , and it may be false if the entity wasn&#8217;t present in the repository. If you are interested in the removed value itself, you can use the overloads of the methods above that allow you to express that: <markup lang=\"java\" >Person removed = people.remove(person, true); Person removed = people.removeById(\"111-22-3333\", true); removes specified entity from the repository and returns it as the result removes entity with the specified identifier from the repository and returns it as the result Note that this will result in additional network traffic, so unless you really need the removed entity it is probably best not to ask for it. The examples above are useful when you want to remove a single entity from the repository. In cases when you want to remove multiple entities as part of a single network call, you should use one of removeAll methods instead, which allow you to remove a set of entities by specifying either their identifiers explicitly, or the criteria for removal via the Filter . <markup lang=\"java\" >boolean fChanged = people.removeAll(Filters.equal(Person::getGender, Gender.MALE)); boolean fChanged = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\")); removes all men from the repository and returns true if any entity has been removed removes entities with the specified identifiers from the repository and returns true if any entity has been removed Just like with single-entity removal operations, you can also use overloads that allow you to return the removed entities as the result: <markup lang=\"java\" >Map&lt;String, Person&gt; mapRemoved = people.removeAll(Filters.equal(Person::getGender, Gender.MALE), true); Map&lt;String, Person&gt; mapRemoved = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\"), true); removes all men from the repository and returns the map of removed entities, keyed by identifier removes entities with the specified identifiers from the repository and returns the map of removed entities, keyed by identifier ",
            "title": "Basic CRUD Operations"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " While querying repository for a collection of entities that satisfy some criteria is certainly a common and useful operation, sometimes you don&#8217;t need all the attributes within the entity. For example, if you only need a person&#8217;s name, querying for and then discarding all the information contained within the Person instances is unnecessary and wasteful. It is the equivalent of executing <markup lang=\"sql\" >SELECT * FROM PEOPLE against a relational database, when a simple <markup lang=\"sql\" >SELECT name FROM PEOPLE would suffice. Coherence Repository API allows you to limit the amount of data collected by performing server-side projection of the entity attributes you are interested in. For example, if you only need a person&#8217;s name, you can get just the name: <markup lang=\"java\" >String name = people.get(\"111-22-3333\", Person::getName); Map&lt;String, String&gt; mapNames = people.getAll(Filters.less(Person::getAge, 18), Person::getName); return the name of the person with a specified identifier return the map of names of all the people younger than 18, keyed by person&#8217;s identifier Obviously, returning either the whole entity or a single attribute from an entity are two ends of the spectrum, and more often than not you need something in between. For example, you may need the person&#8217;s name and age. For situations like that, Coherence allows you to use fragments : <markup lang=\"java\" >Fragment&lt;Person&gt; fragment = people.get(\"111-22-3333\", Extractors.fragment(Person::getName, Person::getAge)); String name = fragment.get(Person::getName); int age = fragment.get(Person::getAge); return a fragment containing the name and age of the person with a specified identifier retrieve the person&#8217;s name from a fragment retrieve the person&#8217;s age from a fragment You can, of course, perform the same projection across multiple entities using one of getAll methods: <markup lang=\"java\" >Map&lt;String, Fragment&lt;Person&gt;&gt; fragments = people.getAll( Filters.less(Person::getAge, 18), Extractors.fragment(Person::getName, Person::getAge)); return a map of fragments containing the name and age of all the people younger than 18, keyed by person&#8217;s identifier Unlike the relational database, which contains a set of columns for each row in the table, Coherence stores each entity as a full object graph, which means that the attributes can be other object graphs and can be nested to any level. This means that we also may need to be able to project attributes of the nested objects. For example, our Person class may have a nested Address object as an attribute, which in turn has street , city , and country attributes. If we want to retrieve the name and the country of a person in a repository, we can do it like this: <markup lang=\"java\" >Fragment&lt;Person&gt; person = people.get( \"111-22-3333\", Extractors.fragment(Person::getName, Extractors.fragment(Person::getAddress, Address::getCountry))); String name = person.get(Person::getName); Fragment&lt;Address&gt; address = person.getFragment(Person::getAddress); String country = address.get(Address::getCountry); return a fragment containing the name and the Address fragment of the person with a specified identifier retrieve the person&#8217;s name from the Person fragment retrieve the Address fragment from the Person fragment retrieve the person&#8217;s country from the Address fragment ",
            "title": "Projection"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " By far the most common approach for updating data in modern applications is the read-modify-write pattern. For example, the typical code to update an attribute of a Person may look similar to the following: <markup lang=\"java\" >Person person = people.get(\"111-22-3333\"); person.setAge(55); people.save(person); This is true regardless of whether the underlying data store provides a better, more efficient way of updating data. For example, RDBMS provide stored procedures for that purpose, but very few developers use them because they are not as convenient to use, and do not fit well into popular application frameworks, such as JPA, Spring Data or Micronaut Data. They also fragment the code base to some extent, splitting the business logic across the application and the data store, and require that some application code is written in SQL. However, the approach above is suboptimal, for a number of reasons: It at least doubles the number of network calls the application makes to the data store, increasing the overall latency of the operation. It moves (potentially a lot) more data over the network than absolutely necessary. It may require expensive construction of a complex entity in order to perform a very simple update operation of a single attribute (this is particularly true with JPA and RDBMS back ends). It puts additional, unnecessary load on the data store, which is typically the hardest component of the application to scale. It introduces concurrency issues (ie. what should happen if the entity in the data store changes between the initial read and subsequent write), which typically requires that both the read and the write happen within the same transaction. A much better, more efficient way to perform the updates is to send the update function to the data store, and execute it locally, within the data store itself (which is pretty much what stored procedures are for). Coherence has always had support for these types of updates via entry processors , but the Repository API makes it even simpler to do so. For example, the code above can be rewritten as: <markup lang=\"java\" >people.update(\"111-22-3333\", Person::setAge, 55); We are basically telling Coherence to update Person instance with a given identifier by calling setAge method on it with a number 55 as an argument. This is not only significantly more efficient, but I&#8217;m sure you&#8217;ll agree, shorter and easier to write, and to read. Note that we don&#8217;t know, or care, where in the cluster a Person instance with a given identifier is&#8201;&#8212;&#8201;all we care about is that Coherence guarantees that it will invoke the setAge method on the entity with a specified ID, on a primary owner , and automatically create a backup of the modified entity for fault tolerance. It is also worth pointing out that the approach above provides the same benefits stored procedures do in RDBMS, but without the downsides: you are still writing all your code in Java, and keeping it in the same place. As a matter of fact, this approach allows you to implement rich domain models for your data, and execute business logic on your entities remotely, which works exceptionally well with DDD applications. Calling a setter on an entity remotely is only the tip of the iceberg, and far from sufficient for all data mutation needs. For example, conventional JavaBean setter returns void , but you often want to know what the entity value is after the update. The solution to that problem is simple: Coherence will return the result of the specified method invocation, so all you need to do is change the setAge method to implement fluent API: <markup lang=\"java\" >public Person setAge(int age) { this.age = age; return this; } You will now get the modified Person instance as the result of the update call: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", Person::setAge, 55); assert person.getAge() == 55; Sometimes you need to perform more complex updates, or update multiple attributes at the same time. While you could certainly accomplish both of those by making multiple update calls, that is inefficient because each update will result in a separate network call. You are better off using the update overload that allows you to specify the function to execute in that situation: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", p -&gt; { p.setAge(55); p.setGender(Gender.MALE); return p; }); assert person.getAge() == 55; assert person.getGender() == Gender.MALE; This way you have full control of the update logic that will be executed, and the return value. You may sometimes want to update an entity that does not exist in the repository yet, in which case you want to create a new instance. For example, you may want to create a shopping cart entity for a customer when they add the first item to the cart. While you could implement the code to check whether the Cart for a given customer exists, and create new one if it doesn&#8217;t, this again results in network calls that can be avoided if you simply create the Cart instance as part of Cart::addItem call. The Repository API allows you to accomplish that via optional EntityFactory argument: <markup lang=\"java\" >carts.update(customerId, Cart::addItem, item, Cart::new); the cart/customer identifier the method to invoke on a target Cart instance the CartItem to add to the cart the EntityFactory to use to create a new Cart instance if the cart with the specified identifier doesn&#8217;t exist The EntityFactory interface is quite simple: <markup lang=\"java\" >@FunctionalInterface public interface EntityFactory&lt;ID, T&gt; extends Serializable { /** * Create an entity instance with the specified identity. * * @param id identifier to create entity instance with * * @return a created entity instance */ T create(ID id); } Basically, it has a single create method that accepts entity identifier and returns a new instance of the entity with a given identifier. In the example above, that implies that our Cart class has a constructor similar to this: <markup lang=\"java\" >public Cart(Long cartId) { this.cartId = cartId; } Just like with projections and other operations, in addition to update methods that can be used to modify a single entity, there are also a number of updateAll methods that can be used to modify multiple entities in a single call. An example where this may be useful is when you want to apply the same exact function to multiple entities, as is the case when performing stock split: <markup lang=\"java\" >positions.updateAll( Filters.equal(Position::getSymbol, \"AAPL\"), Position::split, 5); the Filter used to determine the set of positions to update the function to apply to each position; in this case split(5) will be called on each Position entity with AAPL symbol Just like with single-entity updates, the result of each function invocation will be returned to the client, this time in the form of a Map containing the identifiers of the processed entities as keys, and the result of the function applied to that entity as the value. ",
            "title": "In-place Updates"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " I mentioned earlier that Coherence can use indexes to optimize queries and aggregations. The indexes allow you to avoid deserializing entities stored across the cluster, which is a potentially expensive operation when you have large data set, with complex entity classes. The indexes themselves can also be sorted, which is helpful when executing range-based queries, such as less , greater or between . The standard way to create indexes is by calling NamedMap.addIndex method, which is certainly still an option. However, Repository API introduces a simpler, declarative way of index creation. To define an index, simply annotate the accessor for the entity attribute(s) that you&#8217;d like to create an index for with @Indexed annotation: <markup lang=\"java\" >public class Person { @Indexed public String getName() { return name; } @Indexed(ordered = true) public int getAge() { return age; } } defines an unordered index on Person::getName , which is suitable for filters such as equal , like , and regex defines an ordered index on Person::getAge , which is better suited for filters such as less , greater and between When the repository is created, it will introspect the entity class for @Indexed annotation and automatically create an index for each attribute that has one. The created index will then be used whenever that attribute is referenced within the query expression. In some cases you may want to keep deserialized entity instances around instead of discarding them. This can be useful when you are making frequent queries, aggregations, and using Stream API, or even in-place updates or projection, as the cost of maintaining individual indexes on all the attributes you need may end up being greater than to simply keep deserialized entity instances around. For situations like that Coherence provides a special index type you can use, DeserializationAccelerator , but if you are using Repository API you once again have an easier way of configuring it&#8201;&#8212;&#8201;simply annotate either the entity class, or the repository class itself with the @Accelerated annotation: <markup lang=\"java\" >@Accelerated public class Person { } Obviously, you will require additional storage capacity in your cluster in order to be able to store both the serialized and deserialized copy of all the entities, but in some situations the performance benefits can significantly outweigh the cost. In other words, acceleration is a classic example of a time vs. space tradeoff, and it is entirely up to you to decide when it makes sense to use it. ",
            "title": "Declarative Acceleration and Index Creation"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " We&#8217;ve already covered how you can query the repository to retrieve a subset of entities using a getAll method and a Filter , but sometimes you don&#8217;t need the entities themselves, but a result of some computation applied to a subset of entities in the repository. For example, you may need to calculate average salary of all the employees in a department, or the total value of all equity positions in a portfolio. While you could certainly query the repository for the entities that need to be processed and perform processing itself on the client, this is very inefficient way to accomplish the task, as you may end up moving significant amount of data over the network, just to discard it after the client-side processing. As you&#8217;ve probably noticed by now, Coherence provides a number of feature that allow you to perform various types of distributed processing efficiently, and this situation is no exception. Just like the in-place updates leverage Coherence Entry Processor API to perform data mutation on cluster members that store the data, Repository API support for data aggregation leverages Coherence Remote Stream API and the Aggregation API to perform read-only distributed computations efficiently. This once again allows you to move processing to the data, instead of the other way around, and to perform computation in parallel across as many CPU cores as your cluster has, instead of a handful of (or in many cases only one) cores on the client. The first option is to use the Stream API, which you are probably already familiar with because it&#8217;s a standard Java API introduced in Java 8. For example, you could calculate the average salary of all employees like this: <markup lang=\"java\" >double avgSalary = employees.stream() .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); If you wanted to calculate average salary only for the employees in a specific department instead, you could filter the employees to process: <markup lang=\"java\" >double avgSalary = employees.stream() .filter(e -&gt; e.getDepartmentId == departmentId) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); However, while it works, the code above is not ideal, as it will end up processing, and potentially deserializing all the employees in the repository in order to determine whether they belong to a specified department. A better way to accomplish the same task is to use Coherence-specific stream method overload which allows you to specify the Filter to create a stream based on: <markup lang=\"java\" >double avgSalary = employees.stream(Filters.equal(Employee::getDepartmentId, departmentId)) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); The difference is subtle, but important: unlike previous example, this allows Coherence to perform query before creating the stream, and leverage any indexes you may have in the process. This can significantly reduce the overhead when dealing with large data sets. However, there is also an easier way to accomplish the same thing: <markup lang=\"java\" >double avgSalary = employees.average(Employee::getSalary); or, for a specific department: <markup lang=\"java\" >double avgSalary = employees.average( Filters.equal(Employee::getDepartmentId, departmentId), Employee::getSalary); These are the examples of using repository aggregation methods directly, which turn common tasks such as finding min , max , average and sum of any entity attribute as simple as it can be. There are also more advanced aggregations, such as groupBy and top : <markup lang=\"java\" >Map&lt;Gender, Set&lt;Person&gt;&gt; peopleByGender = people.groupBy(Person::getGender); Map&lt;Long, Double&gt; avgSalaryByDept = employees.groupBy(Employee::getDepartmentId, averagingDouble(Employee::getSalary)); List&lt;Double&gt; top5salaries = employees.top(Employee::getSalary, 5); as well as the simpler ones, such as count and distinct . Finally, in many cases you may care not only about min , max or top values of a certain attribute, but also about which entities those values belong to. For those situations, you can use minBy , maxBy and topBy methods, which returns the entities containing minimum, maximum and top values of an attribute, respectively: <markup lang=\"java\" >Optional&lt;Person&gt; oldestPerson = people.maxBy(Person::getAge); Optional&lt;Person&gt; youngestPerson = people.minBy(Person::getAge); List&lt;Employee&gt; highestPaidEmployees = employees.topBy(Employee::getSalary, 5); Declarative Acceleration and Index Creation I mentioned earlier that Coherence can use indexes to optimize queries and aggregations. The indexes allow you to avoid deserializing entities stored across the cluster, which is a potentially expensive operation when you have large data set, with complex entity classes. The indexes themselves can also be sorted, which is helpful when executing range-based queries, such as less , greater or between . The standard way to create indexes is by calling NamedMap.addIndex method, which is certainly still an option. However, Repository API introduces a simpler, declarative way of index creation. To define an index, simply annotate the accessor for the entity attribute(s) that you&#8217;d like to create an index for with @Indexed annotation: <markup lang=\"java\" >public class Person { @Indexed public String getName() { return name; } @Indexed(ordered = true) public int getAge() { return age; } } defines an unordered index on Person::getName , which is suitable for filters such as equal , like , and regex defines an ordered index on Person::getAge , which is better suited for filters such as less , greater and between When the repository is created, it will introspect the entity class for @Indexed annotation and automatically create an index for each attribute that has one. The created index will then be used whenever that attribute is referenced within the query expression. In some cases you may want to keep deserialized entity instances around instead of discarding them. This can be useful when you are making frequent queries, aggregations, and using Stream API, or even in-place updates or projection, as the cost of maintaining individual indexes on all the attributes you need may end up being greater than to simply keep deserialized entity instances around. For situations like that Coherence provides a special index type you can use, DeserializationAccelerator , but if you are using Repository API you once again have an easier way of configuring it&#8201;&#8212;&#8201;simply annotate either the entity class, or the repository class itself with the @Accelerated annotation: <markup lang=\"java\" >@Accelerated public class Person { } Obviously, you will require additional storage capacity in your cluster in order to be able to store both the serialized and deserialized copy of all the entities, but in some situations the performance benefits can significantly outweigh the cost. In other words, acceleration is a classic example of a time vs. space tradeoff, and it is entirely up to you to decide when it makes sense to use it. ",
            "title": "Stream API and Data Aggregation"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Coherence not only allows you to store, modify, query and aggregate your data entities efficiently, but you can also register to receive event notifications whenever any entity in the repository changes. To do that, you can create and register a listener that will be notified whenever an entity is inserted, updated or removed: <markup lang=\"java\" > public static class PeopleListener implements PeopleRepository.Listener&lt;Person&gt; { public void onInserted(Person personNew) { // handle INSERT event } public void onUpdated(Person personOld, Person personNew) { // handle UPDATE event } public void onRemoved(Person personOld) { // handle REMOVE event } } <markup lang=\"java\" >people.addListener(new PeopleListener()); people.addListener(\"111-22-3333\", new PeopleListener()); people.addListener(Filters.greater(Person::getAge, 17), new PeopleListener()); registers a listener that will be notified whenever any entity in the repository is inserted, updated or removed registers a listener that will be notified when an entity with the specified identifier is inserted, updated or removed registers a listener that will be notified when any Person older than 17 is inserted, updated or removed As you can see from the example above, there are several ways to register only for the events you are interested in, in order to reduce the number of events received, and the amount of data sent over the network. Note that all of the listener methods above have a default no-op implementation, so you only need to implement the ones you actually want to handle. However, having to implement a separate class each time you want to register a listener is a bit cumbersome, so Repository API also provides a default listener implementation, and a fluent builder for it that make the task a bit easier: <markup lang=\"java\" >people.addListener( people.listener() .onInsert(personNew -&gt; { /* handle INSERT event */ }) .onUpdate((personOld, personNew) -&gt; { /* handle UPDATE event with old value */ }) .onUpdate(personNew -&gt; { /* handle UPDATE event without old value */ }) .onRemove(personOld -&gt; { /* handle REMOVE event */ }) .build() ); Note that when using Listener Builder API you have the option of omitting the old entity value from the onUpdate event handler arguments list. You can also specify multiple handlers for the same event type, in which case they will be composed and invoked in the specified order. There is also an option of providing a single event handler that will receive all the events, regardless of the event type: <markup lang=\"java\" >people.addListener( people.listener() .onEvent(person -&gt; { /* handle all events */ }) .build() ); Just like when implementing listener class explicitly, you can still pass entity identifier or a Filter as the first argument to addListener method in order to limit the scope of the events received. ",
            "title": "Event Listeners"
        },
        {
            "location": "/docs/core/05_repository",
            "text": "<markup lang=\"java\" >public class AsyncPeopleRepository extends AbstractAsyncRepository&lt;String, Person&gt; { private AsyncNamedMap&lt;String, Person&gt; people; public AsyncPeopleRepository(AsyncNamedMap&lt;String, Person&gt; people) { this.people = people; } protected AsyncNamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person entity) { return entity.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the AsyncNamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository An example using the AsyncPersonRepository make a simple query for an entity: <markup lang=\"java\" >String upercaseName = asyncPeople.get(\"111-22-3333\") .thenApply(Person::getName) .thenApply(String::toUpperCase) .get() Get a CompletableFuture&lt;Person&gt; based on their ID When the future is completed, obtain the person&#8217;s name from the Person instance Convert the name to uppercase Block and return the upper-cased name This usage pattern will be similar across all the methods that return CompletableFuture , which is pretty much all of them. ",
            "title": "AbstractAsyncRepository Examples"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Instead of dealing with an entire collection being realized for the results, it is possible to define a callback that will be invoked as results become available. These APIs will return CompletableFuture&lt;Void&gt; to signal all the results have been processed. For example, if we simply want to print out the people as they are streamed back from the server, without accumulating result set on the client, we can simply do the following: <markup lang=\"java\" >asyncPeople.getAll(person -&gt; System.out.println(person.getName())); .thenApply(done -&gt; System.out.println(\"DONE!\")) Print the name of each Person within the repository Print DONE! when all people have been processed Of course, you can also extract the name attribute only by providing a ValueExtractor for it as the first argument, in which case the code above could be rewritten to move less data over the network like this: <markup lang=\"java\" >asyncPeople.getAll(Person::getName, (id, name) -&gt; System.out.println(name)); .thenApply(done -&gt; System.out.println(\"DONE!\")) Print the name of each Person within the repository Print DONE! when all people have been processed In the example above, the callback is implemented as a BiConsumer that will receive entity identifer and the extracted value as arguments. Of course, we could&#8217;ve used fragment extractor as the first argument to getAll method above, in which case the second argument to the callback would&#8217;ve been Fragment&lt;Person&gt; instead of just the name attribute. ",
            "title": "Asynchronous Callbacks"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " In addition to the synchronous repository, AbstractRepository&lt;ID, T&gt; , we offer an asynchronous version, AbstractAsyncRepository&lt;ID, T&gt; . The same abstract methods as previously described will need to be implemented. The main differences between the two APIs is that the asynchronous API returns java.util.CompletableFuture of the return type. For example, Collection&lt;T&gt; getAll() in the blocking version would be CompletableFuture&lt;Collection&lt;T&gt;&gt; in the asynchronous version of the Repository API. The asynchronous API also offers callbacks that will be passed the results of the operation as they become available, instead of buffering the result into a collection prior to returning. This allows you to stream and process very large result sets without paying the cost of accumulating all the results in memory at once, which is not possible with the blocking API. AbstractAsyncRepository Examples <markup lang=\"java\" >public class AsyncPeopleRepository extends AbstractAsyncRepository&lt;String, Person&gt; { private AsyncNamedMap&lt;String, Person&gt; people; public AsyncPeopleRepository(AsyncNamedMap&lt;String, Person&gt; people) { this.people = people; } protected AsyncNamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person entity) { return entity.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the AsyncNamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository An example using the AsyncPersonRepository make a simple query for an entity: <markup lang=\"java\" >String upercaseName = asyncPeople.get(\"111-22-3333\") .thenApply(Person::getName) .thenApply(String::toUpperCase) .get() Get a CompletableFuture&lt;Person&gt; based on their ID When the future is completed, obtain the person&#8217;s name from the Person instance Convert the name to uppercase Block and return the upper-cased name This usage pattern will be similar across all the methods that return CompletableFuture , which is pretty much all of them. Asynchronous Callbacks Instead of dealing with an entire collection being realized for the results, it is possible to define a callback that will be invoked as results become available. These APIs will return CompletableFuture&lt;Void&gt; to signal all the results have been processed. For example, if we simply want to print out the people as they are streamed back from the server, without accumulating result set on the client, we can simply do the following: <markup lang=\"java\" >asyncPeople.getAll(person -&gt; System.out.println(person.getName())); .thenApply(done -&gt; System.out.println(\"DONE!\")) Print the name of each Person within the repository Print DONE! when all people have been processed Of course, you can also extract the name attribute only by providing a ValueExtractor for it as the first argument, in which case the code above could be rewritten to move less data over the network like this: <markup lang=\"java\" >asyncPeople.getAll(Person::getName, (id, name) -&gt; System.out.println(name)); .thenApply(done -&gt; System.out.println(\"DONE!\")) Print the name of each Person within the repository Print DONE! when all people have been processed In the example above, the callback is implemented as a BiConsumer that will receive entity identifer and the extracted value as arguments. Of course, we could&#8217;ve used fragment extractor as the first argument to getAll method above, in which case the second argument to the callback would&#8217;ve been Fragment&lt;Person&gt; instead of just the name attribute. ",
            "title": "Asynchronous Repository API"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " The Coherence Repository API was introduced to make the implementation of data access layer within the applications easier, regardless of which framework you use to implement applications that use Coherence as a data store. It works equally well for plain Java applications and applications that use CDI, where you can simply create your own repository implementations, as described at the beginning of this document. It is also the foundation for our Micronaut Data and Spring Data repository implementations, so all the functionality described here is available when using those frameworks as well. The only difference is how you define your own repositories, which is framework-specific and documented separately. We hope you&#8217;ll find this new feature useful, and that it will make implementation of your Coherence-backed data access layers even easier. ",
            "title": "Summary"
        },
        {
            "location": "/docs/core/05_repository",
            "text": " Coherence Repository API provides a higher-level, DDD-friendly way to access data managed in Coherence. It is implemented on top of the existing NamedMap API, but it provides a number of features that make it easier to use for many typical use cases where Coherence is used as a Key-Value data store. Features and Benefits In addition to the basic CRUD functionality, the Repository API provides many features that simplify common data management tasks: Powerful projection features Flexible in-place entity updates First-class data aggregation support Stream API support Asynchronous API support Event listener support Declarative acceleration and index creation CDI Support Implementing a Repository Coherence provides an abstract base class com.oracle.coherence.repository.AbstractRepository , which your custom repository implementation needs to extend and provide implementation of three abstract methods: <markup lang=\"java\" > /** * Return the identifier of the specified entity instance. * * @param entity the entity to get the identifier from * * @return the identifier of the specified entity instance */ protected abstract ID getId(T entity); /** * Return the type of entities in this repository. * * @return the type of entities in this repository */ protected abstract Class&lt;? extends T&gt; getEntityType(); /** * Return the map that is used as the underlying entity store. * * @return the map that is used as the underlying entity store */ protected abstract M getMap(); For example, a repository implementation that can be used to store Person entities, with String identifiers, can be as simple as: <markup lang=\"java\" >public class PeopleRepository extends AbstractRepository&lt;String, Person&gt; { private NamedMap&lt;String, Person&gt; people; public PeopleRepository(NamedMap&lt;String, Person&gt; people) { this.people = people; } protected NamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person person) { return person.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the NamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository That is it in a nutshell: a trivial repository implementation above will allow you to access all the Repository API features described in the remaining sections, which are provided by the AbstractRepository class you extended. However, you are free (and encouraged) to add additional business methods to the repository class above that will make it easier to use within your application. The most common example of such methods would be various \"finder\" methods that your application needs. For example, if your application needs to frequently query the repository to find people based on their name, you may want to add a method for that purpose: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return getAll(filter); } You can then invoke findByName method directly within the application to find all the people whose first or last name starts with a letter A , for example: <markup lang=\"java\" >for (Person p : people.findByName(\"A%\")) { // processing } Basic CRUD Operations We&#8217;ve already seen one read operation, getAll , in the example above, but let&#8217;s start from the beginning and look into how we can add, remove, update and query our repository. To add new entities to the repository, or replace the existing ones, you can use either the save or the saveAll method. The former takes a single entity as an argument and stores it in the backing NamedMap : <markup lang=\"java\" >people.save(new Person(\"555-11-2222\", \"Aleks\", 46)); The latter allows you to store a batch of entities at once by passing either a collection or a stream of entities as an argument. Once you have some entities stored in a repository, you can query the repository using get and getAll methods. <markup lang=\"java\" >Person person = people.get(\"555-11-2222\"); assert person.getName().equals(\"Aleks\"); assert person.getAge() == 46; Collection&lt;Person&gt; allPeople = people.getAll(); Collection&lt;Person&gt; allAdults = people.getAll(Filters.greaterOrEqual(Person::getAge, 18)); get a single Person by identifier get all the people from the repository get all the people from the repository that are 18 or older You can retrieve sorted results by calling getAllOrderedBy method and specifying a Comparable property via a method reference: <markup lang=\"java\" >Collection&lt;Person&gt; peopleOrderedByAge = people.getAllOrderedBy(Person::getAge) the result will contain all people from the repository, sorted by age from the youngest to the oldest For more complex use cases, you can specify a Comparator to use instead. For example, if we wanted to always sort the results of the findByName method defined above first by last name and then by first name, we could re-implement it as: <markup lang=\"java\" > public Collection&lt;Person&gt; findByName(String name) { Filter&lt;Person&gt; filter = Filters.like(Person::getFirstName, name) .or(Filters.like(Person::getLastName, name)); return getAllOrderedBy(filter, Remote.comparator(Person::getLastName) .thenComparing(Person::getFirstName)); } the results will be sorted by last name, and then by first name; note that we are using Coherence Remote.comparator instead of standard Java Comparator in order to ensure that the specified comparator is serializable and can be sent to remote cluster members Finally, to remove entities from a repository you can use one of the several remove methods: <markup lang=\"java\" >boolean fRemoved = people.remove(person); boolean fRemoved = people.removeById(\"111-22-3333\"); removes specified entity from the repository removes entity with the specified identifier from the repository In both examples above the result will be a boolean indicating whether the entity was actually removed from the backing NamedMap , and it may be false if the entity wasn&#8217;t present in the repository. If you are interested in the removed value itself, you can use the overloads of the methods above that allow you to express that: <markup lang=\"java\" >Person removed = people.remove(person, true); Person removed = people.removeById(\"111-22-3333\", true); removes specified entity from the repository and returns it as the result removes entity with the specified identifier from the repository and returns it as the result Note that this will result in additional network traffic, so unless you really need the removed entity it is probably best not to ask for it. The examples above are useful when you want to remove a single entity from the repository. In cases when you want to remove multiple entities as part of a single network call, you should use one of removeAll methods instead, which allow you to remove a set of entities by specifying either their identifiers explicitly, or the criteria for removal via the Filter . <markup lang=\"java\" >boolean fChanged = people.removeAll(Filters.equal(Person::getGender, Gender.MALE)); boolean fChanged = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\")); removes all men from the repository and returns true if any entity has been removed removes entities with the specified identifiers from the repository and returns true if any entity has been removed Just like with single-entity removal operations, you can also use overloads that allow you to return the removed entities as the result: <markup lang=\"java\" >Map&lt;String, Person&gt; mapRemoved = people.removeAll(Filters.equal(Person::getGender, Gender.MALE), true); Map&lt;String, Person&gt; mapRemoved = people.removeAllById(Set.of(\"111-22-3333\", \"222-33-4444\"), true); removes all men from the repository and returns the map of removed entities, keyed by identifier removes entities with the specified identifiers from the repository and returns the map of removed entities, keyed by identifier Projection While querying repository for a collection of entities that satisfy some criteria is certainly a common and useful operation, sometimes you don&#8217;t need all the attributes within the entity. For example, if you only need a person&#8217;s name, querying for and then discarding all the information contained within the Person instances is unnecessary and wasteful. It is the equivalent of executing <markup lang=\"sql\" >SELECT * FROM PEOPLE against a relational database, when a simple <markup lang=\"sql\" >SELECT name FROM PEOPLE would suffice. Coherence Repository API allows you to limit the amount of data collected by performing server-side projection of the entity attributes you are interested in. For example, if you only need a person&#8217;s name, you can get just the name: <markup lang=\"java\" >String name = people.get(\"111-22-3333\", Person::getName); Map&lt;String, String&gt; mapNames = people.getAll(Filters.less(Person::getAge, 18), Person::getName); return the name of the person with a specified identifier return the map of names of all the people younger than 18, keyed by person&#8217;s identifier Obviously, returning either the whole entity or a single attribute from an entity are two ends of the spectrum, and more often than not you need something in between. For example, you may need the person&#8217;s name and age. For situations like that, Coherence allows you to use fragments : <markup lang=\"java\" >Fragment&lt;Person&gt; fragment = people.get(\"111-22-3333\", Extractors.fragment(Person::getName, Person::getAge)); String name = fragment.get(Person::getName); int age = fragment.get(Person::getAge); return a fragment containing the name and age of the person with a specified identifier retrieve the person&#8217;s name from a fragment retrieve the person&#8217;s age from a fragment You can, of course, perform the same projection across multiple entities using one of getAll methods: <markup lang=\"java\" >Map&lt;String, Fragment&lt;Person&gt;&gt; fragments = people.getAll( Filters.less(Person::getAge, 18), Extractors.fragment(Person::getName, Person::getAge)); return a map of fragments containing the name and age of all the people younger than 18, keyed by person&#8217;s identifier Unlike the relational database, which contains a set of columns for each row in the table, Coherence stores each entity as a full object graph, which means that the attributes can be other object graphs and can be nested to any level. This means that we also may need to be able to project attributes of the nested objects. For example, our Person class may have a nested Address object as an attribute, which in turn has street , city , and country attributes. If we want to retrieve the name and the country of a person in a repository, we can do it like this: <markup lang=\"java\" >Fragment&lt;Person&gt; person = people.get( \"111-22-3333\", Extractors.fragment(Person::getName, Extractors.fragment(Person::getAddress, Address::getCountry))); String name = person.get(Person::getName); Fragment&lt;Address&gt; address = person.getFragment(Person::getAddress); String country = address.get(Address::getCountry); return a fragment containing the name and the Address fragment of the person with a specified identifier retrieve the person&#8217;s name from the Person fragment retrieve the Address fragment from the Person fragment retrieve the person&#8217;s country from the Address fragment In-place Updates By far the most common approach for updating data in modern applications is the read-modify-write pattern. For example, the typical code to update an attribute of a Person may look similar to the following: <markup lang=\"java\" >Person person = people.get(\"111-22-3333\"); person.setAge(55); people.save(person); This is true regardless of whether the underlying data store provides a better, more efficient way of updating data. For example, RDBMS provide stored procedures for that purpose, but very few developers use them because they are not as convenient to use, and do not fit well into popular application frameworks, such as JPA, Spring Data or Micronaut Data. They also fragment the code base to some extent, splitting the business logic across the application and the data store, and require that some application code is written in SQL. However, the approach above is suboptimal, for a number of reasons: It at least doubles the number of network calls the application makes to the data store, increasing the overall latency of the operation. It moves (potentially a lot) more data over the network than absolutely necessary. It may require expensive construction of a complex entity in order to perform a very simple update operation of a single attribute (this is particularly true with JPA and RDBMS back ends). It puts additional, unnecessary load on the data store, which is typically the hardest component of the application to scale. It introduces concurrency issues (ie. what should happen if the entity in the data store changes between the initial read and subsequent write), which typically requires that both the read and the write happen within the same transaction. A much better, more efficient way to perform the updates is to send the update function to the data store, and execute it locally, within the data store itself (which is pretty much what stored procedures are for). Coherence has always had support for these types of updates via entry processors , but the Repository API makes it even simpler to do so. For example, the code above can be rewritten as: <markup lang=\"java\" >people.update(\"111-22-3333\", Person::setAge, 55); We are basically telling Coherence to update Person instance with a given identifier by calling setAge method on it with a number 55 as an argument. This is not only significantly more efficient, but I&#8217;m sure you&#8217;ll agree, shorter and easier to write, and to read. Note that we don&#8217;t know, or care, where in the cluster a Person instance with a given identifier is&#8201;&#8212;&#8201;all we care about is that Coherence guarantees that it will invoke the setAge method on the entity with a specified ID, on a primary owner , and automatically create a backup of the modified entity for fault tolerance. It is also worth pointing out that the approach above provides the same benefits stored procedures do in RDBMS, but without the downsides: you are still writing all your code in Java, and keeping it in the same place. As a matter of fact, this approach allows you to implement rich domain models for your data, and execute business logic on your entities remotely, which works exceptionally well with DDD applications. Calling a setter on an entity remotely is only the tip of the iceberg, and far from sufficient for all data mutation needs. For example, conventional JavaBean setter returns void , but you often want to know what the entity value is after the update. The solution to that problem is simple: Coherence will return the result of the specified method invocation, so all you need to do is change the setAge method to implement fluent API: <markup lang=\"java\" >public Person setAge(int age) { this.age = age; return this; } You will now get the modified Person instance as the result of the update call: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", Person::setAge, 55); assert person.getAge() == 55; Sometimes you need to perform more complex updates, or update multiple attributes at the same time. While you could certainly accomplish both of those by making multiple update calls, that is inefficient because each update will result in a separate network call. You are better off using the update overload that allows you to specify the function to execute in that situation: <markup lang=\"java\" >Person person = people.update(\"111-22-3333\", p -&gt; { p.setAge(55); p.setGender(Gender.MALE); return p; }); assert person.getAge() == 55; assert person.getGender() == Gender.MALE; This way you have full control of the update logic that will be executed, and the return value. You may sometimes want to update an entity that does not exist in the repository yet, in which case you want to create a new instance. For example, you may want to create a shopping cart entity for a customer when they add the first item to the cart. While you could implement the code to check whether the Cart for a given customer exists, and create new one if it doesn&#8217;t, this again results in network calls that can be avoided if you simply create the Cart instance as part of Cart::addItem call. The Repository API allows you to accomplish that via optional EntityFactory argument: <markup lang=\"java\" >carts.update(customerId, Cart::addItem, item, Cart::new); the cart/customer identifier the method to invoke on a target Cart instance the CartItem to add to the cart the EntityFactory to use to create a new Cart instance if the cart with the specified identifier doesn&#8217;t exist The EntityFactory interface is quite simple: <markup lang=\"java\" >@FunctionalInterface public interface EntityFactory&lt;ID, T&gt; extends Serializable { /** * Create an entity instance with the specified identity. * * @param id identifier to create entity instance with * * @return a created entity instance */ T create(ID id); } Basically, it has a single create method that accepts entity identifier and returns a new instance of the entity with a given identifier. In the example above, that implies that our Cart class has a constructor similar to this: <markup lang=\"java\" >public Cart(Long cartId) { this.cartId = cartId; } Just like with projections and other operations, in addition to update methods that can be used to modify a single entity, there are also a number of updateAll methods that can be used to modify multiple entities in a single call. An example where this may be useful is when you want to apply the same exact function to multiple entities, as is the case when performing stock split: <markup lang=\"java\" >positions.updateAll( Filters.equal(Position::getSymbol, \"AAPL\"), Position::split, 5); the Filter used to determine the set of positions to update the function to apply to each position; in this case split(5) will be called on each Position entity with AAPL symbol Just like with single-entity updates, the result of each function invocation will be returned to the client, this time in the form of a Map containing the identifiers of the processed entities as keys, and the result of the function applied to that entity as the value. Stream API and Data Aggregation We&#8217;ve already covered how you can query the repository to retrieve a subset of entities using a getAll method and a Filter , but sometimes you don&#8217;t need the entities themselves, but a result of some computation applied to a subset of entities in the repository. For example, you may need to calculate average salary of all the employees in a department, or the total value of all equity positions in a portfolio. While you could certainly query the repository for the entities that need to be processed and perform processing itself on the client, this is very inefficient way to accomplish the task, as you may end up moving significant amount of data over the network, just to discard it after the client-side processing. As you&#8217;ve probably noticed by now, Coherence provides a number of feature that allow you to perform various types of distributed processing efficiently, and this situation is no exception. Just like the in-place updates leverage Coherence Entry Processor API to perform data mutation on cluster members that store the data, Repository API support for data aggregation leverages Coherence Remote Stream API and the Aggregation API to perform read-only distributed computations efficiently. This once again allows you to move processing to the data, instead of the other way around, and to perform computation in parallel across as many CPU cores as your cluster has, instead of a handful of (or in many cases only one) cores on the client. The first option is to use the Stream API, which you are probably already familiar with because it&#8217;s a standard Java API introduced in Java 8. For example, you could calculate the average salary of all employees like this: <markup lang=\"java\" >double avgSalary = employees.stream() .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); If you wanted to calculate average salary only for the employees in a specific department instead, you could filter the employees to process: <markup lang=\"java\" >double avgSalary = employees.stream() .filter(e -&gt; e.getDepartmentId == departmentId) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); However, while it works, the code above is not ideal, as it will end up processing, and potentially deserializing all the employees in the repository in order to determine whether they belong to a specified department. A better way to accomplish the same task is to use Coherence-specific stream method overload which allows you to specify the Filter to create a stream based on: <markup lang=\"java\" >double avgSalary = employees.stream(Filters.equal(Employee::getDepartmentId, departmentId)) .collect(RemoteCollectors.averagingDouble(Employee::getSalary)); The difference is subtle, but important: unlike previous example, this allows Coherence to perform query before creating the stream, and leverage any indexes you may have in the process. This can significantly reduce the overhead when dealing with large data sets. However, there is also an easier way to accomplish the same thing: <markup lang=\"java\" >double avgSalary = employees.average(Employee::getSalary); or, for a specific department: <markup lang=\"java\" >double avgSalary = employees.average( Filters.equal(Employee::getDepartmentId, departmentId), Employee::getSalary); These are the examples of using repository aggregation methods directly, which turn common tasks such as finding min , max , average and sum of any entity attribute as simple as it can be. There are also more advanced aggregations, such as groupBy and top : <markup lang=\"java\" >Map&lt;Gender, Set&lt;Person&gt;&gt; peopleByGender = people.groupBy(Person::getGender); Map&lt;Long, Double&gt; avgSalaryByDept = employees.groupBy(Employee::getDepartmentId, averagingDouble(Employee::getSalary)); List&lt;Double&gt; top5salaries = employees.top(Employee::getSalary, 5); as well as the simpler ones, such as count and distinct . Finally, in many cases you may care not only about min , max or top values of a certain attribute, but also about which entities those values belong to. For those situations, you can use minBy , maxBy and topBy methods, which returns the entities containing minimum, maximum and top values of an attribute, respectively: <markup lang=\"java\" >Optional&lt;Person&gt; oldestPerson = people.maxBy(Person::getAge); Optional&lt;Person&gt; youngestPerson = people.minBy(Person::getAge); List&lt;Employee&gt; highestPaidEmployees = employees.topBy(Employee::getSalary, 5); Declarative Acceleration and Index Creation I mentioned earlier that Coherence can use indexes to optimize queries and aggregations. The indexes allow you to avoid deserializing entities stored across the cluster, which is a potentially expensive operation when you have large data set, with complex entity classes. The indexes themselves can also be sorted, which is helpful when executing range-based queries, such as less , greater or between . The standard way to create indexes is by calling NamedMap.addIndex method, which is certainly still an option. However, Repository API introduces a simpler, declarative way of index creation. To define an index, simply annotate the accessor for the entity attribute(s) that you&#8217;d like to create an index for with @Indexed annotation: <markup lang=\"java\" >public class Person { @Indexed public String getName() { return name; } @Indexed(ordered = true) public int getAge() { return age; } } defines an unordered index on Person::getName , which is suitable for filters such as equal , like , and regex defines an ordered index on Person::getAge , which is better suited for filters such as less , greater and between When the repository is created, it will introspect the entity class for @Indexed annotation and automatically create an index for each attribute that has one. The created index will then be used whenever that attribute is referenced within the query expression. In some cases you may want to keep deserialized entity instances around instead of discarding them. This can be useful when you are making frequent queries, aggregations, and using Stream API, or even in-place updates or projection, as the cost of maintaining individual indexes on all the attributes you need may end up being greater than to simply keep deserialized entity instances around. For situations like that Coherence provides a special index type you can use, DeserializationAccelerator , but if you are using Repository API you once again have an easier way of configuring it&#8201;&#8212;&#8201;simply annotate either the entity class, or the repository class itself with the @Accelerated annotation: <markup lang=\"java\" >@Accelerated public class Person { } Obviously, you will require additional storage capacity in your cluster in order to be able to store both the serialized and deserialized copy of all the entities, but in some situations the performance benefits can significantly outweigh the cost. In other words, acceleration is a classic example of a time vs. space tradeoff, and it is entirely up to you to decide when it makes sense to use it. Event Listeners Coherence not only allows you to store, modify, query and aggregate your data entities efficiently, but you can also register to receive event notifications whenever any entity in the repository changes. To do that, you can create and register a listener that will be notified whenever an entity is inserted, updated or removed: <markup lang=\"java\" > public static class PeopleListener implements PeopleRepository.Listener&lt;Person&gt; { public void onInserted(Person personNew) { // handle INSERT event } public void onUpdated(Person personOld, Person personNew) { // handle UPDATE event } public void onRemoved(Person personOld) { // handle REMOVE event } } <markup lang=\"java\" >people.addListener(new PeopleListener()); people.addListener(\"111-22-3333\", new PeopleListener()); people.addListener(Filters.greater(Person::getAge, 17), new PeopleListener()); registers a listener that will be notified whenever any entity in the repository is inserted, updated or removed registers a listener that will be notified when an entity with the specified identifier is inserted, updated or removed registers a listener that will be notified when any Person older than 17 is inserted, updated or removed As you can see from the example above, there are several ways to register only for the events you are interested in, in order to reduce the number of events received, and the amount of data sent over the network. Note that all of the listener methods above have a default no-op implementation, so you only need to implement the ones you actually want to handle. However, having to implement a separate class each time you want to register a listener is a bit cumbersome, so Repository API also provides a default listener implementation, and a fluent builder for it that make the task a bit easier: <markup lang=\"java\" >people.addListener( people.listener() .onInsert(personNew -&gt; { /* handle INSERT event */ }) .onUpdate((personOld, personNew) -&gt; { /* handle UPDATE event with old value */ }) .onUpdate(personNew -&gt; { /* handle UPDATE event without old value */ }) .onRemove(personOld -&gt; { /* handle REMOVE event */ }) .build() ); Note that when using Listener Builder API you have the option of omitting the old entity value from the onUpdate event handler arguments list. You can also specify multiple handlers for the same event type, in which case they will be composed and invoked in the specified order. There is also an option of providing a single event handler that will receive all the events, regardless of the event type: <markup lang=\"java\" >people.addListener( people.listener() .onEvent(person -&gt; { /* handle all events */ }) .build() ); Just like when implementing listener class explicitly, you can still pass entity identifier or a Filter as the first argument to addListener method in order to limit the scope of the events received. Asynchronous Repository API In addition to the synchronous repository, AbstractRepository&lt;ID, T&gt; , we offer an asynchronous version, AbstractAsyncRepository&lt;ID, T&gt; . The same abstract methods as previously described will need to be implemented. The main differences between the two APIs is that the asynchronous API returns java.util.CompletableFuture of the return type. For example, Collection&lt;T&gt; getAll() in the blocking version would be CompletableFuture&lt;Collection&lt;T&gt;&gt; in the asynchronous version of the Repository API. The asynchronous API also offers callbacks that will be passed the results of the operation as they become available, instead of buffering the result into a collection prior to returning. This allows you to stream and process very large result sets without paying the cost of accumulating all the results in memory at once, which is not possible with the blocking API. AbstractAsyncRepository Examples <markup lang=\"java\" >public class AsyncPeopleRepository extends AbstractAsyncRepository&lt;String, Person&gt; { private AsyncNamedMap&lt;String, Person&gt; people; public AsyncPeopleRepository(AsyncNamedMap&lt;String, Person&gt; people) { this.people = people; } protected AsyncNamedMap&lt;String, Person&gt; getMap() { return people; } protected String getId(Person entity) { return entity.getSsn(); } protected Class&lt;? extends Person&gt; getEntityType() { return Person.class; } } The getMap method returns the AsyncNamedMap that should be used as a backing data store for the repository, which is in this case provided via constructor argument, but could just as easily be injected via CDI The getId method returns an identifier for a given entity The getEntityType method returns the class of the entities stored in the repository An example using the AsyncPersonRepository make a simple query for an entity: <markup lang=\"java\" >String upercaseName = asyncPeople.get(\"111-22-3333\") .thenApply(Person::getName) .thenApply(String::toUpperCase) .get() Get a CompletableFuture&lt;Person&gt; based on their ID When the future is completed, obtain the person&#8217;s name from the Person instance Convert the name to uppercase Block and return the upper-cased name This usage pattern will be similar across all the methods that return CompletableFuture , which is pretty much all of them. Asynchronous Callbacks Instead of dealing with an entire collection being realized for the results, it is possible to define a callback that will be invoked as results become available. These APIs will return CompletableFuture&lt;Void&gt; to signal all the results have been processed. For example, if we simply want to print out the people as they are streamed back from the server, without accumulating result set on the client, we can simply do the following: <markup lang=\"java\" >asyncPeople.getAll(person -&gt; System.out.println(person.getName())); .thenApply(done -&gt; System.out.println(\"DONE!\")) Print the name of each Person within the repository Print DONE! when all people have been processed Of course, you can also extract the name attribute only by providing a ValueExtractor for it as the first argument, in which case the code above could be rewritten to move less data over the network like this: <markup lang=\"java\" >asyncPeople.getAll(Person::getName, (id, name) -&gt; System.out.println(name)); .thenApply(done -&gt; System.out.println(\"DONE!\")) Print the name of each Person within the repository Print DONE! when all people have been processed In the example above, the callback is implemented as a BiConsumer that will receive entity identifer and the extracted value as arguments. Of course, we could&#8217;ve used fragment extractor as the first argument to getAll method above, in which case the second argument to the callback would&#8217;ve been Fragment&lt;Person&gt; instead of just the name attribute. Summary The Coherence Repository API was introduced to make the implementation of data access layer within the applications easier, regardless of which framework you use to implement applications that use Coherence as a data store. It works equally well for plain Java applications and applications that use CDI, where you can simply create your own repository implementations, as described at the beginning of this document. It is also the foundation for our Micronaut Data and Spring Data repository implementations, so all the functionality described here is available when using those frameworks as well. The only difference is how you define your own repositories, which is framework-specific and documented separately. We hope you&#8217;ll find this new feature useful, and that it will make implementation of your Coherence-backed data access layers even easier. ",
            "title": "Repository API"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " Coherence MP Metrics provides support for [Eclipse MicroProfile Metrics] ( https://microprofile.io/project/eclipse/microprofile-metrics ) within Coherence cluster members. This is a very simple module that allows you to publish Coherence metrics into MicroProfile Metric Registries available at runtime, and adds Coherence-specific tags to all the metrics published within the process, in order to distinguish them on the monitoring server, such as Prometheus. ",
            "title": "Coherence MicroProfile Metrics"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " In order to use Coherence MP Metrics, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-metrics&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; That&#8217;s it&#8201;&#8212;&#8201;once the module above is in the class path, Coherence will discover MpMetricRegistryAdapter service it provides, and use it to publish all standard Coherence metrics to the vendor registry, and any user-defined application metrics to the application registry. All the metrics will be published as gauges, because they represent point-in-time values of various MBean attributes. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " There could be hundreds of members in a Coherence cluster, with each member publishing potentially the same set of metrics. There could also be many Coherence clusters in the environment, possibly publishing to the same monitoring server instance. In order to distinguish metrics coming from different clusters, as well as from different members of the same cluster, Coherence MP Metrics will automatically add several tags to ALL the metrics published within the process. The tags added are: Tag Name Tag Value cluster the cluster name site the site the member belongs to (if set) machine the machine member is on (if set) member the name of the member (if set) node_id the node ID of the member role the member&#8217;s role This ensures that the metrics published by one member do not collide with and overwrite the metrics published by another members, and allows you to query and aggregate metrics based on the values of the tags above if desired. ",
            "title": "Coherence Global Tags"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " About 15 Minutes IntelliJ IDEA JDK 6 or later ",
            "title": "What You Need"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. ",
            "title": "Installing IntelliJ IDEA"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git ",
            "title": "Clone the Coherence CE Repository"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. ",
            "title": "Importing a Guide or Tutorial"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " IntelliJ IDEA ",
            "title": "See Also"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " You will pick a Coherence guide or example and import it into IntelliJ IDEA. Then you can then read and follow the individual guide or tutorial documentation. What You Need About 15 Minutes IntelliJ IDEA JDK 6 or later Installing IntelliJ IDEA If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. Clone the Coherence CE Repository To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git Importing a Guide or Tutorial With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. See Also IntelliJ IDEA ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " This guide walks you through importing one of the Coherence guides or tutorials into IntelliJ IDEA. What You Will Build You will pick a Coherence guide or example and import it into IntelliJ IDEA. Then you can then read and follow the individual guide or tutorial documentation. What You Need About 15 Minutes IntelliJ IDEA JDK 6 or later Installing IntelliJ IDEA If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. Clone the Coherence CE Repository To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git Importing a Guide or Tutorial With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. See Also IntelliJ IDEA ",
            "title": "Import a Project Into IntelliJ IDEA"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " Coherence gRPC proxy is the server-side implementation of the services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. ",
            "title": "Coherence gRPC Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The gRPC server will start automatically when com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ) is started. Typically, Coherence will be used as the application&#8217;s main class, alternatively an instance of Coherence can be started using the bootstrap API. When reviewing the log output, you should see the following two log messages: <markup lang=\"log\" >Coherence gRPC proxy is now listening for connections on 0.0.0.0:1408 Coherence gRPC in-process proxy 'default' is now listening for connections The service is now ready to process requests from one of the Coherence gRPC client implementations. ",
            "title": "Start the Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " In order to use Coherence gRPC Server, you need to declare it as a dependency of your project; for example if using Maven: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; or for Gradle: <markup lang=\"groovy\" >implementation 'com.oracle.coherence.ce:coherence-grpc-proxy:21.12.6-SNAPSHOT' Start the Server The gRPC server will start automatically when com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ) is started. Typically, Coherence will be used as the application&#8217;s main class, alternatively an instance of Coherence can be started using the bootstrap API. When reviewing the log output, you should see the following two log messages: <markup lang=\"log\" >Coherence gRPC proxy is now listening for connections on 0.0.0.0:1408 Coherence gRPC in-process proxy 'default' is now listening for connections The service is now ready to process requests from one of the Coherence gRPC client implementations. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The port the gRPC server listens on can be changed using the coherence.grpc.server.port system property, for example -Dcoherence.grpc.server.port=7001 will cause the server to bind to port 7001 . ",
            "title": "Set the Port"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The name used by the in-process server can be changed using the coherence.grpc.inprocess.name system property, for example -Dcoherence.grpc.inprocess.name=foo will set the in-process server name to foo . ",
            "title": "Set the In-Process Server Name"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " By default, the gRPC server runs in plaintext mode. The gRPC server can be configured to use TLS. The relevant key, cert and optional CA files must be provided, either on the classpath or file system. Assuming that the TLS key is in a file named /certs/server.key , and that the server TLS cert is in a file named /certs/server.pem then TLS can be configured with the following system properties. <markup >coherence.grpc.server.credentials=tls coherence.grpc.server.tls.key=/certs/server.key coherence.grpc.server.tls.cert=/certs/server.pem An optional server key file password can be provided if required, for example if the key file password is secret : <markup >coherence.grpc.server.tls.password=secret By default, the server does not require client certificates. The server can be configured to use mutual authentication to verify the client certificates. <markup >coherence.grpc.server.tls.client=REQUIRED The valid values for the coherence.grpc.server.tls.client system property are NONE , REQUIRED , or OPTIONAL . If a CA file is required to verify the client certs it can be provided to the server, for example if the CA file is called /certs/ca.pem : <markup >coherence.grpc.server.tls.ca=/certs/ca.pem ",
            "title": "Using TLS"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " It is possible to have full control over the configuration of the server by implementing the interface com.oracle.coherence.grpc.proxy.GrpcServerConfiguration . Implementations of this interface will be loaded using the Java ServiceLoader before the server starts allowing the ServerBuilder used to build both the server and in-process server to be modified. For example, the class below implements GrpcServerConfiguration and configures both servers to use transport security certificates. <markup lang=\"java\" title=\"MyServerConfig.java\" >package com.acme.application; import com.oracle.coherence.grpc.proxy.GrpcServerConfiguration; import io.grpc.ServerBuilder; import io.grpc.inprocess.InProcessServerBuilder; public class MyServerConfig implements GrpcServerConfiguration { public void configure(ServerBuilder&lt;?&gt; serverBuilder, InProcessServerBuilder inProcessServerBuilder) { File fileCert = new File(\"/grpc.crt\"); File fileKey = new File(\"grpc.key\"); serverBuilder.useTransportSecurity(fileCert, fileKey); inProcessServerBuilder.useTransportSecurity(fileCert, fileKey); } } For the Coherence gRPC proxy to find the above configuration class via the ServiceLoader a file named com.oracle.coherence.grpc.proxy.GrpcServerConfiguration needs to be added to application classes META-INF/services directory. <markup title=\"com.oracle.coherence.grpc.proxy.GrpcServerConfiguration\" >com.acme.application.MyServerConfig When the gRPC proxy starts it will now discover the MyServerConfig and will call it to modify the server builders. As well as security as in the example, other configuration such as interceptors and even additional gRPC services can be added to the server before it starts. ",
            "title": "Advanced gRPC Proxy Server Configuration"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " The default gRPC server will listen for remote connections on port 1408 as well as in-process connections on an in-process server named default . Set the Port The port the gRPC server listens on can be changed using the coherence.grpc.server.port system property, for example -Dcoherence.grpc.server.port=7001 will cause the server to bind to port 7001 . Set the In-Process Server Name The name used by the in-process server can be changed using the coherence.grpc.inprocess.name system property, for example -Dcoherence.grpc.inprocess.name=foo will set the in-process server name to foo . Using TLS By default, the gRPC server runs in plaintext mode. The gRPC server can be configured to use TLS. The relevant key, cert and optional CA files must be provided, either on the classpath or file system. Assuming that the TLS key is in a file named /certs/server.key , and that the server TLS cert is in a file named /certs/server.pem then TLS can be configured with the following system properties. <markup >coherence.grpc.server.credentials=tls coherence.grpc.server.tls.key=/certs/server.key coherence.grpc.server.tls.cert=/certs/server.pem An optional server key file password can be provided if required, for example if the key file password is secret : <markup >coherence.grpc.server.tls.password=secret By default, the server does not require client certificates. The server can be configured to use mutual authentication to verify the client certificates. <markup >coherence.grpc.server.tls.client=REQUIRED The valid values for the coherence.grpc.server.tls.client system property are NONE , REQUIRED , or OPTIONAL . If a CA file is required to verify the client certs it can be provided to the server, for example if the CA file is called /certs/ca.pem : <markup >coherence.grpc.server.tls.ca=/certs/ca.pem Advanced gRPC Proxy Server Configuration It is possible to have full control over the configuration of the server by implementing the interface com.oracle.coherence.grpc.proxy.GrpcServerConfiguration . Implementations of this interface will be loaded using the Java ServiceLoader before the server starts allowing the ServerBuilder used to build both the server and in-process server to be modified. For example, the class below implements GrpcServerConfiguration and configures both servers to use transport security certificates. <markup lang=\"java\" title=\"MyServerConfig.java\" >package com.acme.application; import com.oracle.coherence.grpc.proxy.GrpcServerConfiguration; import io.grpc.ServerBuilder; import io.grpc.inprocess.InProcessServerBuilder; public class MyServerConfig implements GrpcServerConfiguration { public void configure(ServerBuilder&lt;?&gt; serverBuilder, InProcessServerBuilder inProcessServerBuilder) { File fileCert = new File(\"/grpc.crt\"); File fileKey = new File(\"grpc.key\"); serverBuilder.useTransportSecurity(fileCert, fileKey); inProcessServerBuilder.useTransportSecurity(fileCert, fileKey); } } For the Coherence gRPC proxy to find the above configuration class via the ServiceLoader a file named com.oracle.coherence.grpc.proxy.GrpcServerConfiguration needs to be added to application classes META-INF/services directory. <markup title=\"com.oracle.coherence.grpc.proxy.GrpcServerConfiguration\" >com.acme.application.MyServerConfig When the gRPC proxy starts it will now discover the MyServerConfig and will call it to modify the server builders. As well as security as in the example, other configuration such as interceptors and even additional gRPC services can be added to the server before it starts. ",
            "title": "Configuration"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " As already stated above, the Coherence gRPC server will be started automatically based on DefaultCacheServer lifecycle events. This behaviour can be disabled by setting the coherence.grpc.enabled system property to false , in which case a gRPC server will not be started. ",
            "title": "Disabling the gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If the coherence.grpc.enabled system property has been set to false , the gRPC server can be started manually by calling the start() method on the GrpcController singleton instance, for example: <markup lang=\"java\" >import com.oracle.coherence.grpc.proxy.GrpcServerController; public class MyApplication { public static void main(String[] args) { // do application initialisation... GrpcServerController.INSTANCE.start(); // do more application initialisation... } } The gRPC server can be stopped by calling the corresponding GrpcServerController.INSTANCE.stop() method. ",
            "title": "Programmatically starting the gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If you have application code that needs to run only after the gRPC server has started this can be achieved by using the GrpcServerController.whenStarted() method. This method returns a CompletionStage that will be completed when the gRPC server has started. <markup lang=\"java\" >GrpcServerController.INSTANCE.whenStarted().thenRun(() -&gt; { // run post-start code... System.out.println(\"The gRPC server has started\"); }); ",
            "title": "Waiting For gRPC Server Start"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If using the Helidon Microprofile server with the microprofile gRPC server enabled the Coherence gRPC proxy can be deployed into the Helidon gRPC server instead of the Coherence default gRPC server. For this behaviour to happen automatically just set the coherence.grpc.enabled system property to false , which will disable the built in server. A built-in GrpcMpExtension implementation will then deploy the proxy services to the Helidon gRPC server. When using the Helidon MP gRPC server, if the coherence.grpc.enabled system property has not been set to false , then both the Helidon gRPC server and the Coherence default gRPC server will start and could cause port binding issues unless they are both specifically configured to use different ports. ",
            "title": "Deploy the Proxy Service with Helidon Microprofile gRPC Server"
        },
        {
            "location": "/coherence-grpc-proxy/README",
            "text": " If you are running your own instance of a gRPC server and want to just deploy the Coherence gRPC proxy service to this server then that is possible. If manually deploying the service, ensure that auto-start of the Coherence gRPC server has been disabled by setting the system property coherence.grpc.enabled=false <markup lang=\"java\" >// Create your gRPC ServerBuilder ServerBuilder builder = ServerBuilder.forPort(port); // Obtain the Coherence gRPC services and add them to the builder List&lt;BindableService&gt; services = GrpcServerController.INSTANCE.createGrpcServices() services.forEach(serverBuilder::addService); // Build and start the server Server server = serverBuilder.build(); server.start(); ",
            "title": "Manually Deploy the gRPC Proxy Service"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " The coherence-micrometer module provides integration between Coherence metrics and Micrometer allowing Coherence metrics to be published via any of the Micrometer registries. ",
            "title": "Coherence Micrometer Metrics"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " In order to use Coherence Micrometer metrics, you need to declare the module as a dependency in your pom.xml and bind your Micrometer registry with the Coherence metrics adapter: <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-micrometer&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; The coherence-micrometer provides a Micrometer MeterBinder implementation class called CoherenceMicrometerMetrics . This class is a singleton and cannot be constructed, to access it use the CoherenceMicrometerMetrics.INSTANCE field. Micrometer provides many registry implementations to support different metrics applications and formats. For example, to bind Coherence metrics to the Micrometer PrometheusMeterRegistry , create the PrometheusMeterRegistry as documented in the Micrometer documentation , and call the CoherenceMicrometerMetrics class&#8217;s bindTo method: <markup lang=\"java\" >PrometheusMeterRegistry prometheusRegistry = new PrometheusMeterRegistry(PrometheusConfig.DEFAULT); // complete registy configuration... CoherenceMicrometerMetrics.INSTANCE.bindTo(prometheusRegistry); Micrometer registries can be bound to Coherence at any time, before or after Coherence starts. As Coherence creates or removed metrics they will be registered with or removed from the Micrometer registries. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " Micrometer has a global registry available which Coherence will bind to automatically if the coherence.micrometer.bind.to.global system property has been set to true (this property is false by default). ",
            "title": "Automatic Global Registry Binding"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " The com.tangol.net.Coherence contains a main method that allows it to be used to run a Coherence server as a more powerful to alternative DefaultCacheServer . <markup lang=\"bash\" >$ java -cp coherence.jar com.tangosol.net.Coherence Without any other configuration, the default Coherence instance started this way will run an identical server to that run using DefaultCacheServer . The steps above are covered in more detail below. ",
            "title": "Running A Coherence Server"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " When running Coherence if no configuration is specified the default behaviour is to use the default configuration file to configure Coherence. This behaviour still applies to the bootstrap API. If a Coherence instance is started without specifying any session configurations then a single default Session will be created. This default Session will wrap a ConfigurableCacheFactory that has been created from the default configuration file. The default file name is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig system property. When creating a CoherenceConfiguration the default session can be added using the SessionConfiguration.defaultSession() helper method. This method returns a SessionConfiguration configured to create the default Session . For example, in the code below the default session configuration is specifically added to the CoherenceConfiguration . <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); ",
            "title": "The Default Session"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " All sessions have a name that must be unique within the application. If a name has not been specified when the SessionConfiguration is built the default name of $Default$ will be used. A Coherence instance will fail to start if duplicate Session names exist. For example, this configuration will have the default name. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .build(); This configuration will have the name Test . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .name(\"Test\") .build(); ",
            "title": "Session Name"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " The most common type of session is a wrapper around a ConfigurableCacheFactory . When using the SessionConfiguration builder the configuration file URI is specified using the withConfigUri() method, that takes a string value specifiying the configuration file location. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"cache-config.xml\") .build(); The example above uses a configuration file a named cache-config.xml . If a configuration URI is not specified then the default value will be used. This value is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig System property. ",
            "title": "Session Configuration URI"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Coherence provides many types of events, examples of a few would be life-cycle events for Coherence itself, cache life-cycle events, cache entry events, partition events etc. These events can be listened to by implementing an EventInterceptor that receives specific types of event. Event interceptors can be registered with a Session as part of its configuration. For example, suppose there is an interceptor class in the application called CacheInterceptor that listens to CacheLifecycleEvent when caches get created or destroyed. This interceptor can be added to the session as shown below: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withInterceptor(new CacheInterceptor()) .build(); The interceptor will receive cache life-cycle events for all caches created using the session. ",
            "title": "Session Event Interceptors"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Scope is a concept that has been in Coherence for quite a while that allows services to be scoped and hence isolated from other services with the same name. For example multiple ConfigurableCacheFactory instances could be loaded from the same XML configuration file but given different scope names so that each CCF will have its own services in the cluster. Unless you require multiple Sessions, a scope will not generally be used in a configuration. A scope for a session can be configured using the configuration&#8217;s withScopeName() method, for example: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withScopeName(\"Test\") .build(); The session (and any ConfigurableCacheFactory it wraps) created from the configuration above will have a scope name of Test . It is possible to set a scope name in the &lt;defaults&gt; section of the XML configuration file. <markup lang=\"xml\" title=\"scoped-configuration.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;defaults&gt; &lt;scope-name&gt;Test&lt;/scope-name&gt; &lt;/defaults&gt; A ConfigurableCacheFactory created from the XML above, and hence any Session that wraps it will have a scope of Test . Note When using the bootstrap API any scope name specifically configured in the SessionConfiguration (that is not the default scope name) will override the scope name in the XML file. For example, using the scoped-configuration.xml file above: In this case the scope name will be Foo because the scope name has been explicitly set in the SessionConfiguration . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .withScopeName(\"Foo\") .build(); In this case the scope name will be Foo because although no scope name has been explicitly set in the SessionConfiguration , the name has been set to Foo , so the scope name will default to Foo . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as no scope name or session name has been explicitly set in the SessionConfiguration so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as the session name has been set to Foo but the scope name has been explicitly set to the default scope name using the constant Coherence.DEFAULT_SCOPE so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(Coherence.DEFAULT_SCOPE) .withConfigUri(\"scoped-configuration.xml\") .build(); ",
            "title": "Session Scope"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A SessionConfiguration is created by using the SessionConfiguration builder as shown in the example above. The Default Session When running Coherence if no configuration is specified the default behaviour is to use the default configuration file to configure Coherence. This behaviour still applies to the bootstrap API. If a Coherence instance is started without specifying any session configurations then a single default Session will be created. This default Session will wrap a ConfigurableCacheFactory that has been created from the default configuration file. The default file name is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig system property. When creating a CoherenceConfiguration the default session can be added using the SessionConfiguration.defaultSession() helper method. This method returns a SessionConfiguration configured to create the default Session . For example, in the code below the default session configuration is specifically added to the CoherenceConfiguration . <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); Session Name All sessions have a name that must be unique within the application. If a name has not been specified when the SessionConfiguration is built the default name of $Default$ will be used. A Coherence instance will fail to start if duplicate Session names exist. For example, this configuration will have the default name. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .build(); This configuration will have the name Test . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .name(\"Test\") .build(); Session Configuration URI The most common type of session is a wrapper around a ConfigurableCacheFactory . When using the SessionConfiguration builder the configuration file URI is specified using the withConfigUri() method, that takes a string value specifiying the configuration file location. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"cache-config.xml\") .build(); The example above uses a configuration file a named cache-config.xml . If a configuration URI is not specified then the default value will be used. This value is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig System property. Session Event Interceptors Coherence provides many types of events, examples of a few would be life-cycle events for Coherence itself, cache life-cycle events, cache entry events, partition events etc. These events can be listened to by implementing an EventInterceptor that receives specific types of event. Event interceptors can be registered with a Session as part of its configuration. For example, suppose there is an interceptor class in the application called CacheInterceptor that listens to CacheLifecycleEvent when caches get created or destroyed. This interceptor can be added to the session as shown below: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withInterceptor(new CacheInterceptor()) .build(); The interceptor will receive cache life-cycle events for all caches created using the session. Session Scope Scope is a concept that has been in Coherence for quite a while that allows services to be scoped and hence isolated from other services with the same name. For example multiple ConfigurableCacheFactory instances could be loaded from the same XML configuration file but given different scope names so that each CCF will have its own services in the cluster. Unless you require multiple Sessions, a scope will not generally be used in a configuration. A scope for a session can be configured using the configuration&#8217;s withScopeName() method, for example: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withScopeName(\"Test\") .build(); The session (and any ConfigurableCacheFactory it wraps) created from the configuration above will have a scope name of Test . It is possible to set a scope name in the &lt;defaults&gt; section of the XML configuration file. <markup lang=\"xml\" title=\"scoped-configuration.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;defaults&gt; &lt;scope-name&gt;Test&lt;/scope-name&gt; &lt;/defaults&gt; A ConfigurableCacheFactory created from the XML above, and hence any Session that wraps it will have a scope of Test . Note When using the bootstrap API any scope name specifically configured in the SessionConfiguration (that is not the default scope name) will override the scope name in the XML file. For example, using the scoped-configuration.xml file above: In this case the scope name will be Foo because the scope name has been explicitly set in the SessionConfiguration . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .withScopeName(\"Foo\") .build(); In this case the scope name will be Foo because although no scope name has been explicitly set in the SessionConfiguration , the name has been set to Foo , so the scope name will default to Foo . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as no scope name or session name has been explicitly set in the SessionConfiguration so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as the session name has been set to Foo but the scope name has been explicitly set to the default scope name using the constant Coherence.DEFAULT_SCOPE so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(Coherence.DEFAULT_SCOPE) .withConfigUri(\"scoped-configuration.xml\") .build(); ",
            "title": "Session Configurations"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A Coherence instance manages one or more Session instances, which are added to the CoherenceConfiguration by adding the SessionConfiguration instances to the builder. If no sessions have been added to the builder the Coherence instance will run a single Session that uses the default configuration file. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); The configuration above will configure a Coherence instance with the default name and with a single Sessions that wil use the default configuration file. The default session can also be explicitly added to the CoherenceConfiguration : <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); As already shown, other session configurations may also be added to the CoherenceConfiguration : <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(session) .build(); Whilst there is no limit to the number of sessions that can be configured the majority of applications would only ever require a single session - more than likely just the default session. ",
            "title": "Adding Sessions"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A CoherenceConfiguration can be configured to automatically discover SessionConfiguration instances. These are discovered using the Java ServiceLoader . Any instances of SessionConfiguration or SessionConfiguration.Provider configured as services in META-INF/services/ files will be loaded. This is useful if you are building modular applications where you want to include functionality in a separate application module that uses its own Session . The SessionConfiguration for the module is made discoverable by the ServiceLoader then whenever the module&#8217;s jar file is on the classpath the Session will be created, and the module&#8217;s functionality will be available to the application. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .discoverSessions() .build(); The call to discoverSessions() will load discovered SessionConfiguration instances. ",
            "title": "Session Configuration Auto-Discovery"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Each Coherence instance must be uniquely named. A name can be specified using the named() method on the builder, if no name has been specified the default name of $Default$ will be used. In the majority of use-cases an application would only ever require a single Coherence instance so there would be no requirement to specify a name. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); The configuration above will create a Coherence instance with the name Carts . ",
            "title": "Coherence Instance Name"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " As already mentioned, event interceptors can be added to a SessionConfiguration to receive events for a session. Event interceptors can also be added to the Coherence instance to receive events for all Session instances managed by that Coherence instance. For example, reusing the previous CacheInterceptor class, but this time for caches in all sessions: <markup lang=\"java\" >SessionConfiguration cartsSession = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(cartsSession) .withInterceptor(new CacheInterceptor()) .build(); Now the CacheInterceptor will receive events for both the default session and the Certs session. ",
            "title": "Add Global Event Interceptors"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A Coherence application is started by creating a Coherence instance from a CoherenceConfiguration . An instance of CoherenceConfiguration is created using the builder. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Adding Sessions A Coherence instance manages one or more Session instances, which are added to the CoherenceConfiguration by adding the SessionConfiguration instances to the builder. If no sessions have been added to the builder the Coherence instance will run a single Session that uses the default configuration file. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); The configuration above will configure a Coherence instance with the default name and with a single Sessions that wil use the default configuration file. The default session can also be explicitly added to the CoherenceConfiguration : <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); As already shown, other session configurations may also be added to the CoherenceConfiguration : <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(session) .build(); Whilst there is no limit to the number of sessions that can be configured the majority of applications would only ever require a single session - more than likely just the default session. Session Configuration Auto-Discovery A CoherenceConfiguration can be configured to automatically discover SessionConfiguration instances. These are discovered using the Java ServiceLoader . Any instances of SessionConfiguration or SessionConfiguration.Provider configured as services in META-INF/services/ files will be loaded. This is useful if you are building modular applications where you want to include functionality in a separate application module that uses its own Session . The SessionConfiguration for the module is made discoverable by the ServiceLoader then whenever the module&#8217;s jar file is on the classpath the Session will be created, and the module&#8217;s functionality will be available to the application. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .discoverSessions() .build(); The call to discoverSessions() will load discovered SessionConfiguration instances. Coherence Instance Name Each Coherence instance must be uniquely named. A name can be specified using the named() method on the builder, if no name has been specified the default name of $Default$ will be used. In the majority of use-cases an application would only ever require a single Coherence instance so there would be no requirement to specify a name. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); The configuration above will create a Coherence instance with the name Carts . Add Global Event Interceptors As already mentioned, event interceptors can be added to a SessionConfiguration to receive events for a session. Event interceptors can also be added to the Coherence instance to receive events for all Session instances managed by that Coherence instance. For example, reusing the previous CacheInterceptor class, but this time for caches in all sessions: <markup lang=\"java\" >SessionConfiguration cartsSession = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(cartsSession) .withInterceptor(new CacheInterceptor()) .build(); Now the CacheInterceptor will receive events for both the default session and the Certs session. ",
            "title": "Coherence Configuration"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " It is possible to create a Coherence instance without specifying any configuration. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(); <markup lang=\"java\" >Coherence coherence = Coherence.client(); In both of the above examples the Coherence instance will have the default Session and any discovered sessions . ",
            "title": "Create a Default Coherence Instance"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A CoherenceConfiguration can be used to create a Coherence instance. A Coherence instance is created in one of two modes, either cluster member or client. The mode chosen affects how some types of Session are created and whether auto-start services are started. As the name suggests a \"cluster member\" is a Coherence instance that expects to start or join a Coherence cluster. In a cluster member any Session that wraps a ConfigurableCacheFactory will be have its services auto-started and monitored (this is the same behaviour that would have happened when using DefaultCacheServer to start a server). A \"client\" Coherence instance is typically not a cluster member, i.e. it is a Coherence*Extend or gRPC client. As such, Session instances that wrap a ConfigurableCacheFactory will not be auto-started, they will start on demand as resources such as maps, caches or topics are requested from them. The com.tangosol.net.Coherence class has static factory methods to create Coherence instances in different modes. For example, to create a Coherence instance that is a cluster member the Coherence.clusterMember method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.clusterMember(cfg); For example, to create a Coherence instance that is a client the Coherence.client method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.client(cfg); Create a Default Coherence Instance It is possible to create a Coherence instance without specifying any configuration. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(); <markup lang=\"java\" >Coherence coherence = Coherence.client(); In both of the above examples the Coherence instance will have the default Session and any discovered sessions . ",
            "title": "Create a Coherence Instance"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " A Coherence instance it must be started to start all the sessions that the Coherence instance is managing. This is done by calling the start() method. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(cfg); coherence.start(); ",
            "title": "Start Coherence"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " To avoid having to pass around the instance of Coherence that was used to bootstrap an application the Coherence class has some static methods that make it simple to retrieve an instance. If only a single instance of Coherence is being used in an application (which will cover most use-cases) then the getInstance() method can be used: <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); It is also possible to retrieve an instance by name: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); Coherence.create(cfg); &#8230;&#8203;then later&#8230;&#8203; <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); ",
            "title": "Obtaining a Coherence Instance"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " If application code needs to ensure that a Coherence instance has started before doing some work then the whenStarted() method can be used to obtain a CompletableFuture that will be completed when the Coherence instance has started. <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); CompletableFuture&lt;Void&gt; future = coherence.whenStarted(); future.join(); There is also a corresponding whenStopped() method that returns a future that will be completed when the Coherence instance stops. ",
            "title": "Ensuring Coherence Has Started"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Besides using the future methods described above it is possible to add and EventInterceptor to the configuration of a Coherence instance that will receive life-cycle events. Below is an example interceptor that implements Coherence.LifecycleListener . <markup lang=\"java\" >public class MyInterceptor implements Coherence.LifecycleListener { public void onEvent(CoherenceLifecycleEvent event) { // process event } } The interceptor can be added to the configuration: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withInterceptor(new MyInterceptor()) .build(); When a Coherence instance created from this configuration is start or stopped the MyInterceptor instance will receive events. ",
            "title": "Coherence Lifecycle Interceptors"
        },
        {
            "location": "/docs/core/02_bootstrap",
            "text": " Coherence has a simple bootstrap API that allows a Coherence application to be configured and started by building a com.tangol.net.Coherence instance and starting it. The Coherence instance provides access to one or more com.tangosol.net.Session instances. A com.tangosol.net.Session gives access to Coherence clustered resources, such as NamedMap , NamedCache , NamedTopic etc. Sessions can be of different types, for example a session can be related to a ConfigurableCacheFactory , itself configured from a configuration file, or a session might be a client-side gRPC session. An example of some application bootstrap code might look like this: <markup lang=\"java\" >import com.tangosol.net.Coherence; import com.tangosol.net.CoherenceConfiguration; import com.tangosol.net.SessionConfiguration; public class Main { public static void main(String[] args) { // Create a Session configuration SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); // Create a Coherence instance configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(session) .build(); // Create the Coherence instance from the configuration Coherence coherence = Coherence.clusterMember(cfg); // Start Coherence coherence.start(); } } A SessionConfiguration is created. In this case the Session will be named Carts and will be created from the cache-config.xml configuration file. A CoherenceConfiguration is created to configure the Coherence instance. This configuration contains the Carts session configuration. A Coherence cluster member instance is created from the CoherenceConfiguration The Coherence instance is started. Running A Coherence Server The com.tangol.net.Coherence contains a main method that allows it to be used to run a Coherence server as a more powerful to alternative DefaultCacheServer . <markup lang=\"bash\" >$ java -cp coherence.jar com.tangosol.net.Coherence Without any other configuration, the default Coherence instance started this way will run an identical server to that run using DefaultCacheServer . The steps above are covered in more detail below. Session Configurations A SessionConfiguration is created by using the SessionConfiguration builder as shown in the example above. The Default Session When running Coherence if no configuration is specified the default behaviour is to use the default configuration file to configure Coherence. This behaviour still applies to the bootstrap API. If a Coherence instance is started without specifying any session configurations then a single default Session will be created. This default Session will wrap a ConfigurableCacheFactory that has been created from the default configuration file. The default file name is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig system property. When creating a CoherenceConfiguration the default session can be added using the SessionConfiguration.defaultSession() helper method. This method returns a SessionConfiguration configured to create the default Session . For example, in the code below the default session configuration is specifically added to the CoherenceConfiguration . <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); Session Name All sessions have a name that must be unique within the application. If a name has not been specified when the SessionConfiguration is built the default name of $Default$ will be used. A Coherence instance will fail to start if duplicate Session names exist. For example, this configuration will have the default name. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .build(); This configuration will have the name Test . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .name(\"Test\") .build(); Session Configuration URI The most common type of session is a wrapper around a ConfigurableCacheFactory . When using the SessionConfiguration builder the configuration file URI is specified using the withConfigUri() method, that takes a string value specifiying the configuration file location. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"cache-config.xml\") .build(); The example above uses a configuration file a named cache-config.xml . If a configuration URI is not specified then the default value will be used. This value is coherence-cache-config.xml unless this has been overridden with the coherence.cacheconfig System property. Session Event Interceptors Coherence provides many types of events, examples of a few would be life-cycle events for Coherence itself, cache life-cycle events, cache entry events, partition events etc. These events can be listened to by implementing an EventInterceptor that receives specific types of event. Event interceptors can be registered with a Session as part of its configuration. For example, suppose there is an interceptor class in the application called CacheInterceptor that listens to CacheLifecycleEvent when caches get created or destroyed. This interceptor can be added to the session as shown below: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withInterceptor(new CacheInterceptor()) .build(); The interceptor will receive cache life-cycle events for all caches created using the session. Session Scope Scope is a concept that has been in Coherence for quite a while that allows services to be scoped and hence isolated from other services with the same name. For example multiple ConfigurableCacheFactory instances could be loaded from the same XML configuration file but given different scope names so that each CCF will have its own services in the cluster. Unless you require multiple Sessions, a scope will not generally be used in a configuration. A scope for a session can be configured using the configuration&#8217;s withScopeName() method, for example: <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withScopeName(\"Test\") .build(); The session (and any ConfigurableCacheFactory it wraps) created from the configuration above will have a scope name of Test . It is possible to set a scope name in the &lt;defaults&gt; section of the XML configuration file. <markup lang=\"xml\" title=\"scoped-configuration.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;defaults&gt; &lt;scope-name&gt;Test&lt;/scope-name&gt; &lt;/defaults&gt; A ConfigurableCacheFactory created from the XML above, and hence any Session that wraps it will have a scope of Test . Note When using the bootstrap API any scope name specifically configured in the SessionConfiguration (that is not the default scope name) will override the scope name in the XML file. For example, using the scoped-configuration.xml file above: In this case the scope name will be Foo because the scope name has been explicitly set in the SessionConfiguration . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .withScopeName(\"Foo\") .build(); In this case the scope name will be Foo because although no scope name has been explicitly set in the SessionConfiguration , the name has been set to Foo , so the scope name will default to Foo . <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as no scope name or session name has been explicitly set in the SessionConfiguration so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .withConfigUri(\"scoped-configuration.xml\") .build(); In this case the scope name will be Test as the session name has been set to Foo but the scope name has been explicitly set to the default scope name using the constant Coherence.DEFAULT_SCOPE so the scope name of Test will be used from the XML configuration. <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(Coherence.DEFAULT_SCOPE) .withConfigUri(\"scoped-configuration.xml\") .build(); Coherence Configuration A Coherence application is started by creating a Coherence instance from a CoherenceConfiguration . An instance of CoherenceConfiguration is created using the builder. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Adding Sessions A Coherence instance manages one or more Session instances, which are added to the CoherenceConfiguration by adding the SessionConfiguration instances to the builder. If no sessions have been added to the builder the Coherence instance will run a single Session that uses the default configuration file. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); The configuration above will configure a Coherence instance with the default name and with a single Sessions that wil use the default configuration file. The default session can also be explicitly added to the CoherenceConfiguration : <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .build(); As already shown, other session configurations may also be added to the CoherenceConfiguration : <markup lang=\"java\" >SessionConfiguration session = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(session) .build(); Whilst there is no limit to the number of sessions that can be configured the majority of applications would only ever require a single session - more than likely just the default session. Session Configuration Auto-Discovery A CoherenceConfiguration can be configured to automatically discover SessionConfiguration instances. These are discovered using the Java ServiceLoader . Any instances of SessionConfiguration or SessionConfiguration.Provider configured as services in META-INF/services/ files will be loaded. This is useful if you are building modular applications where you want to include functionality in a separate application module that uses its own Session . The SessionConfiguration for the module is made discoverable by the ServiceLoader then whenever the module&#8217;s jar file is on the classpath the Session will be created, and the module&#8217;s functionality will be available to the application. For example: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .discoverSessions() .build(); The call to discoverSessions() will load discovered SessionConfiguration instances. Coherence Instance Name Each Coherence instance must be uniquely named. A name can be specified using the named() method on the builder, if no name has been specified the default name of $Default$ will be used. In the majority of use-cases an application would only ever require a single Coherence instance so there would be no requirement to specify a name. <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); The configuration above will create a Coherence instance with the name Carts . Add Global Event Interceptors As already mentioned, event interceptors can be added to a SessionConfiguration to receive events for a session. Event interceptors can also be added to the Coherence instance to receive events for all Session instances managed by that Coherence instance. For example, reusing the previous CacheInterceptor class, but this time for caches in all sessions: <markup lang=\"java\" >SessionConfiguration cartsSession = SessionConfiguration.builder() .named(\"Carts\") .withConfigUri(\"cache-config.xml\") .build(); CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withSession(cartsSession) .withInterceptor(new CacheInterceptor()) .build(); Now the CacheInterceptor will receive events for both the default session and the Certs session. Create a Coherence Instance A CoherenceConfiguration can be used to create a Coherence instance. A Coherence instance is created in one of two modes, either cluster member or client. The mode chosen affects how some types of Session are created and whether auto-start services are started. As the name suggests a \"cluster member\" is a Coherence instance that expects to start or join a Coherence cluster. In a cluster member any Session that wraps a ConfigurableCacheFactory will be have its services auto-started and monitored (this is the same behaviour that would have happened when using DefaultCacheServer to start a server). A \"client\" Coherence instance is typically not a cluster member, i.e. it is a Coherence*Extend or gRPC client. As such, Session instances that wrap a ConfigurableCacheFactory will not be auto-started, they will start on demand as resources such as maps, caches or topics are requested from them. The com.tangosol.net.Coherence class has static factory methods to create Coherence instances in different modes. For example, to create a Coherence instance that is a cluster member the Coherence.clusterMember method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.clusterMember(cfg); For example, to create a Coherence instance that is a client the Coherence.client method is used: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .build(); Coherence coherence = Coherence.client(cfg); Create a Default Coherence Instance It is possible to create a Coherence instance without specifying any configuration. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(); <markup lang=\"java\" >Coherence coherence = Coherence.client(); In both of the above examples the Coherence instance will have the default Session and any discovered sessions . Start Coherence A Coherence instance it must be started to start all the sessions that the Coherence instance is managing. This is done by calling the start() method. <markup lang=\"java\" >Coherence coherence = Coherence.clusterMember(cfg); coherence.start(); Obtaining a Coherence Instance To avoid having to pass around the instance of Coherence that was used to bootstrap an application the Coherence class has some static methods that make it simple to retrieve an instance. If only a single instance of Coherence is being used in an application (which will cover most use-cases) then the getInstance() method can be used: <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); It is also possible to retrieve an instance by name: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .named(\"Carts\") .build(); Coherence.create(cfg); &#8230;&#8203;then later&#8230;&#8203; <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); Ensuring Coherence Has Started If application code needs to ensure that a Coherence instance has started before doing some work then the whenStarted() method can be used to obtain a CompletableFuture that will be completed when the Coherence instance has started. <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(\"Carts\"); CompletableFuture&lt;Void&gt; future = coherence.whenStarted(); future.join(); There is also a corresponding whenStopped() method that returns a future that will be completed when the Coherence instance stops. Coherence Lifecycle Interceptors Besides using the future methods described above it is possible to add and EventInterceptor to the configuration of a Coherence instance that will receive life-cycle events. Below is an example interceptor that implements Coherence.LifecycleListener . <markup lang=\"java\" >public class MyInterceptor implements Coherence.LifecycleListener { public void onEvent(CoherenceLifecycleEvent event) { // process event } } The interceptor can be added to the configuration: <markup lang=\"java\" >CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.defaultSession()) .withInterceptor(new MyInterceptor()) .build(); When a Coherence instance created from this configuration is start or stopped the MyInterceptor instance will receive events. ",
            "title": "Bootstrap API"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config provides support for Eclipse MicroProfile Config within Coherence cluster members. It allows you both to configure various Coherence parameters from the values specified in any of the supported config sources, and to use Coherence cache as another, mutable config source. ",
            "title": "Coherence MicroProfile Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " In order to use Coherence MP Config, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-config&lt;/artifactId&gt; &lt;version&gt;21.12.6-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; You will also need an implementation of the Eclipse MP Config specification as a dependency. For example, if you are using Helidon , add the following to your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-config&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- optional: add it if you want YAML config file support --&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-config-yaml&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence provides a number of configuration properties that can be specified by the users in order to define certain attributes or to customize cluster member behavior at runtime. For example, attributes such as cluster and role name, as well as whether a cluster member should or should not store data, can be specified via system properties: -Dcoherence.cluster=MyCluster -Dcoherence.role=Proxy -Dcoherence.distributed.localstorage=false Most of these attributes can also be defined within the operational or cache configuration file. For example, you could define first two attributes, cluster name and role, within the operational config override file: <markup lang=\"xml\" > &lt;cluster-config&gt; &lt;member-identity&gt; &lt;cluster-name&gt;MyCluster&lt;/cluster-name&gt; &lt;role-name&gt;Proxy&lt;/role-name&gt; &lt;/member-identity&gt; &lt;/cluster-config&gt; While these two options are more than enough in most cases, there are some issues with them being the only way to configure Coherence: When you are using one of Eclipse MicroProfile implementations, such as Helidon as the foundation of your application, it would be nice to define some of Coherence configuration parameters along with your other configuration parameters, and not in the separate file or via system properties. In some environments, such as Kubernetes, Java system properties are cumbersome to use, and environment variables are a preferred way of passing configuration properties to containers. Unfortunately, neither of the two use cases above is supported out of the box, but that&#8217;s the gap Coherence MP Config is designed to fill. As long as you have coherence-mp-config and an implementation of Eclipse MP Config specification to your class path, Coherence will use any of the standard or custom config sources to resolve various configuration options it understands. Standard config sources in MP Config include META-INF/microprofile-config.properties file, if present in the class path, environment variables, and system properties (in that order, with the properties in the latter overriding the ones from the former). That will directly address problem #2 above, and allow you to specify Coherence configuration options via environment variables within Kubernetes YAML files, for example: <markup lang=\"yaml\" > containers: - name: my-app image: my-company/my-app:1.0.0 env: - name: COHERENCE_CLUSTER value: \"MyCluster\" - name: COHERENCE_ROLE value: \"Proxy\" - name: COHERENCE_DISTRIBUTED_LOCALSTORAGE value: \"false\" Of course, the above is just an example&#8201;&#8212;&#8201;if you are running your Coherence cluster in Kubernetes, you should really be using Coherence Operator instead, as it will make both the configuration and the operation of your Coherence cluster much easier. You will also be able to specify Coherence configuration properties along with the other configuration properties of your application, which will allow you to keep everything in one place, and not scattered across many files. For example, if you are writing a Helidon application, you can simply add coherence section to your application.yaml : <markup lang=\"yaml\" >coherence: cluster: MyCluster role: Proxy distributed: localstorage: false ",
            "title": "Configuring Coherence using MP Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config also provides an implementation of Eclipse MP Config ConfigSource interface, which allows you to store configuration parameters in a Coherence cache. This has several benefits: Unlike pretty much all of the default configuration sources, which are static, configuration options stored in a Coherence cache can be modified without forcing you to rebuild your application JARs or Docker images. You can change the value in one place, and it will automatically be visible and up to date on all the members. While the features above give you incredible amount of flexibility, we also understand that such flexibility is not always desired, and the feature is disabled by default. If you want to enable it, you need to do so explicitly, by registering CoherenceConfigSource as a global interceptor in your cache configuration file: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.mp.config.CoherenceConfigSource&lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;!-- your cache mappings and schemes... --&gt; &lt;/cache-config&gt; Once you do that, CoherenceConfigSource will be activated as soon as your cache factory is initialized, and injected into the list of available config sources for your application to use via standard MP Config APIs. By default, it will be configured with a priority (ordinal) of 500, making it higher priority than all the standard config sources, thus allowing you to override the values provided via config files, environment variables and system properties. However, you have full control over that behavior and can specify different ordinal via coherence.mp.config.source.ordinal configuration property. ",
            "title": "Using Coherence Cache as a Config Source"
        }
 ]
}