{
    "docs": [
        {
            "location": "/coherence-helidon-grpc-proxy/README",
            "text": " Coherence gRPC proxy is the server-side implementation of the services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. If using the Helidon Microprofile server with the microprofile gRPC server enabled the Coherence gRPC proxy can be deployed into the Helidon gRPC server instead of the Coherence default gRPC server. ",
            "title": "Helidon MP gRPC Server"
        },
        {
            "location": "/coherence-helidon-grpc-proxy/README",
            "text": " In order to use Coherence gRPC Server, you need to declare it as a dependency of your project; for example if using Maven: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-helidon-grpc-proxy&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; or for Gradle: <markup lang=\"groovy\" >implementation 'com.oracle.coherence.ce:coherence-helidon-grpc-proxy:23.03-SNAPSHOT' ",
            "title": "Usage"
        },
        {
            "location": "/coherence-helidon-grpc-proxy/README",
            "text": " For this behaviour to happen automatically just set the coherence.grpc.enabled system property to false , which will disable the built in server. A built-in GrpcMpExtension implementation will then deploy the proxy services to the Helidon gRPC server. When using the Helidon MP gRPC server, if the coherence.grpc.enabled system property has not been set to false , then both the Helidon gRPC server and the Coherence default gRPC server will start and could cause port binding issues unless they are both specifically configured to use different ports. ",
            "title": "Enable the Proxy Service in Helidon MP"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; ",
            "title": "Data Model"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. ",
            "title": "Topics Cache Configuration"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = session.createPublisher(\"public-messages\"); // create a subscriber to receive public messages subscriberPublic = session.createSubscriber(\"public-messages\"); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = session.createPublisher(\"private-messages\"); // create a subscriber to receive private messages subscriberPrivate = session.createSubscriber(\"private-messages\", inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } ",
            "title": "The Chat Application"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = session.createPublisher(\"public-messages\"); // create a subscriber to receive public messages subscriberPublic = session.createSubscriber(\"public-messages\"); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = session.createPublisher(\"private-messages\"); // create a subscriber to receive private messages subscriberPrivate = session.createSubscriber(\"private-messages\", inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? ",
            "title": "Build and Run the Example"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " In this tutorial you have learned how use Coherence Topics. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " Topics Overview and Configuration Performing Topics Operations ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/460-topics/README",
            "text": " This tutorial walks through the steps to use Coherence Topics using a simple Chat Application. Table of Contents What You Will Build What You Need Review the Initial Project Maven Configuration Data Model Topics Cache Configuration The Chat Application Build and Run the Example Summary See Also What You Will Build You will review, build and run a simple chat client which showcases using Coherence Topics. When running the chat client, the user can send a message in two ways: Send to all connected users using a publish/ subscribe model. For this functionality we create a topic called public-messages and all users are anonymous subscribers. Any messages to this topic will only be received by subscribers that are active. Send a private message to an individual user using a subscriber group. This uses a separate topic called private-messages and each subscriber to the topic specifies their userId as a subscriber group. Each value is only delivered to one of its subscriber group members, meaning the message will only be received by the individual user. We do not cover all features in Coherence Topics, so if you wish to read more about Coherence Topics, please see the Coherence Documentation . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Initial Project Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a server profile to run one or more DefaultCacheServer processes. <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;${coherence.common.properties}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Data Model The data model consists of the ChatMessage client which contains chat messages sent either on the private or publish topics. The properties are shown below: <markup lang=\"java\" >/** * Date the message was sent. */ private final long date; /** * The user who sent the message. */ private final String fromUserId; /** * The recipient of the message or null if public message. */ private final String toUserId; /** * The type of message. */ private final Type type; /** * The contents of the message. */ private final String message; Topics Cache Configuration The following topic-scheme-mapping element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;topic-mapping&gt; &lt;topic-name&gt;public-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;private-messages&lt;/topic-name&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;value-type&gt;com.oracle.coherence.guides.topics.ChatMessage&lt;/value-type&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;admin&lt;/name&gt; &lt;/subscriber-group&gt; &lt;/subscriber-groups&gt; &lt;/topic-mapping&gt; The topics defined are described below: public-messages - contains public messages private-messages - contains private messages and contains an initial subscriber group named admin in configuration. Because we have specifically add the admin subscriber group in the cache config, this means that it will be created on startup of the cache server and messages to admin will be durable. Messages for subscriber groups created on the fly, by specifying Name.of(\"groupName\") when creating a subscriber, are only durable from the time the subscribe group is created. The following caching-schemes element is defined in src/main/resources/topics-cache-config.xml : <markup lang=\"xml\" >&lt;!-- partitioned topic scheme for servers --&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-server&lt;/scheme-name&gt; &lt;service-name&gt;${coherence.service.name Partitioned}Topic&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;autostart system-property=\"coherence.topic.enabled\"&gt;true&lt;/autostart&gt; &lt;high-units&gt;{topic-high-units-bytes 0B}&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The above paged-topic-scheme has no size limit and is automatically started. The Chat Application The chat application is a simple text based client which does the following: Starts up with an argument specifying the user id of the user Displays a menu, shown below, where a user can send a message to all connected users or privately to an individual. <markup lang=\"bash\" >Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message We will examine each of the components in detail below: Topics, Subscribers and Publishers <markup lang=\"java\" >/** * Publisher for public messages. */ private final Publisher&lt;ChatMessage&gt; publisherPublic; /** * Publisher for private messages. */ private final Publisher&lt;ChatMessage&gt; publisherPrivate; /** * Subscriber for public messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPublic; /** * Subscriber for private messages. */ private final Subscriber&lt;ChatMessage&gt; subscriberPrivate; System Properties As we are creating a shaded Jar, we are including the following system properties to set the cache configuration file, turn off local storage and reduce the log level. <markup lang=\"java\" >System.setProperty(\"coherence.distributed.localstorage\", \"false\"); System.setProperty(\"coherence.log.level\", \"2\"); Obtain a Coherence session <markup lang=\"java\" >Coherence coherence = Coherence.getInstance(); if (coherence == null) { Coherence.clusterMember().start().join(); coherence = Coherence.getInstance(); } Session session = coherence.getSession(); Create the public Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish public messages publisherPublic = session.createPublisher(\"public-messages\"); // create a subscriber to receive public messages subscriberPublic = session.createSubscriber(\"public-messages\"); Creates a publisher to publish messages to the topic Creates a subscriber (anonymous) to receive all messages published to the topic Create the private Topic, Subscribers and Publishers <markup lang=\"java\" >// create a publisher to publish private messages publisherPrivate = session.createPublisher(\"private-messages\"); // create a subscriber to receive private messages subscriberPrivate = session.createSubscriber(\"private-messages\", inGroup(userId)); Creates a publisher to publish messages to the topic Creates a subscriber with a subscriber group of the user to receive private messages When the application starts, two subscriptions are initiated. One to receive messages from the public topic and one to receive messages from the private topic. <markup lang=\"java\" >// subscription for anonymous subscriber/ public messages subscriberPublic.receive().handle((v, err) -&gt; receive(v, err, subscriberPublic)); // subscription for subscriber group / private durable messages subscriberPrivate.receive().handle((v, err) -&gt; receive(v, err, subscriberPrivate)); We are just using the default ForkJoin pool for this example but handleAsync can accept and Executor which would be better practice. Each of the above subscribers call the receive message which will resubscribe. <markup lang=\"java\" >/** * Receive a message from a given {@link Subscriber} and once processed, re-subscribe. * @param element {@link Element} received * @param throwable {@link Throwable} if any errors * @param subscriber {@link Subscriber} to re-subscribe to * @return void */ public Void receive(Element&lt;ChatMessage&gt; element, Throwable throwable, Subscriber&lt;ChatMessage&gt; subscriber) { if (throwable != null) { if (throwable instanceof CancellationException) { // exiting process, ignore. } else { log(throwable.getMessage()); } } else { ChatMessage chatMessage = element.getValue(); getMessageLog(chatMessage) .ifPresent(message -&gt; { messagesReceived.incrementAndGet(); log(message); }); element.commit(); subscriber.receive().handle((v, err) -&gt; receive(v, err, subscriber)); } return null; } Retrieve the ChatMessage Call a method to generate a string representation of the message and display it Commit the element so that we do not receive the message again Receive the next message Generate a join message on startup <markup lang=\"java\" >// generate a join message and send synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.JOIN, null)).join(); Send a public message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"send \")) { // send public message synchronously publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.MESSAGE, line.substring(5))) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Send a private message when the user uses the sendpm command: <markup lang=\"java\" >} else if (line.startsWith(\"sendpm \")) { // send private durable message String[] parts = line.split(\" \"); // extract the target user and message if (parts.length &lt; 3) { log(\"Usage: sendpm user message\"); } else { String user = parts[1]; String message = line.replaceAll(parts[0] + \" \" + parts[1] + \" \", \"\"); publisherPrivate.publish(new ChatMessage(userId, user, ChatMessage.Type.MESSAGE, message)) .handle(this::handleSend); Asynchronously send the message and increment the messages sent when complete Generate a leave message on exit and cleanup <markup lang=\"java\" >private void cleanup() { // generate a leave message if (publisherPublic.isActive()) { publisherPublic.publish(new ChatMessage(userId, null, ChatMessage.Type.LEAVE, null)).join(); publisherPublic.flush().join(); publisherPublic.close(); } if (subscriberPublic.isActive()) { subscriberPublic.close(); } if (publisherPrivate.isActive()) { publisherPrivate.flush().join(); publisherPrivate.close(); } if (subscriberPrivate.isActive()) { subscriberPrivate.close(); } } Build and Run the Example Build the project using either of the following: <markup lang=\"bash\" >./mvnw clean package or <markup lang=\"bash\" >./gradlew clean build Start one or more Coherence Cache Servers using the following: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer Start the first chat client with the user Tim <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar Tim or <markup lang=\"bash\" >./gradlew runClient -PuserId=Tim --console=plain You will notice output similar to the following: <markup lang=\"bash\" >Oracle Coherence Version 20.12 Build demo Grid Edition: Development mode Copyright (c) 2000, 2021, Oracle and/or its affiliates. All rights reserved. User: Tim Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (Tim)&gt; Start a second second client with the name Helen . You will see a message on Tim&#8217;s chat application indicating Helen has joined the chat. <markup lang=\"bash\" >Chat (Tim)&gt; 14:14:30 Helen joined the chat Use send hello from Helen&#8217;s chat and you will notice that the message is dispalyed on Tim&#8217;s chat. To show how subscriber groups work, send a private message using the following from Tim to JK . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK Hello JK Also send a private message to admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Please ping me when you get in as i have an issue with my Laptop Start a third chat application with JK as the user: <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar JK User: JK Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (JK)&gt; You will notice that the private message for JK was not delivered as the subscriber group JK was only created when he joined and therefore messages send previously are not stored. You will also see join messages on the other terminals. Type quit in Helen&#8217;s terminal and restart the client as admin <markup lang=\"bash\" >java -jar target/topics-1.0.0-SNAPSHOT.jar admin User: admin Commands: quit - Quit the chat help - Display help send - Send public message sendpm userId - Send private message Chat (admin)&gt; 14:18:29 Tim (Private) - Please ping me when you get in as i have an issue with my Laptop You will notice that the message sent before admin joined is now delivered as the admin subscriber group was created in configuration and add on server startup. Type a message send Got to go, bye on JK&#8217;s chat application and then quit . The message along with the leave notification will be shown on the other terminals. <markup lang=\"bash\" >Chat (JK)&gt; send Got to go, bye Now that JK has quit the application, send a private message from Tim to JK using sendpm JK please ping me . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm JK please ping me Start the client as JK and you will see the message displayed now as the subscriber group is created. Finally send a private messge from Tim to admin using sendpm admin Are you free for lunch? . You will notice this message is only displayed for admin . <markup lang=\"bash\" >Chat (Tim)&gt; sendpm admin Are you free for lunch? Summary In this tutorial you have learned how use Coherence Topics. See Also Topics Overview and Configuration Performing Topics Operations ",
            "title": "Topics"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " About 30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This example shows how to build a custom aggregator which we will use to count how many times a particular word occurs in documents stored in Coherence maps. The Document class is a standard POJO with an identifier, and a string for the document contents. What You Need About 30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " The data model consists of the Document class which represents a document with text contents that we are going to search. <markup lang=\"java\" >public class Document implements Serializable { private String id; private String contents; Review the Example Code The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " In this guide we have shown you how to create custom aggregators that allow you to process data stored in Coherence in parallel. You have created a custom aggregator to count the number of times a word appears in documents stored in Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " Performing Data Grid Operations Streams ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/121-custom-aggregators/README",
            "text": " This guide walks you through how to create custom aggregators that allow you to process data stored in Coherence in parallel. Coherence supports entry aggregators that perform operations against all, or a subset of entries to obtain a single result. This aggregation is carried out in parallel across the cluster and is a map-reduce type of operation which can be performed efficiently across large amounts of data. See the Coherence Documentation for detailed information on Aggregations. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build This example shows how to build a custom aggregator which we will use to count how many times a particular word occurs in documents stored in Coherence maps. The Document class is a standard POJO with an identifier, and a string for the document contents. What You Need About 30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the Document class which represents a document with text contents that we are going to search. <markup lang=\"java\" >public class Document implements Serializable { private String id; private String contents; Review the Example Code The WordCount class implements the InvocableMap.StreamingAggregator as well as Serializable for serialization. When you implement InvocableMap.StreamingAggregator , you must implement the following methods: supply() - creates an instance we can accumulate into in parallel accumulate() - adds single entry to partial result when executing on storage members getPartialResult() - returns the partial result combine() - combines partial results on the client finalizeResult() - applies finishing transformation to the final result and returns it See below for details of each of the WordCount class. Implementing interfaces <markup lang=\"java\" >public class WordCount&lt;K extends String, V extends Document&gt; implements InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt;, Serializable { Implement the InvocableMap.StreamingAggregator with key, value, partial result and final result Implement serialization The following constructor is used to set the words to search for. <markup lang=\"java\" >/** * Constructs a {@link WordCount}. * * @param setWords {@link Set} of words to search for */ public WordCount(Set&lt;String&gt; setWords) { this.setWords = setWords; } Creates an instance we can accumulate into in parallel when executing on the storage members <markup lang=\"java\" >@Override public InvocableMap.StreamingAggregator&lt;K, V, Map&lt;String, Integer&gt;, Map&lt;String, Integer&gt;&gt; supply() { return new WordCount&lt;&gt;(setWords); } Adds single entry to partial result when executing on storage members <markup lang=\"java\" >@Override public boolean accumulate(InvocableMap.Entry&lt;? extends K, ? extends V&gt; entry) { Document document = entry.getValue(); for (String word : setWords) { // count how many times the word exists in the the documents and accumulate int count = document.getContents().split(\"\\\\b\" + word + \"\\\\b\", -1).length - 1; this.mapResults.compute(word, (k, v) -&gt; v == null ? count : v + count); } return true; } Count the number of times the word occurs in the document Add or update the count for the word in the results Map Return the partial result <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; getPartialResult() { Logger.info(\"getPartialResult: \" + mapResults); return mapResults; } Combine all the partial results on the client <markup lang=\"java\" >@Override public boolean combine(Map&lt;String, Integer&gt; mapPartialResult) { Logger.info(\"combine: Received partial result \" + mapPartialResult); // combine the results passed in with the current set of results. if (!mapPartialResult.isEmpty()) { mapPartialResult.forEach((k, v) -&gt; mapResults.compute(k, (key, value) -&gt; value == null ? v : value + v)); } return true; } This method is called on the client to combine the results passed in with the current result. This is used to get the final set of results from all members. Take the final partial result and applies any finishing transformation <markup lang=\"java\" >@Override public Map&lt;String, Integer&gt; finalizeResult() { return mapResults; } Characteristics for the aggregator <markup lang=\"java\" >@Override public int characteristics() { return PARALLEL | PRESENT_ONLY; } We specifically set the PARALLEL and PRESENT_ONLY characteristics to indicate this can be run in parallel and to execute to only run on entries that are present. CustomAggregationExample Class The runExample() method contains the code that exercises the above custom aggregator. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { System.out.println(\"Documents added \" + documents.size()); // choose up to 5 random words from the list to search for Set&lt;String&gt; setWords = new HashSet&lt;&gt;(); for (int i = 0; i &lt; 5; i++) { setWords.add(getRandomValue(WORDS)); } System.out.println(\"Running against the following words: \" + setWords); Map&lt;String, Integer&gt; results = documents.aggregate(new WordCount&lt;&gt;(setWords)); results.forEach((k, v) -&gt; System.out.println(\"Word \" + k + \", number of occurrences: \" + v)); } Run the aggregator against 5 randomly chosen words Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running the example The example can be run direct from the IDE by directly running the CustomAggregationExample class , or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following indicating the documents and times that a word exists in a document. <markup lang=\"bash\" >Documents added 2000 Running against the following words: [fifteen, tv, trumpet, this, launch] &lt;Info&gt; (thread=PartitionedCacheWorker:0x0000:2, member=2): ***** getPartialResult: {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=177, tv=376, trumpet=210, this=1173, launch=193} &lt;Info&gt; (thread=main, member=2): ***** combine: Received partial result {fifteen=181, tv=350, trumpet=194, this=1155, launch=189} Word fifteen, number of occurrences: 358 Word tv, number of occurrences: 726 Word trumpet, number of occurrences: 404 Word this, number of occurrences: 2328 Word launch, number of occurrences: 382 The messages above containing combine are when the client called the combine() method to aggregate the final results returned from the storage members. In this case we had 2 storage members including the test itself. Summary In this guide we have shown you how to create custom aggregators that allow you to process data stored in Coherence in parallel. You have created a custom aggregator to count the number of times a word appears in documents stored in Coherence. See Also Performing Data Grid Operations Streams ",
            "title": "Custom Aggregators"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. ",
            "title": "Coherence NamedMap "
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Obtain a NamedMap Instance"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionally there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . ",
            "title": "A Quick Word About Serialization"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } ",
            "title": "The Put Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . ",
            "title": "The Get Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . ",
            "title": "Get Multiple Values"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . ",
            "title": "The Remove Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . ",
            "title": "The Remove Mapping Method"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. ",
            "title": " NamedCache Transient Data"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/100-put-get-remove/README",
            "text": " This guide walks you through the basic CRUD operations on a Coherence NamedMap . What You Will Build The example code is written as a set of unit tests, as this is the simplest way to demonstrate something as basic as individual NamedMap operations. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Coherence NamedMap The Coherence NamedMap is an extension of Java&#8217;s java.util.Map interface and as such, it has all the Map methods that a Java developer is familiar with. Coherence also has a NamedCache which extends NamedMap and is form more transient data storage in caching use cases. The most basic operations on a NamedMap are the simple CRUD methods, put , get and remove , which this guide is all about. Create the Test Class The first step is to create the test class that will show and test the various NamedMap operations, we&#8217;ll call this class BasicCrudTest . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class BasicCrudTest { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence v20.12. As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide <markup lang=\"java\" > @BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Second, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" > @AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different NamedMap operations. Obtain a NamedMap Instance All the tests in this guide need to obtain a NamedMap instance, we will use a Coherence Session for this. A Session is a means to access Coherence clustered resources. Creation of Session instances is part of the bootstrap API, which we can obtain named Session instances from. In this case we are using the bootstrap API&#8217;s default, so we can simply obtain the default Session . To get a NamedMap from a Session we use the Session.getMap() method. This take a String value, which is the name of the map to obtain from the Session . There are a number of ways we could have encapsulated this common code in the test class. In this case we will create a simple utility method to get a NamedMap with a give name that the different test methods can call. <markup lang=\"java\" > &lt;K, V&gt; NamedMap&lt;K, V&gt; getMap(String name) { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); return session.getMap(name); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method. Obtain the default Session from the Coherence instance. Obtain and return the NamedMap instance with the required name. A Quick Word About Serialization In almost every case a NamedMap is backed by a distributed, clustered, Coherence resource. For this reason all Objects used as keys and values must be serializable so that they can be transferred between cluster members and clients during requests. Coherence Serialization support is a topic that deserves a guide of its own The Serializer implementation used by a NamedMap is configurable and Coherence comes with some out of the box Serializer implementations. The default is Java serialization, so all keys and values must be Java Serializable or implement Coherence ExternalizableLite interface for more control of serialization. Alternatively Coherence can also be configured to use Portable Object Format for serialization and additionally there is a JSON Coherence module that provides a JSON serializer that may be used. To keep this guide simple we are going to stick with the default serializer, so all NamedMap operations will use classes that are Serializable . The Put Method The obvious place to start is to add data to a NamedMap using the put method. We will create a simple test method that uses put to add a new key and value to a NamedMap . <markup lang=\"java\" > @Test void shouldPutNewKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); String oldValue = map.put(\"key-1\", \"value-1\"); assertNull(oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We call the put method to map the key \"key-1\" to the value \"value-1\" . As NamedMap implements java.util.Map , the put contract says that the put method returns the previous valued mapped to the key. In this case there was no previous value mapped to \"key-1\" , so the returned value must be null . To show that we do indeed get back the old value returned from a put , we can write a slightly different test method that puts a new key and value into a NamedMap then updates the mapping with a new value. <markup lang=\"java\" > @Test void shouldPutExistingKeyAndValue() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-2\", \"value-1\"); String oldValue = map.put(\"key-2\", \"value-2\"); assertEquals(\"value-1\", oldValue); } The Get Method We have seen how we can add data to a NamedMap using the put method, so the obvious next step is to get the data back out using the get method. <markup lang=\"java\" > @Test void shouldGet() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-3\", \"value-1\"); String value = map.get(\"key-3\"); assertEquals(\"value-1\", value); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the NamedMap mapping the key \"key-3\" to the value \"value-1\" ; We use the get method to get the value from the NamedMap that is mapped to the key \"key-3\" , which obviously must be \"value-1\" . Get Multiple Values The Coherence NamedMap contains a getAll(java.util.Collection) method that takes a collection of keys as a parameter and returns a new Map that contains the requested mappings. <markup lang=\"java\" > @Test void shouldGetAll() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-5\", \"value-5\"); map.put(\"key-6\", \"value-6\"); map.put(\"key-7\", \"value-7\"); Map&lt;String, String&gt; results = map.getAll(Arrays.asList(\"key-5\", \"key-7\", \"key-8\")); assertEquals(2, results.size()); assertEquals(\"value-5\", results.get(\"key-5\")); assertEquals(\"value-7\", results.get(\"key-7\")); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. We call the getAll method requesting keys \"key-5\" , \"key-7\" and \"key-8\" . The result map returned should only contain two keys, because although we requested the mappings for three keys, \"key-8\" was not added to the NamedMap . The value mapped to \"key-5\" should be \"value-5\" . The value mapped to \"key-7\" should be \"value-7\" . The Remove Method We&#8217;ve now seen adding data to and getting data from a NamedMap , we can also remove values mapped to a key with the remove method. <markup lang=\"java\" > @Test void shouldRemove() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-9\", \"value-9\"); String oldValue = map.remove(\"key-9\"); assertEquals(\"value-9\", oldValue); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-9\" . The contract of the remove method says that the value returned should be the value that was mapped to the key that was removed (or null if there was no mapping to the key). In this case the returned value must be \"value-9\" . The Remove Mapping Method An alternate version of the remove method is the two argument remove method that removes a mapping to a key if the key is mapped to a specific value. <markup lang=\"java\" > @Test void shouldRemoveMapping() { NamedMap&lt;String, String&gt; map = getMap(\"data\"); map.put(\"key-10\", \"value-10\"); boolean removed = map.remove(\"key-10\", \"Foo\"); assertFalse(removed); removed = map.remove(\"key-10\", \"value-10\"); assertTrue(removed); } We call the getMap utility method we wrote above to get a NamedMap with the name data . In this case the map&#8217;s keys and values are both of type String . We add some data to the map. Call the remove method to remove the value mapped to key \"key-10\" with a value of \"Foo\" . This must return false as we mapped \"key-10\" to the value \"value-10\" , so nothing will be removed from the NamedMap . Call the remove method to remove the value mapped to key \"key-10\" with a value of \"value-10\" . This must return true as we mapped \"key-10\" to the value \"value-10\" , so the mapping will be removed from the NamedMap . NamedCache Transient Data As already stated, a NamedCache is typically used to store transient data in caching use-cases. The NamedCache has an alternative put(K,V,long) method that takes a key, value, and an expiry value. The expiry value is the number of milli-seconds that the key and value should remain in the cache. When the expiry time has passed the key and value will be removed from the cache. <markup lang=\"java\" > @Test void shouldPutWithExpiry() throws Exception { Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedCache&lt;String, String&gt; cache = session.getCache(\"test\"); cache.put(\"key-1\", \"value-1\", 2000); String value = cache.get(\"key-1\"); assertEquals(\"value-1\", value); Thread.sleep(3000); value = cache.get(\"key-1\"); assertNull(value); } In the same way that we obtained a NamedMap from the default Session , we can obtain a NamedCache using the getCache method, in this case the cache named test . Using the put with expiry method, we can add a key of \"key-1\" mapped to value \"value-1\" with an expiry of 2000 milli-seconds (or 2 seconds). If we now do a get for \"key-1\" we should get back \"value-1\" because two seconds has not yet passed (unless you are running this test on a terribly slow machine). Now we wait for three seconds to be sure the expiry time has passed. This time when we get \"key-1\" the value returned must be null because the value has expired, and been removed from the cache. Summary You have seen how simple it is to use simple CRUD methods on NamedMap and NamedCache instances, as well as the simplest way to bootstrap a default Coherence storage enabled server instance. ",
            "title": "Put Get and Remove Operations"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What You Will Build What You Need Building The Example Code The Power of CohQL Query Caches Programmatically Create the Test Class Bootstrap Coherence Filter ValueExtractor Aggregate Results Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The example code is written as a set of unit tests, showing you how can simply executed sophisticated queries against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " Before we start querying caches programmatically, you should be aware of the power of the Coherence Query Language (CohQL). CohQL is inspired by SQL and is a quick and easy way to interact with your caches. Commonly it is used as a command-line tool. Let&#8217;s assume we have a cache called countries that contains a map of Country classes with the 2-letter country code being the key of each cache entry. The Country class will have some basic properties such as name , capital and population . The simplest CohQL query you could write is a query that will return all countries is: <markup lang=\"sql\" >select * from countries As you can see, if you&#8217;re familiar with SQL, you will feel right at home. And of course from here we can make the query more sophisticated. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. In order to give you a way experiment with the cache using CohQL, we provide a simple sample app that pre-populates a Coherence cache countries wih countries and starts the CohQL Console. To get started, execute com.oracle.coherence.guides.queries.StartCohQLConsole . Typically, you would want to start the CohQL Console as a stand-alone application. Please see the following instructions to learn more. Once the console application is started, let&#8217;s execute: <markup lang=\"sql\" >select * from countries The result should be a list of 5 countries: <markup lang=\"java\" >Results Country{name='Colombia', capital='Bogotá', population=50.4} Country{name='Australia', capital='Canberra', population=26.0} Country{name='Ukraine', capital='Kyiv', population=41.2} Country{name='France', capital='Paris', population=67.4} Country{name='Germany', capital='Berlin', population=83.2} What if you would like to just retrieve the list of capitals? We can achieve that by selecting just the capital: <markup lang=\"sql\" >select capital from countries which yields: <markup lang=\"java\" >Results \"Bogotá\" \"Paris\" \"Canberra\" \"Kyiv\" \"Berlin\" Of course, you can also apply where clauses to further limit the results. For example, if you like to retrieve the countries with a population that is greater than 60 million you may add the following where clause: <markup lang=\"sql\" >select capital from countries c where population &gt; 60.0 which results in: <markup lang=\"java\" >Results \"Paris\" \"Berlin\" Another option is to aggregate results. For example, let&#8217;s calculate the total population of countries with a population larger than 60 million: <markup lang=\"sql\" >select sum(population) from countries c where population &gt; 60.0 which yields a value of 150.6 . CohQL is not merely a tool for query caches. It can also be used to create and delete caches, to insert , delete and update cache value, to create indices and more. For more information please see the official reference documentation. ",
            "title": "The Power of CohQL"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " So how would we create queries programmatically to retrieve the same results? The key here is to understand the following concepts: Filter ValueExtractor Aggregator ",
            "title": "Query Caches Programmatically"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence 20.12 . As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We will also populate the cache with several countries and thus let&#8217;s create a small helper class CoherenceHelper : <markup lang=\"java\" >public static void startCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); Session session = coherence.getSession(); NamedCache&lt;String, Country&gt; countries = session.getCache(\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); countries.put(\"fr\", new Country(\"France\", \"Paris\", 67.4)); countries.put(\"ua\", new Country(\"Ukraine\", \"Kyiv\", 41.2)); countries.put(\"co\", new Country(\"Colombia\", \"Bogotá\", 50.4)); countries.put(\"au\", new Country(\"Australia\", \"Canberra\", 26)); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Obtain the default Session Get the countries cache Populate the countries cache with several new Country instances We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide . <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { CoherenceHelper.startCoherence(); } Call CoherenceHelper and start the Coherence instance and populate the country data. Lastly, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different querying operations. ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " To get started, we would like to retrieve all countries that have a population of more than 60 million people. For that we will use a Filter : <markup lang=\"java\" >@Test void testGreaterEqualsFilter() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); final Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results).hasSize(2); map.entrySet(filter).forEach(entry -&gt; { //assertThat(entry.getKey()).containcontainsAnyOf(\"de\", \"fr\"); assertThat(entry.getValue().getPopulation()).isGreaterThan(60.0); }); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the Map The result should be 2 countries only Assert that only France and Germany were selected ",
            "title": "Filter"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What if we don&#8217;t want to return Countries but just the collection of country names for which the population is 60 million people or higher? This is where we can use a ValueExtractor in combination with a ReducerAggregator . A value extractor is used to extract a property from a given object. In most instances developers would use the ReflectionExtractor as an implementation. The ReducerAggregator on the other hand, is used to run a ValueExtractor against cache entries, and it returns the extracted value. The result returned by the ReducerAggregator is a Map where the key is the key of the cache entry and the value is the extracted value. <markup lang=\"java\" >@Test void testValueExtractor() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ReducerAggregator&lt;String, Country, String, String&gt; aggregator = new ReducerAggregator&lt;&gt;(\"getName\"); Map&lt;String, String&gt; result = map.aggregate(filter, aggregator); result.forEach((key, value) -&gt; { assertThat(key).containsAnyOf(\"de\", \"fr\"); assertThat(value).containsAnyOf(\"Germany\", \"France\"); }); } Get the countries Map We create the same GreaterEqualsFilter as in the previous test (Select countries with more than 60 million people, only) Create a ReducerAggregator instance and specify that we only want the name of the countries returned Apply the Filter and Aggregator Verify that only the two country names France and Germany are returned as filtered values ",
            "title": "ValueExtractor"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " What if we want to group queried data together? Let&#8217;s query for countries, where the population is greater than 60 million but instead of returning the countries, we will return the sum of the population of thsoe 2 countries instead. <markup lang=\"java\" >@Test void testAggregate() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(\"getPopulation\"); BigDecimal result = map.aggregate(filter, aggregator); String resultAsString = result.setScale(2, RoundingMode.HALF_UP) .stripTrailingZeros() .toPlainString(); assertThat(resultAsString).isEqualTo(\"150.6\"); } Get the countries Map We create the same GreaterEqualsFilter as in the previous test (Select countries with more than 60 million people, only) We will use a different Aggregator . BigDecimalSum will aggregate the population and return a Bigecimal value Apply the Filter and Aggregator For assertion purposes we will convert the BigDecimal value to a String The generated String shall not have any trailing zeros Return the String Verify that the returned value is 150.6 To learn much more about built-in Aggregators, please take a look at the respective guide . ",
            "title": "Aggregate Results"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " The first step is to create the test class that will show and test the various query operations, we&#8217;ll call this class QueryTests . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class QueryTests { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence 20.12 . As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We will also populate the cache with several countries and thus let&#8217;s create a small helper class CoherenceHelper : <markup lang=\"java\" >public static void startCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); Session session = coherence.getSession(); NamedCache&lt;String, Country&gt; countries = session.getCache(\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); countries.put(\"fr\", new Country(\"France\", \"Paris\", 67.4)); countries.put(\"ua\", new Country(\"Ukraine\", \"Kyiv\", 41.2)); countries.put(\"co\", new Country(\"Colombia\", \"Bogotá\", 50.4)); countries.put(\"au\", new Country(\"Australia\", \"Canberra\", 26)); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Obtain the default Session Get the countries cache Populate the countries cache with several new Country instances We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide . <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { CoherenceHelper.startCoherence(); } Call CoherenceHelper and start the Coherence instance and populate the country data. Lastly, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different querying operations. Filter To get started, we would like to retrieve all countries that have a population of more than 60 million people. For that we will use a Filter : <markup lang=\"java\" >@Test void testGreaterEqualsFilter() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); final Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results).hasSize(2); map.entrySet(filter).forEach(entry -&gt; { //assertThat(entry.getKey()).containcontainsAnyOf(\"de\", \"fr\"); assertThat(entry.getValue().getPopulation()).isGreaterThan(60.0); }); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the Map The result should be 2 countries only Assert that only France and Germany were selected ValueExtractor What if we don&#8217;t want to return Countries but just the collection of country names for which the population is 60 million people or higher? This is where we can use a ValueExtractor in combination with a ReducerAggregator . A value extractor is used to extract a property from a given object. In most instances developers would use the ReflectionExtractor as an implementation. The ReducerAggregator on the other hand, is used to run a ValueExtractor against cache entries, and it returns the extracted value. The result returned by the ReducerAggregator is a Map where the key is the key of the cache entry and the value is the extracted value. <markup lang=\"java\" >@Test void testValueExtractor() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ReducerAggregator&lt;String, Country, String, String&gt; aggregator = new ReducerAggregator&lt;&gt;(\"getName\"); Map&lt;String, String&gt; result = map.aggregate(filter, aggregator); result.forEach((key, value) -&gt; { assertThat(key).containsAnyOf(\"de\", \"fr\"); assertThat(value).containsAnyOf(\"Germany\", \"France\"); }); } Get the countries Map We create the same GreaterEqualsFilter as in the previous test (Select countries with more than 60 million people, only) Create a ReducerAggregator instance and specify that we only want the name of the countries returned Apply the Filter and Aggregator Verify that only the two country names France and Germany are returned as filtered values Aggregate Results What if we want to group queried data together? Let&#8217;s query for countries, where the population is greater than 60 million but instead of returning the countries, we will return the sum of the population of thsoe 2 countries instead. <markup lang=\"java\" >@Test void testAggregate() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(\"getPopulation\"); BigDecimal result = map.aggregate(filter, aggregator); String resultAsString = result.setScale(2, RoundingMode.HALF_UP) .stripTrailingZeros() .toPlainString(); assertThat(resultAsString).isEqualTo(\"150.6\"); } Get the countries Map We create the same GreaterEqualsFilter as in the previous test (Select countries with more than 60 million people, only) We will use a different Aggregator . BigDecimalSum will aggregate the population and return a Bigecimal value Apply the Filter and Aggregator For assertion purposes we will convert the BigDecimal value to a String The generated String shall not have any trailing zeros Return the String Verify that the returned value is 150.6 To learn much more about built-in Aggregators, please take a look at the respective guide . ",
            "title": "Create the Test Class"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " In this guide we showed how easy it is to query Coherence caches either using CohQL or programmatically using Filters, ValueExtractors and Aggregators. Please see the Coherence reference guide, specifically the chapter Querying Data In a Cache for more details. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " Using Coherence Query Language Querying Data In a Cache ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/110-queries/README",
            "text": " This guide walks you through the basic concepts of querying Coherence caches. We will provide a quick overview and examples of using Coherence Query Language (CohQL) before learning more about Filters , ValueExtractors and Aggregators to query caches programmatically. Table of Contents What You Will Build What You Need Building The Example Code The Power of CohQL Query Caches Programmatically Create the Test Class Bootstrap Coherence Filter ValueExtractor Aggregate Results Summary See Also What You Will Build The example code is written as a set of unit tests, showing you how can simply executed sophisticated queries against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build The Power of CohQL Before we start querying caches programmatically, you should be aware of the power of the Coherence Query Language (CohQL). CohQL is inspired by SQL and is a quick and easy way to interact with your caches. Commonly it is used as a command-line tool. Let&#8217;s assume we have a cache called countries that contains a map of Country classes with the 2-letter country code being the key of each cache entry. The Country class will have some basic properties such as name , capital and population . The simplest CohQL query you could write is a query that will return all countries is: <markup lang=\"sql\" >select * from countries As you can see, if you&#8217;re familiar with SQL, you will feel right at home. And of course from here we can make the query more sophisticated. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. In order to give you a way experiment with the cache using CohQL, we provide a simple sample app that pre-populates a Coherence cache countries wih countries and starts the CohQL Console. To get started, execute com.oracle.coherence.guides.queries.StartCohQLConsole . Typically, you would want to start the CohQL Console as a stand-alone application. Please see the following instructions to learn more. Once the console application is started, let&#8217;s execute: <markup lang=\"sql\" >select * from countries The result should be a list of 5 countries: <markup lang=\"java\" >Results Country{name='Colombia', capital='Bogotá', population=50.4} Country{name='Australia', capital='Canberra', population=26.0} Country{name='Ukraine', capital='Kyiv', population=41.2} Country{name='France', capital='Paris', population=67.4} Country{name='Germany', capital='Berlin', population=83.2} What if you would like to just retrieve the list of capitals? We can achieve that by selecting just the capital: <markup lang=\"sql\" >select capital from countries which yields: <markup lang=\"java\" >Results \"Bogotá\" \"Paris\" \"Canberra\" \"Kyiv\" \"Berlin\" Of course, you can also apply where clauses to further limit the results. For example, if you like to retrieve the countries with a population that is greater than 60 million you may add the following where clause: <markup lang=\"sql\" >select capital from countries c where population &gt; 60.0 which results in: <markup lang=\"java\" >Results \"Paris\" \"Berlin\" Another option is to aggregate results. For example, let&#8217;s calculate the total population of countries with a population larger than 60 million: <markup lang=\"sql\" >select sum(population) from countries c where population &gt; 60.0 which yields a value of 150.6 . CohQL is not merely a tool for query caches. It can also be used to create and delete caches, to insert , delete and update cache value, to create indices and more. For more information please see the official reference documentation. Query Caches Programmatically So how would we create queries programmatically to retrieve the same results? The key here is to understand the following concepts: Filter ValueExtractor Aggregator Create the Test Class The first step is to create the test class that will show and test the various query operations, we&#8217;ll call this class QueryTests . We will use Junit 5 for this test, so the class does not have to be public. <markup lang=\"java\" >class QueryTests { } Bootstrap Coherence The first thing the test class will do is start Coherence using the bootstrap API introduced in Coherence 20.12 . As this is a JUnit test class, we can do this in a static @BeforeAll annotated setup method. We will also populate the cache with several countries and thus let&#8217;s create a small helper class CoherenceHelper : <markup lang=\"java\" >public static void startCoherence() { Coherence coherence = Coherence.clusterMember(); CompletableFuture&lt;Coherence&gt; future = coherence.start(); future.join(); Session session = coherence.getSession(); NamedCache&lt;String, Country&gt; countries = session.getCache(\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); countries.put(\"fr\", new Country(\"France\", \"Paris\", 67.4)); countries.put(\"ua\", new Country(\"Ukraine\", \"Kyiv\", 41.2)); countries.put(\"co\", new Country(\"Colombia\", \"Bogotá\", 50.4)); countries.put(\"au\", new Country(\"Australia\", \"Canberra\", 26)); } Obtain a default storage enabled cluster member Coherence instance. Start the Coherence instance, this wil start all the Coherence services. Block until Coherence instance has fully started before proceeding with the tests Obtain the default Session Get the countries cache Populate the countries cache with several new Country instances We are going to start a storage enabled cluster member using the most basic bootstrap API methods. For more details on the bootstrap API see the corresponding guide . <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { CoherenceHelper.startCoherence(); } Call CoherenceHelper and start the Coherence instance and populate the country data. Lastly, we create a static @AfterAll annotated tear-down method that will shut down Coherence at the end of the test. <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } We only created a single default Coherence instance, so we can obtain that instance with the Coherence.getInstance() method, and then close it. Now the basic framework of the test is in place we can add methods to show different querying operations. Filter To get started, we would like to retrieve all countries that have a population of more than 60 million people. For that we will use a Filter : <markup lang=\"java\" >@Test void testGreaterEqualsFilter() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); final Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results).hasSize(2); map.entrySet(filter).forEach(entry -&gt; { //assertThat(entry.getKey()).containcontainsAnyOf(\"de\", \"fr\"); assertThat(entry.getValue().getPopulation()).isGreaterThan(60.0); }); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the Map The result should be 2 countries only Assert that only France and Germany were selected ValueExtractor What if we don&#8217;t want to return Countries but just the collection of country names for which the population is 60 million people or higher? This is where we can use a ValueExtractor in combination with a ReducerAggregator . A value extractor is used to extract a property from a given object. In most instances developers would use the ReflectionExtractor as an implementation. The ReducerAggregator on the other hand, is used to run a ValueExtractor against cache entries, and it returns the extracted value. The result returned by the ReducerAggregator is a Map where the key is the key of the cache entry and the value is the extracted value. <markup lang=\"java\" >@Test void testValueExtractor() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ReducerAggregator&lt;String, Country, String, String&gt; aggregator = new ReducerAggregator&lt;&gt;(\"getName\"); Map&lt;String, String&gt; result = map.aggregate(filter, aggregator); result.forEach((key, value) -&gt; { assertThat(key).containsAnyOf(\"de\", \"fr\"); assertThat(value).containsAnyOf(\"Germany\", \"France\"); }); } Get the countries Map We create the same GreaterEqualsFilter as in the previous test (Select countries with more than 60 million people, only) Create a ReducerAggregator instance and specify that we only want the name of the countries returned Apply the Filter and Aggregator Verify that only the two country names France and Germany are returned as filtered values Aggregate Results What if we want to group queried data together? Let&#8217;s query for countries, where the population is greater than 60 million but instead of returning the countries, we will return the sum of the population of thsoe 2 countries instead. <markup lang=\"java\" >@Test void testAggregate() { NamedMap&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(\"getPopulation\"); BigDecimal result = map.aggregate(filter, aggregator); String resultAsString = result.setScale(2, RoundingMode.HALF_UP) .stripTrailingZeros() .toPlainString(); assertThat(resultAsString).isEqualTo(\"150.6\"); } Get the countries Map We create the same GreaterEqualsFilter as in the previous test (Select countries with more than 60 million people, only) We will use a different Aggregator . BigDecimalSum will aggregate the population and return a Bigecimal value Apply the Filter and Aggregator For assertion purposes we will convert the BigDecimal value to a String The generated String shall not have any trailing zeros Return the String Verify that the returned value is 150.6 To learn much more about built-in Aggregators, please take a look at the respective guide . Summary In this guide we showed how easy it is to query Coherence caches either using CohQL or programmatically using Filters, ValueExtractors and Aggregators. Please see the Coherence reference guide, specifically the chapter Querying Data In a Cache for more details. See Also Using Coherence Query Language Querying Data In a Cache ",
            "title": "Querying Caches"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A common use case in Coherence is for caches to hold data that comes from other sources, typically a database. Often there is a requirement to preload data into caches when an application starts up. Using a CacheLoader to load data on demand may be suitable for many caching use cases, but other use cases such as querying and aggregating caches require all the data to be present in the cache. Over the years, there have been a number of patters to achieve preloading, this guide will cover some currently recommended approaches to preloading data. Whilst this guide reads data from a database and pushes it into caches, the same patterns can apply to any data source, for example, preloading from files, messaging systems, data streaming systems, etc. ",
            "title": "Bulk Loading Caches"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The example in this guide builds a simple application that uses different techniques to preload caches from a database. This includes examples of preloading from a database into a cache that uses a cachestore to write cache data to the same database. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The cache preloader is typically implemented as a process that is run after the storage enabled cluster members have started. This is typically a simple Java class with a main method that runs and controls loading the data. As this application will need to communicate with the Coherence storage members, it can obviously run in two modes, either as a storage disabled cluster member or as an Extend client. As the preload application&#8217;s only job is to push a large amount of data into Coherence caches as fast as possible, it is recommended to run the preloader as a storage disabled cluster member. Running as an Extend client would cause a bottleneck trying to push all the data over a single Extend connection to the proxy, where it is then distributed across the cluster. ",
            "title": "Extend Client or Cluster Member?"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A simple preload task might look like the example below: <markup lang=\"java\" >public class CustomerPreloadTask implements Runnable { private final Connection connection; private final Session session; public PreloadTask(Connection connection, Session session) { this.connection = connection; this.session = session; } @Override public void run() { NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); String query = \"SELECT id, name, address, creditLimit FROM customers\"; int batchSize = 10; try (PreparedStatement statement = connection.prepareStatement(query); ResultSet resultSet = statement.executeQuery()) { Map&lt;Integer, Customer&gt; batch = new HashMap&lt;&gt;(batchSize); while (resultSet.next()) { int key = resultSet.getInt(\"id\"); Customer value = new Customer(resultSet.getInt(\"id\"), resultSet.getString(\"name\"), resultSet.getString(\"address\"), resultSet.getInt(\"creditLimit\")); batch.put(key, value); if (batch.size() &gt;= batchSize) { namedMap.putAll(batch); batch.clear(); } } if (!batch.isEmpty()) { namedMap.putAll(batch); batch.clear(); } } catch (SQLException e) { throw Exceptions.ensureRuntimeException(e); } } } Obtain the cache to be loaded from the Coherence Session , in this case the \"customers\" cache. Perform the query on the database to get the data to load. There are many ways this could be done depending on the database. This is a very simple approach using a PreparedStatement . Create a Map to hold a batch of entries to load. This preload task will call NamedMap.putAll() to load the dat ain batches into the cache. This is more efficient that multiple single put calls. Iterate over all the rows of data returned by the ResultSet . Create the cache key from the current row in the ResultSet (in this case the key is just the \"id\" int). Create the cache value from the current row in the ResultSet . Add the key and new Customer to the batch map. If the batch map is &gt;= the batch size, put the batch into the cache using putAll() , then clear the batch map. After all the rows in the ResultSet have been iterated over, there may be entries in the batch map that need to be loaded, so put them in the cache, A different implementation of this class can be created to load different caches from different database tables. Ideally the common code would be extracted into an abstract base class. This is what the example code does in the AbstractJdbcPreloadTask , which is a base class for loading caches from a database. Concrete implementations are in the example test code in the CustomerJdbcLoader and OrderJdbcLoader classes. ",
            "title": "A Simple Preload Runnable"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " If the cache loaders are written as Runnable instances, as shown above, they can easily be run in parallel using a Java Executor . For example, if there were three preload tasks, written like the example above, to load Customers, Orders, and Products, they could be submitted to an ExecutorService as shown below. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); The loader application can wait for the executor to complete all the tasks, ideally with a timeout so that if there is an issue, it does not run forever. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); executor.shutdown(); boolean terminated = executor.awaitTermination(1, TimeUnit.HOURS); if (!terminated) { executor.shutdownNow(); } Stop the executor accepting any more requests. Wait a maximum of one hour for the executor to complete running the tasks. If the executor has not terminated in one hour, forcefully terminate. ",
            "title": "Running the Loaders"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The basic code to load a simple cache from a database (or other data source) is usually very simple, as shown above. There are many variations on this pattern to make loading scale better and execute faster. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The typical preloading use case is to pull data from a datasource and load it into caches as fast as possible. This means ideally scaling out the preloading to be batched and multi-threaded, to load multiple caches at once. The pre-loaders in this example will be written as simple Java Runnable implementations. This allows them to be easily scaled up by running them in a Java Executor or daemon thread pool. A separate Coherence example will show how to use Coherence concurrent Executor Service module to scale out preloading by distributing the preload runnables to execute on Coherence cluster members. Extend Client or Cluster Member? The cache preloader is typically implemented as a process that is run after the storage enabled cluster members have started. This is typically a simple Java class with a main method that runs and controls loading the data. As this application will need to communicate with the Coherence storage members, it can obviously run in two modes, either as a storage disabled cluster member or as an Extend client. As the preload application&#8217;s only job is to push a large amount of data into Coherence caches as fast as possible, it is recommended to run the preloader as a storage disabled cluster member. Running as an Extend client would cause a bottleneck trying to push all the data over a single Extend connection to the proxy, where it is then distributed across the cluster. A Simple Preload Runnable A simple preload task might look like the example below: <markup lang=\"java\" >public class CustomerPreloadTask implements Runnable { private final Connection connection; private final Session session; public PreloadTask(Connection connection, Session session) { this.connection = connection; this.session = session; } @Override public void run() { NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); String query = \"SELECT id, name, address, creditLimit FROM customers\"; int batchSize = 10; try (PreparedStatement statement = connection.prepareStatement(query); ResultSet resultSet = statement.executeQuery()) { Map&lt;Integer, Customer&gt; batch = new HashMap&lt;&gt;(batchSize); while (resultSet.next()) { int key = resultSet.getInt(\"id\"); Customer value = new Customer(resultSet.getInt(\"id\"), resultSet.getString(\"name\"), resultSet.getString(\"address\"), resultSet.getInt(\"creditLimit\")); batch.put(key, value); if (batch.size() &gt;= batchSize) { namedMap.putAll(batch); batch.clear(); } } if (!batch.isEmpty()) { namedMap.putAll(batch); batch.clear(); } } catch (SQLException e) { throw Exceptions.ensureRuntimeException(e); } } } Obtain the cache to be loaded from the Coherence Session , in this case the \"customers\" cache. Perform the query on the database to get the data to load. There are many ways this could be done depending on the database. This is a very simple approach using a PreparedStatement . Create a Map to hold a batch of entries to load. This preload task will call NamedMap.putAll() to load the dat ain batches into the cache. This is more efficient that multiple single put calls. Iterate over all the rows of data returned by the ResultSet . Create the cache key from the current row in the ResultSet (in this case the key is just the \"id\" int). Create the cache value from the current row in the ResultSet . Add the key and new Customer to the batch map. If the batch map is &gt;= the batch size, put the batch into the cache using putAll() , then clear the batch map. After all the rows in the ResultSet have been iterated over, there may be entries in the batch map that need to be loaded, so put them in the cache, A different implementation of this class can be created to load different caches from different database tables. Ideally the common code would be extracted into an abstract base class. This is what the example code does in the AbstractJdbcPreloadTask , which is a base class for loading caches from a database. Concrete implementations are in the example test code in the CustomerJdbcLoader and OrderJdbcLoader classes. Running the Loaders If the cache loaders are written as Runnable instances, as shown above, they can easily be run in parallel using a Java Executor . For example, if there were three preload tasks, written like the example above, to load Customers, Orders, and Products, they could be submitted to an ExecutorService as shown below. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); The loader application can wait for the executor to complete all the tasks, ideally with a timeout so that if there is an issue, it does not run forever. <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); ExecutorService executor = Executors.newCachedThreadPool(); executor.submit(new CustomerPreloadTask(session)); executor.submit(new OrdersPreloadTask(session)); executor.submit(new ProductsPreloadTask(session)); executor.shutdown(); boolean terminated = executor.awaitTermination(1, TimeUnit.HOURS); if (!terminated) { executor.shutdownNow(); } Stop the executor accepting any more requests. Wait a maximum of one hour for the executor to complete running the tasks. If the executor has not terminated in one hour, forcefully terminate. Summary The basic code to load a simple cache from a database (or other data source) is usually very simple, as shown above. There are many variations on this pattern to make loading scale better and execute faster. ",
            "title": "Typical Preloading Use Case"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " In this example, a simple controller is used with a boolean field for enabling or disabling the CacheStore . The example source code contains the SimpleController class shown below: <markup lang=\"java\" title=\"SimpleController.java\" >public class SimpleController implements ControllableCacheStore.Controller { @Override public boolean isEnabled() { return enabled; } public void setEnabled(boolean enabled) { this.enabled = enabled; } } ",
            "title": "The Controller Implementation"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " For a cache to use a cache store, it needs to be configured in the cache configuration file to use a &lt;read-write-backing-map&gt; , which in turn configures the CacheStore implementation to use. There are a few ways to configure the CacheStore , either using the implementation class name directly, or using a factory class and static factory method. In this example we will use the second approach, this means determining the cache store to use will be done in a factory class rather than in configuration, but this fits our use case better. The &lt;distributed-scheme&gt; used in the example test code is shown below: <markup lang=\"xml\" title=\"controllable-cachestore-cache-config.xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;db-storage&lt;/scheme-name&gt; &lt;service-name&gt;StorageService&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-factory-name&gt; com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory &lt;/class-factory-name&gt; &lt;method-name&gt;createControllableCacheStore&lt;/method-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"jdbc.url\"&gt; jdbc:hsqldb:mem:test &lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; The CacheStore factory class is com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory The static factory method on the CacheStoreFactory class that will be called to create a CacheStore is createControllableCacheStore The createControllableCacheStore has two parameters, which are configured in the &lt;init-params&gt; , the first is the name of the cache. The Coherence configuration macro {cache-name} will pass the name of the cache being created to the factory method. The second parameter is the JDBC URL of the database to load data from, in the example this defaults to the HSQL in-memory test database. The CacheStoreFactory method createControllableCacheStore used in the example is shown below <markup lang=\"java\" title=\"filename.java\" >public static CacheStore createControllableCacheStore(String cacheName, String jdbcURL) throws Exception { CacheStore delegate; switch (cacheName.toLowerCase()) { case \"customers\": delegate = new CustomerCacheStore(jdbcURL); break; case \"orders\": delegate = new OrderCacheStore(jdbcURL); break; default: throw new IllegalArgumentException(\"Cannot create cache store for cache \" + cacheName); } return new ControllableCacheStore&lt;&gt;(new SimpleController(), delegate); } The code does a simple switch on the cache name to determine the actual CacheStore to create. If the cache name is \"customers\", then the CustomerCacheStore is created If the cache name is \"orders\", then the OrderCacheStore is created An exception is thrown for an unknown cache name Finally, a ControllableCacheStore is returned that uses a SimpleController and wraps the delegate CacheStore Now, when application code first requests either the \"customers\" cache or the \"orders\" cache, the cache Coherence will create the cache and call the CacheStoreFactory.createControllableCacheStore method to create the CacheStore . ",
            "title": "Configuring and Creating the CacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " Now we have caches that are configured to use the ControllableCacheStore with the SimpleController , the next step is to actually be able to enable or disable the controller so that the preload can run. In the SimpleController the setEnabled() method needs to be called to set the controlling boolean flag. For each cache configured to use the ControllableCacheStore there will be an instance of SimpleController in every storage enabled cluster member. A method is needed of calling the setEnabled() method on all these instances, and this can be done with an EntryProcessor . A section of the source for the SetEnabledProcessor is shown below (the methods for serialization have been omitted here, but are in the actual GitHub source). <markup lang=\"java\" title=\"SetEnabledProcessor.java\" >public static class SetEnabledProcessor&lt;K, V&gt; implements InvocableMap.EntryProcessor&lt;K, V, Void&gt;, PortableObject, ExternalizableLite { private boolean enabled; public SetEnabledProcessor() { } public SetEnabledProcessor(boolean enabled) { this.enabled = enabled; } @Override public Void process(InvocableMap.Entry&lt;K, V&gt; entry) { ObservableMap&lt;? extends K, ? extends V&gt; backingMap = entry.asBinaryEntry().getBackingMap(); if (backingMap instanceof ReadWriteBackingMap) { ReadWriteBackingMap.StoreWrapper wrapper = ((ReadWriteBackingMap) backingMap).getCacheStore(); Object o = wrapper.getStore(); if (o instanceof ControllableCacheStore) { ControllableCacheStore.Controller controller = ((ControllableCacheStore) o).getController(); if (controller instanceof SimpleController) { ((SimpleController) controller).setEnabled(enabled); } } } return null; } // PortableObject and ExternalizableLite method are omitted here... } In the process method, the backing map is obtained from the entry . The getBackingMap() is deprecated, mainly as a warning that this is quite a dangerous thing to do if you are not careful. The backing map is the internal structure used by Coherence to hold cache data, and directly manipulating it can have adverse effects. In this case we are not manipulating the backing map, so we are safe. If the cache is configured as with a &lt;read-write-backing-map&gt; then the implementation of the backing map here will be ReadWriteBackingMap . We can obtain the CacheStore being used from the ReadWriteBackingMap API, and check whether it is a ControllableCacheStore . If it is, we can get the Controller being used, and if it is a SimpleController set the flag. Now we have the SetEnabledProcessor we need to execute it so that we guarantee it runs on every storage enabled member. Using something like cache.invokeAll(Filters.always(), new SetEnabledProcessor()) will not work, because this will only execute on members where there are entries, and there are none as we are about to do a preload. One of the things to remember about methods like cache.invokeAll(keySet, new SetEnabledProcessor()) is that the keySet can contain keys that do not need to exist in the cache. As long as the entry processor does not call entry.setValue() the entry it executes against will never exist. Another feature of Coherence is the ability to influence the partition a key belongs to by writing a key class that implements the com.tangosol.net.partition.KeyPartitioningStrategy.PartitionAwareKey interface. Coherence has a built-in implementation of this class called com.tangosol.net.partition.SimplePartitionKey . We can make use of both these features to create a set of keys where we can guarantee we have one key for each partition in a cache. If we use this as the key set in an invokeAll method, we will guarantee to execute the EntryProcessor in every partition, and hence on every storage enable cluster member. The snippet of code below shows how to execute the SetEnabledProcessor to disable the cache stores for a cache. Changing the line new SetEnabledProcessor(false) to new SetEnabledProcessor(true) will instead enable the cache stores. <markup lang=\"java\" title=\"SimpleController.java\" >public static void disableCacheStores(NamedMap&lt;?, ?&gt; namedMap) { CacheService service = namedMap.getService(); int partitionCount = ((DistributedCacheService) service).getPartitionCount(); Set&lt;SimplePartitionKey&gt; keys = new HashSet&lt;&gt;(); for (int i = 0; i &lt; partitionCount; i++) { keys.add(SimplePartitionKey.getPartitionKey(i)); } namedMap.invokeAll(keys, new SetEnabledProcessor(false)); } Obtain the cache service for the NamedMap that has the ControllableCacheSTore to enable The cache service should actually be an instance of DistributedCacheService , from which we can get the partition count. The default is 257, but this could have been changed. Create a Set&lt;SimplePartitionKey&gt; that will hold the keys for the invokeAll In a for loop, create a SimplePartitionKey for every partition, and add it to the keys set The keys set can be used in the invokeAll call to invoke the SetEnabledProcessor on every partition Running the SetEnabledProcessor on every partition means it actually executes more times than it needs to, but this is not a problem, as repeated executions in the same JVM just set the same flag for the enabled value. Now we have a way to enable and disable the ControllableCacheStore , we can execute this code before running the preload, and then re-enable the cache stores after running the preload. ",
            "title": "Enabling and Disabling the ControllableCacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A controllable cache store is reasonably simple, but it will really not work in cases where the cache is configured to use write-behind. With write-behind enabled, if the controllable cache store is turned back on too soon after loading (i.e. within the write delay time) then the data loaded to the cache that is still in the write-behind queue will be written to the database. A controllable cache store is also not going to work in situations where the application could be updating entries in the cache while the preload is still running. If there are a mixture of entries, some needing to be written and some not, the controllable cache store will not be suitable. Another caveat with the SimpleController above, is what happens during failure of a storage member. If a storage member in the cluster fails, that is not an issue, but in environments such as Kubernetes, where that failed member will be replaced automatically, that can be a problem. The new member will join the cluster, caches will be created on it, including the ControllableCacheStore for configured caches. The problem is that the boolean flag in the new member&#8217;s SimpleController will not be set to false , so the new member will start storing entries it receives to the database. Ideally, new members do not join during preload, but this may be out of the developers control. This could require a more complex controller, for example checking an entry in a cache for its initial state, etc. ",
            "title": "ControllableCacheStore Caveats"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " As the name suggests, a controllable CacheStore can be turned on or off. The CacheStore would be turned off before the preload tasks ran, then turned back on after the preload is complete. There have been a few patterns for controllable cache stores suggested in the past, including an example in the official Coherence documentation where the enable/disable flag is a boolean value stored in another cache. With a little more thought we can see that this is not really a good idea. Consider what a bulk preload is doing, it is loading a very large amount of data into caches, that will then call the CacheStore methods. If the CacheStore needed to look up a distributed cache value on every store call, that would be massively inefficient. Even accessing a cache from inside a CacheStore could be problematic due to making a remote call from a cache service worker thread, which may cause a deadlock. Even introducing near caching or a view cache would not necessarily help, as updates to the flag would be async. Checking the flag that controls the CacheStore needs to be as efficient as possible. For that reason, the example here just uses a simple boolean field in the controller itself. An EntryProcessor is then used to directly set the flag for the controller. How this works will be explained below. The code in this example has a ControllableCacheStore class that implements CacheStore and has a Controller that enables or disables operations. This allows the ControllableCacheStore to be controlled in different ways just by implementing different types of Controller . The ControllableCacheStore also just delegates operations to another CacheStore , it does not do anything itself. The ControllableCacheStore calls the delegate if the controller says it is enabled, otherwise it does nothing. This makes the ControllableCacheStore a simple class that can be reused to make any existing, or new, CacheStore implementation be controllable. A small section of the ControllableCacheStore class is shown below: <markup lang=\"java\" title=\"ControllableCacheStore.java\" >public class ControllableCacheStore&lt;K, V&gt; implements CacheStore&lt;K, V&gt; { private final Controller controller; private final CacheStore&lt;K, V&gt; delegate; public ControllableCacheStore(Controller controller, CacheStore&lt;K, V&gt; delegate) { this.controller = controller; this.delegate = delegate; } @Override public void store(K key, V value) { if (controller.isEnabled()) { delegate.store(key, value); } } @Override public void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) { if (controller.isEnabled()) { delegate.storeAll(mapEntries); } } // other methods omitted ... /** * Implementations of {@link Controller} can * control a {@link ControllableCacheStore}. */ public interface Controller { boolean isEnabled(); } } It should be obvious how the class works. The Controller is an inner interface, and an implementation of this is passed to the constructor, along with the delegate CacheStore . Each method call (only store and storeAll are shown above) calls the controller&#8217;s isEnabled() method to determine whether the delegate should be called. The Controller Implementation In this example, a simple controller is used with a boolean field for enabling or disabling the CacheStore . The example source code contains the SimpleController class shown below: <markup lang=\"java\" title=\"SimpleController.java\" >public class SimpleController implements ControllableCacheStore.Controller { @Override public boolean isEnabled() { return enabled; } public void setEnabled(boolean enabled) { this.enabled = enabled; } } Configuring and Creating the CacheStore For a cache to use a cache store, it needs to be configured in the cache configuration file to use a &lt;read-write-backing-map&gt; , which in turn configures the CacheStore implementation to use. There are a few ways to configure the CacheStore , either using the implementation class name directly, or using a factory class and static factory method. In this example we will use the second approach, this means determining the cache store to use will be done in a factory class rather than in configuration, but this fits our use case better. The &lt;distributed-scheme&gt; used in the example test code is shown below: <markup lang=\"xml\" title=\"controllable-cachestore-cache-config.xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;db-storage&lt;/scheme-name&gt; &lt;service-name&gt;StorageService&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-factory-name&gt; com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory &lt;/class-factory-name&gt; &lt;method-name&gt;createControllableCacheStore&lt;/method-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"jdbc.url\"&gt; jdbc:hsqldb:mem:test &lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; The CacheStore factory class is com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory The static factory method on the CacheStoreFactory class that will be called to create a CacheStore is createControllableCacheStore The createControllableCacheStore has two parameters, which are configured in the &lt;init-params&gt; , the first is the name of the cache. The Coherence configuration macro {cache-name} will pass the name of the cache being created to the factory method. The second parameter is the JDBC URL of the database to load data from, in the example this defaults to the HSQL in-memory test database. The CacheStoreFactory method createControllableCacheStore used in the example is shown below <markup lang=\"java\" title=\"filename.java\" >public static CacheStore createControllableCacheStore(String cacheName, String jdbcURL) throws Exception { CacheStore delegate; switch (cacheName.toLowerCase()) { case \"customers\": delegate = new CustomerCacheStore(jdbcURL); break; case \"orders\": delegate = new OrderCacheStore(jdbcURL); break; default: throw new IllegalArgumentException(\"Cannot create cache store for cache \" + cacheName); } return new ControllableCacheStore&lt;&gt;(new SimpleController(), delegate); } The code does a simple switch on the cache name to determine the actual CacheStore to create. If the cache name is \"customers\", then the CustomerCacheStore is created If the cache name is \"orders\", then the OrderCacheStore is created An exception is thrown for an unknown cache name Finally, a ControllableCacheStore is returned that uses a SimpleController and wraps the delegate CacheStore Now, when application code first requests either the \"customers\" cache or the \"orders\" cache, the cache Coherence will create the cache and call the CacheStoreFactory.createControllableCacheStore method to create the CacheStore . Enabling and Disabling the ControllableCacheStore Now we have caches that are configured to use the ControllableCacheStore with the SimpleController , the next step is to actually be able to enable or disable the controller so that the preload can run. In the SimpleController the setEnabled() method needs to be called to set the controlling boolean flag. For each cache configured to use the ControllableCacheStore there will be an instance of SimpleController in every storage enabled cluster member. A method is needed of calling the setEnabled() method on all these instances, and this can be done with an EntryProcessor . A section of the source for the SetEnabledProcessor is shown below (the methods for serialization have been omitted here, but are in the actual GitHub source). <markup lang=\"java\" title=\"SetEnabledProcessor.java\" >public static class SetEnabledProcessor&lt;K, V&gt; implements InvocableMap.EntryProcessor&lt;K, V, Void&gt;, PortableObject, ExternalizableLite { private boolean enabled; public SetEnabledProcessor() { } public SetEnabledProcessor(boolean enabled) { this.enabled = enabled; } @Override public Void process(InvocableMap.Entry&lt;K, V&gt; entry) { ObservableMap&lt;? extends K, ? extends V&gt; backingMap = entry.asBinaryEntry().getBackingMap(); if (backingMap instanceof ReadWriteBackingMap) { ReadWriteBackingMap.StoreWrapper wrapper = ((ReadWriteBackingMap) backingMap).getCacheStore(); Object o = wrapper.getStore(); if (o instanceof ControllableCacheStore) { ControllableCacheStore.Controller controller = ((ControllableCacheStore) o).getController(); if (controller instanceof SimpleController) { ((SimpleController) controller).setEnabled(enabled); } } } return null; } // PortableObject and ExternalizableLite method are omitted here... } In the process method, the backing map is obtained from the entry . The getBackingMap() is deprecated, mainly as a warning that this is quite a dangerous thing to do if you are not careful. The backing map is the internal structure used by Coherence to hold cache data, and directly manipulating it can have adverse effects. In this case we are not manipulating the backing map, so we are safe. If the cache is configured as with a &lt;read-write-backing-map&gt; then the implementation of the backing map here will be ReadWriteBackingMap . We can obtain the CacheStore being used from the ReadWriteBackingMap API, and check whether it is a ControllableCacheStore . If it is, we can get the Controller being used, and if it is a SimpleController set the flag. Now we have the SetEnabledProcessor we need to execute it so that we guarantee it runs on every storage enabled member. Using something like cache.invokeAll(Filters.always(), new SetEnabledProcessor()) will not work, because this will only execute on members where there are entries, and there are none as we are about to do a preload. One of the things to remember about methods like cache.invokeAll(keySet, new SetEnabledProcessor()) is that the keySet can contain keys that do not need to exist in the cache. As long as the entry processor does not call entry.setValue() the entry it executes against will never exist. Another feature of Coherence is the ability to influence the partition a key belongs to by writing a key class that implements the com.tangosol.net.partition.KeyPartitioningStrategy.PartitionAwareKey interface. Coherence has a built-in implementation of this class called com.tangosol.net.partition.SimplePartitionKey . We can make use of both these features to create a set of keys where we can guarantee we have one key for each partition in a cache. If we use this as the key set in an invokeAll method, we will guarantee to execute the EntryProcessor in every partition, and hence on every storage enable cluster member. The snippet of code below shows how to execute the SetEnabledProcessor to disable the cache stores for a cache. Changing the line new SetEnabledProcessor(false) to new SetEnabledProcessor(true) will instead enable the cache stores. <markup lang=\"java\" title=\"SimpleController.java\" >public static void disableCacheStores(NamedMap&lt;?, ?&gt; namedMap) { CacheService service = namedMap.getService(); int partitionCount = ((DistributedCacheService) service).getPartitionCount(); Set&lt;SimplePartitionKey&gt; keys = new HashSet&lt;&gt;(); for (int i = 0; i &lt; partitionCount; i++) { keys.add(SimplePartitionKey.getPartitionKey(i)); } namedMap.invokeAll(keys, new SetEnabledProcessor(false)); } Obtain the cache service for the NamedMap that has the ControllableCacheSTore to enable The cache service should actually be an instance of DistributedCacheService , from which we can get the partition count. The default is 257, but this could have been changed. Create a Set&lt;SimplePartitionKey&gt; that will hold the keys for the invokeAll In a for loop, create a SimplePartitionKey for every partition, and add it to the keys set The keys set can be used in the invokeAll call to invoke the SetEnabledProcessor on every partition Running the SetEnabledProcessor on every partition means it actually executes more times than it needs to, but this is not a problem, as repeated executions in the same JVM just set the same flag for the enabled value. Now we have a way to enable and disable the ControllableCacheStore , we can execute this code before running the preload, and then re-enable the cache stores after running the preload. ControllableCacheStore Caveats A controllable cache store is reasonably simple, but it will really not work in cases where the cache is configured to use write-behind. With write-behind enabled, if the controllable cache store is turned back on too soon after loading (i.e. within the write delay time) then the data loaded to the cache that is still in the write-behind queue will be written to the database. A controllable cache store is also not going to work in situations where the application could be updating entries in the cache while the preload is still running. If there are a mixture of entries, some needing to be written and some not, the controllable cache store will not be suitable. Another caveat with the SimpleController above, is what happens during failure of a storage member. If a storage member in the cluster fails, that is not an issue, but in environments such as Kubernetes, where that failed member will be replaced automatically, that can be a problem. The new member will join the cluster, caches will be created on it, including the ControllableCacheStore for configured caches. The problem is that the boolean flag in the new member&#8217;s SimpleController will not be set to false , so the new member will start storing entries it receives to the database. Ideally, new members do not join during preload, but this may be out of the developers control. This could require a more complex controller, for example checking an entry in a cache for its initial state, etc. ",
            "title": "Controllable CacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A com.tangosol.util.Binary is usually used to hold a serialized binary value (occasionally it may also be a com.tangosol.io.ReadBuffer ). As well as the serialized data, that could have been serialized using any serializer configured in Coherence (Java, POF, json, etc) a Binary can be decorated with other information. Each decoration has an int identifier that is used to add, remove or obtain a specific decoration. Coherence itself uses decorations for a number of functions, so a number of decoration identifiers values are reserved. The identifiers are all stored in constants in com.tangosol.util.ExternalizableHelper class and all have the prefix DECO_ , for example DECO_EXPIRY . There are three identifiers reserved for use by application code, DECO_APP_1 , DECO_APP_2 and DECO_APP_3 which Coherence will not use, so for this example we can use ExternalizableHelper.DECO_APP_1 for the SmartCacheStore decoration. The method used to decorate a Binary is ExternalizableHelper method: <markup lang=\"java\" >public static Binary decorate(Binary bin, int nId, Binary binDecoration) A Binary is decorated with another Binary value. For the SmartCacheStore we do not care what the value of the decoration is, we only check whether it is present or not. Obviously we do not want to use a large Binary decoration, as this will add to the serialized size of the value, the smallest possible Binary is the constant value Binary.NO_BINARY , which is actually zero bytes, but still a Binary . We can therefore decorate a Binary for use in the SmartCacheStore like this: <markup lang=\"java\" >Binary binary = // create the serialized binary value Binary decorated = ExternalizableHelper.decorate(binary, ExternalizableHelper.DECO_APP_1, Binary.NO_BINARY); ",
            "title": "Decorating a Binary"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " The example below shows part of the SmartCacheStore implementation. The store and storeAll methods are shown, but the other BinaryEntryStore are omitted here to make things clearer. <markup lang=\"java\" title=\"SmartCacheStore.java\" >public class SmartCacheStore&lt;K, V&gt; implements BinaryEntryStore&lt;K, V&gt; { private final BinaryEntryStore&lt;K, V&gt; delegate; public SmartCacheStore(CacheStore&lt;K, V&gt; delegate) { this.delegate = new WrapperBinaryEntryStore&lt;&gt;(Objects.requireNonNull(delegate)); } @Override public void store(BinaryEntry&lt;K, V&gt; entry) { if (shouldStore(entry)) { delegate.store(entry); } } @Override public void storeAll(Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entries) { Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entriesToStore = entries.stream() .filter(this::shouldStore) .collect(Collectors.toSet()); if (entriesToStore.size() &gt; 0) { delegate.storeAll(entriesToStore); } } private boolean shouldStore(BinaryEntry&lt;K, V&gt; entry) { return !ExternalizableHelper.isDecorated(entry.getBinaryValue(), ExternalizableHelper.DECO_APP_1); } // other BinaryEntryStore omitted... } The delegate CacheStore passed to the constructor is wrapped in a WrapperBinaryEntryStore to make it look like a BinaryEntryStore that the SmartCacheStore can delegate to. The store method is passed the BinaryEntry to be stored. To check whether the delegate should be called, it calls the shouldStore method. If shouldStore returns true the delegate is called to store the entry. The storeAll method is similar to the store method, but is passed a Set of entries to store. The set is filtered to create a new Set containing only entries that should be stored. If there are any entries to store the delegate is called. The shouldStore method checks to see whether an entry should be stored. The Binary value is obtained from the BinaryEntry and checked to see whether the ExternalizableHelper.DECO_APP_1 decoration is present. ",
            "title": "The SmartCacheStore Implementation"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " Now we have a SmartCacheStore that only stores non-decorated values, how do we load decorated Binary values into a cache? This example only works on a cluster member, because it requires access to the server side cache service that the cache being loaded is using. This will allow the preloader to create serialized Binary keys and values using the correct serilzation method for the cache. Obtain a Binary NamedMap or NamedCache The usual way to obtain a NamedMap (or NamedCache ) from a Coherence Session is to just call session.getMap(cacheName); but the getMap() and getCache() method allow options to be passed in to control the cache returned. One of these is the WithClassLoader option, that takes a ClassLoader . Coherence has a special class loader obtained from com.tangosol.util.NullImplementation.getClassLoader() . If this is used, the cache returned is a binary cache, this means that values passed to methods such as get , put , putAll` etc., must be instances of Binary , i.e. a binary cache gives access to the actual serialized data in the cache. The code below gets the normal \"customers\" cache, with Integer keys and Customer values. <markup lang=\"java\" >NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); Whereas the code below gets the same \"customers\" cache but with Binary keys (serializer Integer instances in this example) and Binary values (serialized Customer instances in this example). <markup lang=\"java\" >NamedMap&lt;Binary, Binary&gt; namedMap = session.getMap(\"customers\", WithClassLoader.using(NullImplementation.getClassLoader())); Load Data to a Binary NamedMap or NamedCache Now we can obtain a binary cache to use to load decorated binary values, we can put everything together to load data from the data source, convert it to decorated binaty values, and call putAll . The method below from the AbstractBinaryJdbcPreloadTask class in the example source loads a batch of decorated values into a cache. The method is passed a batch of values to load (in a Map ), and a binary NamedMap to load the batch of data into, <markup lang=\"java\" title=\"AbstractBinaryJdbcPreloadTask.java\" >private void load(Map&lt;K, V&gt; batch, NamedMap&lt;Binary, Binary&gt; namedMap) { BackingMapManagerContext context = namedMap.getService().getBackingMapManager().getContext(); Converter&lt;K, Binary&gt; keyConverter = context.getKeyToInternalConverter(); Converter&lt;V, Binary&gt; valueConverter = context.getValueToInternalConverter(); Map&lt;Binary, Binary&gt; decoratedBatch = new HashMap&lt;&gt;(); for (Map.Entry&lt;K, V&gt; entry : batch.entrySet()) { Binary binary = valueConverter.convert(entry.getValue()); Binary decorated = ExternalizableHelper.decorate(binary, decorationId, Binary.NO_BINARY); decoratedBatch.put(keyConverter.convert(entry.getKey()), decorated); } namedMap.putAll(decoratedBatch); batch.clear(); } First the BackingMapManagerContext is obtained from the NamedMap . This will allow access to the converters to use to serialize the keys and values into Binary values. Obtain the key converter to serialize keys to Binary . Coherence uses different converters to serialize the key and value, because different logic is used internally to decorate a serialized key. If a key is converted to a Binary incorrectly it will not be possible to get the value back out again with something like a get() call. Obtain the converter to use to serialize values to Binary Create a Map to hold the Binary keys and value we will put into the cache, them iterate over the values in the batch. Convert the value to a Binary and add the decoration to it. Put the serialized key and decorated value into the decoratedBatch map After converting all th keys and value to Binary keys and decorated Binary values the map of binaries can be passed to the namedMap.putAll method. As all the data is already serialized, Coherence will send it unchanged to the storage enabled cluster members that own those entries. The SmartCacheStore works around the caveats of a ControllableCacheStore As the decoration on the value controls whether it is written, this method will work with write-behind. There is no need to turn on or off the cache stores If application code updates caches during the preload, those updated values will not be decorated and will be stored by the cache store If a new cluster member joins and becomes the owner of a number of entries, those entries will still have the decoration present and will not be written by the cache store. ",
            "title": "Loading Decorated Binary Values"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " A smart cache store is a solution to the caveats that a controllable cache store has. The code for a smart cache store and for preloading is slightly more complex, but it can be applied to more use cases. Basically a smart cache store can use some sort of flag on the cache value to determine whether an entry should be stored in the data store. This could be a boolean field in the cache value itself, but this means corrupting the application data model with Coherence control data. This is not ideal, and in fact some applications may not actually own the class files that are being used in the cache and cannot add fields. A better way that leaves the cache value unchanged is to use a feature in Coherence that allows decorations to be added to the serialized binary values in a cache. Coherence uses decorations itself for a number of reasons (for example marking an entry as being successfully stored). In this case we can add a simple decoration to indicate whether an entry should be stored or not, the actual value of the decoration does not matter, we can just use the presence of the decoration to indicate that the entry should not be written. The preloader would then load the cache with decorated values, which would not be stored by the smart cache store. Any other entries updated by the application would be stored, even if they were updated while the preload was running. A normal CacheStore does not have access to the binary values in the cache. To be able to do this, the cache store needs to be an implementation of com.tangosol.net.cache.BinaryEntryStore . The SmartCacheStore class in the example source code is an implementation of a BinaryEntryStore . Like the ControllableCacheStore in the example above, the SmartCacheStore wraps a delagate CacheStore so it can be used to make any CacheStore implementation smart. Decorating a Binary A com.tangosol.util.Binary is usually used to hold a serialized binary value (occasionally it may also be a com.tangosol.io.ReadBuffer ). As well as the serialized data, that could have been serialized using any serializer configured in Coherence (Java, POF, json, etc) a Binary can be decorated with other information. Each decoration has an int identifier that is used to add, remove or obtain a specific decoration. Coherence itself uses decorations for a number of functions, so a number of decoration identifiers values are reserved. The identifiers are all stored in constants in com.tangosol.util.ExternalizableHelper class and all have the prefix DECO_ , for example DECO_EXPIRY . There are three identifiers reserved for use by application code, DECO_APP_1 , DECO_APP_2 and DECO_APP_3 which Coherence will not use, so for this example we can use ExternalizableHelper.DECO_APP_1 for the SmartCacheStore decoration. The method used to decorate a Binary is ExternalizableHelper method: <markup lang=\"java\" >public static Binary decorate(Binary bin, int nId, Binary binDecoration) A Binary is decorated with another Binary value. For the SmartCacheStore we do not care what the value of the decoration is, we only check whether it is present or not. Obviously we do not want to use a large Binary decoration, as this will add to the serialized size of the value, the smallest possible Binary is the constant value Binary.NO_BINARY , which is actually zero bytes, but still a Binary . We can therefore decorate a Binary for use in the SmartCacheStore like this: <markup lang=\"java\" >Binary binary = // create the serialized binary value Binary decorated = ExternalizableHelper.decorate(binary, ExternalizableHelper.DECO_APP_1, Binary.NO_BINARY); The SmartCacheStore Implementation The example below shows part of the SmartCacheStore implementation. The store and storeAll methods are shown, but the other BinaryEntryStore are omitted here to make things clearer. <markup lang=\"java\" title=\"SmartCacheStore.java\" >public class SmartCacheStore&lt;K, V&gt; implements BinaryEntryStore&lt;K, V&gt; { private final BinaryEntryStore&lt;K, V&gt; delegate; public SmartCacheStore(CacheStore&lt;K, V&gt; delegate) { this.delegate = new WrapperBinaryEntryStore&lt;&gt;(Objects.requireNonNull(delegate)); } @Override public void store(BinaryEntry&lt;K, V&gt; entry) { if (shouldStore(entry)) { delegate.store(entry); } } @Override public void storeAll(Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entries) { Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entriesToStore = entries.stream() .filter(this::shouldStore) .collect(Collectors.toSet()); if (entriesToStore.size() &gt; 0) { delegate.storeAll(entriesToStore); } } private boolean shouldStore(BinaryEntry&lt;K, V&gt; entry) { return !ExternalizableHelper.isDecorated(entry.getBinaryValue(), ExternalizableHelper.DECO_APP_1); } // other BinaryEntryStore omitted... } The delegate CacheStore passed to the constructor is wrapped in a WrapperBinaryEntryStore to make it look like a BinaryEntryStore that the SmartCacheStore can delegate to. The store method is passed the BinaryEntry to be stored. To check whether the delegate should be called, it calls the shouldStore method. If shouldStore returns true the delegate is called to store the entry. The storeAll method is similar to the store method, but is passed a Set of entries to store. The set is filtered to create a new Set containing only entries that should be stored. If there are any entries to store the delegate is called. The shouldStore method checks to see whether an entry should be stored. The Binary value is obtained from the BinaryEntry and checked to see whether the ExternalizableHelper.DECO_APP_1 decoration is present. Loading Decorated Binary Values Now we have a SmartCacheStore that only stores non-decorated values, how do we load decorated Binary values into a cache? This example only works on a cluster member, because it requires access to the server side cache service that the cache being loaded is using. This will allow the preloader to create serialized Binary keys and values using the correct serilzation method for the cache. Obtain a Binary NamedMap or NamedCache The usual way to obtain a NamedMap (or NamedCache ) from a Coherence Session is to just call session.getMap(cacheName); but the getMap() and getCache() method allow options to be passed in to control the cache returned. One of these is the WithClassLoader option, that takes a ClassLoader . Coherence has a special class loader obtained from com.tangosol.util.NullImplementation.getClassLoader() . If this is used, the cache returned is a binary cache, this means that values passed to methods such as get , put , putAll` etc., must be instances of Binary , i.e. a binary cache gives access to the actual serialized data in the cache. The code below gets the normal \"customers\" cache, with Integer keys and Customer values. <markup lang=\"java\" >NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); Whereas the code below gets the same \"customers\" cache but with Binary keys (serializer Integer instances in this example) and Binary values (serialized Customer instances in this example). <markup lang=\"java\" >NamedMap&lt;Binary, Binary&gt; namedMap = session.getMap(\"customers\", WithClassLoader.using(NullImplementation.getClassLoader())); Load Data to a Binary NamedMap or NamedCache Now we can obtain a binary cache to use to load decorated binary values, we can put everything together to load data from the data source, convert it to decorated binaty values, and call putAll . The method below from the AbstractBinaryJdbcPreloadTask class in the example source loads a batch of decorated values into a cache. The method is passed a batch of values to load (in a Map ), and a binary NamedMap to load the batch of data into, <markup lang=\"java\" title=\"AbstractBinaryJdbcPreloadTask.java\" >private void load(Map&lt;K, V&gt; batch, NamedMap&lt;Binary, Binary&gt; namedMap) { BackingMapManagerContext context = namedMap.getService().getBackingMapManager().getContext(); Converter&lt;K, Binary&gt; keyConverter = context.getKeyToInternalConverter(); Converter&lt;V, Binary&gt; valueConverter = context.getValueToInternalConverter(); Map&lt;Binary, Binary&gt; decoratedBatch = new HashMap&lt;&gt;(); for (Map.Entry&lt;K, V&gt; entry : batch.entrySet()) { Binary binary = valueConverter.convert(entry.getValue()); Binary decorated = ExternalizableHelper.decorate(binary, decorationId, Binary.NO_BINARY); decoratedBatch.put(keyConverter.convert(entry.getKey()), decorated); } namedMap.putAll(decoratedBatch); batch.clear(); } First the BackingMapManagerContext is obtained from the NamedMap . This will allow access to the converters to use to serialize the keys and values into Binary values. Obtain the key converter to serialize keys to Binary . Coherence uses different converters to serialize the key and value, because different logic is used internally to decorate a serialized key. If a key is converted to a Binary incorrectly it will not be possible to get the value back out again with something like a get() call. Obtain the converter to use to serialize values to Binary Create a Map to hold the Binary keys and value we will put into the cache, them iterate over the values in the batch. Convert the value to a Binary and add the decoration to it. Put the serialized key and decorated value into the decoratedBatch map After converting all th keys and value to Binary keys and decorated Binary values the map of binaries can be passed to the namedMap.putAll method. As all the data is already serialized, Coherence will send it unchanged to the storage enabled cluster members that own those entries. The SmartCacheStore works around the caveats of a ControllableCacheStore As the decoration on the value controls whether it is written, this method will work with write-behind. There is no need to turn on or off the cache stores If application code updates caches during the preload, those updated values will not be decorated and will be stored by the cache store If a new cluster member joins and becomes the owner of a number of entries, those entries will still have the decoration present and will not be written by the cache store. ",
            "title": "Smart CacheStore"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " This guide has shown a few solutions to the preload use case. Which one is suitable depends on your applications requirements. The example code has been built in such a way that it can be taken as a starting framework for a preloader and controllable/smart cache stores and expanded to suit application use cases. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/195-bulk-loading-caches/README",
            "text": " As already shown, a basic cache loader can be very simple. Where it gets complicated is when the cache being loaded has a CacheStore configured that writes data to the same data source that the loader is loading from. It should be obvious what the issue is, the data is read from the database, put into the cache, and the re-written back to the database - this is not desirable. The solution to this problem is to have a CacheStore implementation that can determine whether an entry should be written to the database or not. There are a few ways to do this, a controllable CacheStore that can be turned on or off, or a CacheStore that can check a flag on a value to determine whether that value should be stored or not. In this example we will cover both of these options. Controllable CacheStore As the name suggests, a controllable CacheStore can be turned on or off. The CacheStore would be turned off before the preload tasks ran, then turned back on after the preload is complete. There have been a few patterns for controllable cache stores suggested in the past, including an example in the official Coherence documentation where the enable/disable flag is a boolean value stored in another cache. With a little more thought we can see that this is not really a good idea. Consider what a bulk preload is doing, it is loading a very large amount of data into caches, that will then call the CacheStore methods. If the CacheStore needed to look up a distributed cache value on every store call, that would be massively inefficient. Even accessing a cache from inside a CacheStore could be problematic due to making a remote call from a cache service worker thread, which may cause a deadlock. Even introducing near caching or a view cache would not necessarily help, as updates to the flag would be async. Checking the flag that controls the CacheStore needs to be as efficient as possible. For that reason, the example here just uses a simple boolean field in the controller itself. An EntryProcessor is then used to directly set the flag for the controller. How this works will be explained below. The code in this example has a ControllableCacheStore class that implements CacheStore and has a Controller that enables or disables operations. This allows the ControllableCacheStore to be controlled in different ways just by implementing different types of Controller . The ControllableCacheStore also just delegates operations to another CacheStore , it does not do anything itself. The ControllableCacheStore calls the delegate if the controller says it is enabled, otherwise it does nothing. This makes the ControllableCacheStore a simple class that can be reused to make any existing, or new, CacheStore implementation be controllable. A small section of the ControllableCacheStore class is shown below: <markup lang=\"java\" title=\"ControllableCacheStore.java\" >public class ControllableCacheStore&lt;K, V&gt; implements CacheStore&lt;K, V&gt; { private final Controller controller; private final CacheStore&lt;K, V&gt; delegate; public ControllableCacheStore(Controller controller, CacheStore&lt;K, V&gt; delegate) { this.controller = controller; this.delegate = delegate; } @Override public void store(K key, V value) { if (controller.isEnabled()) { delegate.store(key, value); } } @Override public void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) { if (controller.isEnabled()) { delegate.storeAll(mapEntries); } } // other methods omitted ... /** * Implementations of {@link Controller} can * control a {@link ControllableCacheStore}. */ public interface Controller { boolean isEnabled(); } } It should be obvious how the class works. The Controller is an inner interface, and an implementation of this is passed to the constructor, along with the delegate CacheStore . Each method call (only store and storeAll are shown above) calls the controller&#8217;s isEnabled() method to determine whether the delegate should be called. The Controller Implementation In this example, a simple controller is used with a boolean field for enabling or disabling the CacheStore . The example source code contains the SimpleController class shown below: <markup lang=\"java\" title=\"SimpleController.java\" >public class SimpleController implements ControllableCacheStore.Controller { @Override public boolean isEnabled() { return enabled; } public void setEnabled(boolean enabled) { this.enabled = enabled; } } Configuring and Creating the CacheStore For a cache to use a cache store, it needs to be configured in the cache configuration file to use a &lt;read-write-backing-map&gt; , which in turn configures the CacheStore implementation to use. There are a few ways to configure the CacheStore , either using the implementation class name directly, or using a factory class and static factory method. In this example we will use the second approach, this means determining the cache store to use will be done in a factory class rather than in configuration, but this fits our use case better. The &lt;distributed-scheme&gt; used in the example test code is shown below: <markup lang=\"xml\" title=\"controllable-cachestore-cache-config.xml\" > &lt;distributed-scheme&gt; &lt;scheme-name&gt;db-storage&lt;/scheme-name&gt; &lt;service-name&gt;StorageService&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-factory-name&gt; com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory &lt;/class-factory-name&gt; &lt;method-name&gt;createControllableCacheStore&lt;/method-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"jdbc.url\"&gt; jdbc:hsqldb:mem:test &lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; The CacheStore factory class is com.oracle.coherence.guides.preload.cachestore.CacheStoreFactory The static factory method on the CacheStoreFactory class that will be called to create a CacheStore is createControllableCacheStore The createControllableCacheStore has two parameters, which are configured in the &lt;init-params&gt; , the first is the name of the cache. The Coherence configuration macro {cache-name} will pass the name of the cache being created to the factory method. The second parameter is the JDBC URL of the database to load data from, in the example this defaults to the HSQL in-memory test database. The CacheStoreFactory method createControllableCacheStore used in the example is shown below <markup lang=\"java\" title=\"filename.java\" >public static CacheStore createControllableCacheStore(String cacheName, String jdbcURL) throws Exception { CacheStore delegate; switch (cacheName.toLowerCase()) { case \"customers\": delegate = new CustomerCacheStore(jdbcURL); break; case \"orders\": delegate = new OrderCacheStore(jdbcURL); break; default: throw new IllegalArgumentException(\"Cannot create cache store for cache \" + cacheName); } return new ControllableCacheStore&lt;&gt;(new SimpleController(), delegate); } The code does a simple switch on the cache name to determine the actual CacheStore to create. If the cache name is \"customers\", then the CustomerCacheStore is created If the cache name is \"orders\", then the OrderCacheStore is created An exception is thrown for an unknown cache name Finally, a ControllableCacheStore is returned that uses a SimpleController and wraps the delegate CacheStore Now, when application code first requests either the \"customers\" cache or the \"orders\" cache, the cache Coherence will create the cache and call the CacheStoreFactory.createControllableCacheStore method to create the CacheStore . Enabling and Disabling the ControllableCacheStore Now we have caches that are configured to use the ControllableCacheStore with the SimpleController , the next step is to actually be able to enable or disable the controller so that the preload can run. In the SimpleController the setEnabled() method needs to be called to set the controlling boolean flag. For each cache configured to use the ControllableCacheStore there will be an instance of SimpleController in every storage enabled cluster member. A method is needed of calling the setEnabled() method on all these instances, and this can be done with an EntryProcessor . A section of the source for the SetEnabledProcessor is shown below (the methods for serialization have been omitted here, but are in the actual GitHub source). <markup lang=\"java\" title=\"SetEnabledProcessor.java\" >public static class SetEnabledProcessor&lt;K, V&gt; implements InvocableMap.EntryProcessor&lt;K, V, Void&gt;, PortableObject, ExternalizableLite { private boolean enabled; public SetEnabledProcessor() { } public SetEnabledProcessor(boolean enabled) { this.enabled = enabled; } @Override public Void process(InvocableMap.Entry&lt;K, V&gt; entry) { ObservableMap&lt;? extends K, ? extends V&gt; backingMap = entry.asBinaryEntry().getBackingMap(); if (backingMap instanceof ReadWriteBackingMap) { ReadWriteBackingMap.StoreWrapper wrapper = ((ReadWriteBackingMap) backingMap).getCacheStore(); Object o = wrapper.getStore(); if (o instanceof ControllableCacheStore) { ControllableCacheStore.Controller controller = ((ControllableCacheStore) o).getController(); if (controller instanceof SimpleController) { ((SimpleController) controller).setEnabled(enabled); } } } return null; } // PortableObject and ExternalizableLite method are omitted here... } In the process method, the backing map is obtained from the entry . The getBackingMap() is deprecated, mainly as a warning that this is quite a dangerous thing to do if you are not careful. The backing map is the internal structure used by Coherence to hold cache data, and directly manipulating it can have adverse effects. In this case we are not manipulating the backing map, so we are safe. If the cache is configured as with a &lt;read-write-backing-map&gt; then the implementation of the backing map here will be ReadWriteBackingMap . We can obtain the CacheStore being used from the ReadWriteBackingMap API, and check whether it is a ControllableCacheStore . If it is, we can get the Controller being used, and if it is a SimpleController set the flag. Now we have the SetEnabledProcessor we need to execute it so that we guarantee it runs on every storage enabled member. Using something like cache.invokeAll(Filters.always(), new SetEnabledProcessor()) will not work, because this will only execute on members where there are entries, and there are none as we are about to do a preload. One of the things to remember about methods like cache.invokeAll(keySet, new SetEnabledProcessor()) is that the keySet can contain keys that do not need to exist in the cache. As long as the entry processor does not call entry.setValue() the entry it executes against will never exist. Another feature of Coherence is the ability to influence the partition a key belongs to by writing a key class that implements the com.tangosol.net.partition.KeyPartitioningStrategy.PartitionAwareKey interface. Coherence has a built-in implementation of this class called com.tangosol.net.partition.SimplePartitionKey . We can make use of both these features to create a set of keys where we can guarantee we have one key for each partition in a cache. If we use this as the key set in an invokeAll method, we will guarantee to execute the EntryProcessor in every partition, and hence on every storage enable cluster member. The snippet of code below shows how to execute the SetEnabledProcessor to disable the cache stores for a cache. Changing the line new SetEnabledProcessor(false) to new SetEnabledProcessor(true) will instead enable the cache stores. <markup lang=\"java\" title=\"SimpleController.java\" >public static void disableCacheStores(NamedMap&lt;?, ?&gt; namedMap) { CacheService service = namedMap.getService(); int partitionCount = ((DistributedCacheService) service).getPartitionCount(); Set&lt;SimplePartitionKey&gt; keys = new HashSet&lt;&gt;(); for (int i = 0; i &lt; partitionCount; i++) { keys.add(SimplePartitionKey.getPartitionKey(i)); } namedMap.invokeAll(keys, new SetEnabledProcessor(false)); } Obtain the cache service for the NamedMap that has the ControllableCacheSTore to enable The cache service should actually be an instance of DistributedCacheService , from which we can get the partition count. The default is 257, but this could have been changed. Create a Set&lt;SimplePartitionKey&gt; that will hold the keys for the invokeAll In a for loop, create a SimplePartitionKey for every partition, and add it to the keys set The keys set can be used in the invokeAll call to invoke the SetEnabledProcessor on every partition Running the SetEnabledProcessor on every partition means it actually executes more times than it needs to, but this is not a problem, as repeated executions in the same JVM just set the same flag for the enabled value. Now we have a way to enable and disable the ControllableCacheStore , we can execute this code before running the preload, and then re-enable the cache stores after running the preload. ControllableCacheStore Caveats A controllable cache store is reasonably simple, but it will really not work in cases where the cache is configured to use write-behind. With write-behind enabled, if the controllable cache store is turned back on too soon after loading (i.e. within the write delay time) then the data loaded to the cache that is still in the write-behind queue will be written to the database. A controllable cache store is also not going to work in situations where the application could be updating entries in the cache while the preload is still running. If there are a mixture of entries, some needing to be written and some not, the controllable cache store will not be suitable. Another caveat with the SimpleController above, is what happens during failure of a storage member. If a storage member in the cluster fails, that is not an issue, but in environments such as Kubernetes, where that failed member will be replaced automatically, that can be a problem. The new member will join the cluster, caches will be created on it, including the ControllableCacheStore for configured caches. The problem is that the boolean flag in the new member&#8217;s SimpleController will not be set to false , so the new member will start storing entries it receives to the database. Ideally, new members do not join during preload, but this may be out of the developers control. This could require a more complex controller, for example checking an entry in a cache for its initial state, etc. Smart CacheStore A smart cache store is a solution to the caveats that a controllable cache store has. The code for a smart cache store and for preloading is slightly more complex, but it can be applied to more use cases. Basically a smart cache store can use some sort of flag on the cache value to determine whether an entry should be stored in the data store. This could be a boolean field in the cache value itself, but this means corrupting the application data model with Coherence control data. This is not ideal, and in fact some applications may not actually own the class files that are being used in the cache and cannot add fields. A better way that leaves the cache value unchanged is to use a feature in Coherence that allows decorations to be added to the serialized binary values in a cache. Coherence uses decorations itself for a number of reasons (for example marking an entry as being successfully stored). In this case we can add a simple decoration to indicate whether an entry should be stored or not, the actual value of the decoration does not matter, we can just use the presence of the decoration to indicate that the entry should not be written. The preloader would then load the cache with decorated values, which would not be stored by the smart cache store. Any other entries updated by the application would be stored, even if they were updated while the preload was running. A normal CacheStore does not have access to the binary values in the cache. To be able to do this, the cache store needs to be an implementation of com.tangosol.net.cache.BinaryEntryStore . The SmartCacheStore class in the example source code is an implementation of a BinaryEntryStore . Like the ControllableCacheStore in the example above, the SmartCacheStore wraps a delagate CacheStore so it can be used to make any CacheStore implementation smart. Decorating a Binary A com.tangosol.util.Binary is usually used to hold a serialized binary value (occasionally it may also be a com.tangosol.io.ReadBuffer ). As well as the serialized data, that could have been serialized using any serializer configured in Coherence (Java, POF, json, etc) a Binary can be decorated with other information. Each decoration has an int identifier that is used to add, remove or obtain a specific decoration. Coherence itself uses decorations for a number of functions, so a number of decoration identifiers values are reserved. The identifiers are all stored in constants in com.tangosol.util.ExternalizableHelper class and all have the prefix DECO_ , for example DECO_EXPIRY . There are three identifiers reserved for use by application code, DECO_APP_1 , DECO_APP_2 and DECO_APP_3 which Coherence will not use, so for this example we can use ExternalizableHelper.DECO_APP_1 for the SmartCacheStore decoration. The method used to decorate a Binary is ExternalizableHelper method: <markup lang=\"java\" >public static Binary decorate(Binary bin, int nId, Binary binDecoration) A Binary is decorated with another Binary value. For the SmartCacheStore we do not care what the value of the decoration is, we only check whether it is present or not. Obviously we do not want to use a large Binary decoration, as this will add to the serialized size of the value, the smallest possible Binary is the constant value Binary.NO_BINARY , which is actually zero bytes, but still a Binary . We can therefore decorate a Binary for use in the SmartCacheStore like this: <markup lang=\"java\" >Binary binary = // create the serialized binary value Binary decorated = ExternalizableHelper.decorate(binary, ExternalizableHelper.DECO_APP_1, Binary.NO_BINARY); The SmartCacheStore Implementation The example below shows part of the SmartCacheStore implementation. The store and storeAll methods are shown, but the other BinaryEntryStore are omitted here to make things clearer. <markup lang=\"java\" title=\"SmartCacheStore.java\" >public class SmartCacheStore&lt;K, V&gt; implements BinaryEntryStore&lt;K, V&gt; { private final BinaryEntryStore&lt;K, V&gt; delegate; public SmartCacheStore(CacheStore&lt;K, V&gt; delegate) { this.delegate = new WrapperBinaryEntryStore&lt;&gt;(Objects.requireNonNull(delegate)); } @Override public void store(BinaryEntry&lt;K, V&gt; entry) { if (shouldStore(entry)) { delegate.store(entry); } } @Override public void storeAll(Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entries) { Set&lt;? extends BinaryEntry&lt;K, V&gt;&gt; entriesToStore = entries.stream() .filter(this::shouldStore) .collect(Collectors.toSet()); if (entriesToStore.size() &gt; 0) { delegate.storeAll(entriesToStore); } } private boolean shouldStore(BinaryEntry&lt;K, V&gt; entry) { return !ExternalizableHelper.isDecorated(entry.getBinaryValue(), ExternalizableHelper.DECO_APP_1); } // other BinaryEntryStore omitted... } The delegate CacheStore passed to the constructor is wrapped in a WrapperBinaryEntryStore to make it look like a BinaryEntryStore that the SmartCacheStore can delegate to. The store method is passed the BinaryEntry to be stored. To check whether the delegate should be called, it calls the shouldStore method. If shouldStore returns true the delegate is called to store the entry. The storeAll method is similar to the store method, but is passed a Set of entries to store. The set is filtered to create a new Set containing only entries that should be stored. If there are any entries to store the delegate is called. The shouldStore method checks to see whether an entry should be stored. The Binary value is obtained from the BinaryEntry and checked to see whether the ExternalizableHelper.DECO_APP_1 decoration is present. Loading Decorated Binary Values Now we have a SmartCacheStore that only stores non-decorated values, how do we load decorated Binary values into a cache? This example only works on a cluster member, because it requires access to the server side cache service that the cache being loaded is using. This will allow the preloader to create serialized Binary keys and values using the correct serilzation method for the cache. Obtain a Binary NamedMap or NamedCache The usual way to obtain a NamedMap (or NamedCache ) from a Coherence Session is to just call session.getMap(cacheName); but the getMap() and getCache() method allow options to be passed in to control the cache returned. One of these is the WithClassLoader option, that takes a ClassLoader . Coherence has a special class loader obtained from com.tangosol.util.NullImplementation.getClassLoader() . If this is used, the cache returned is a binary cache, this means that values passed to methods such as get , put , putAll` etc., must be instances of Binary , i.e. a binary cache gives access to the actual serialized data in the cache. The code below gets the normal \"customers\" cache, with Integer keys and Customer values. <markup lang=\"java\" >NamedMap&lt;Integer, Customer&gt; namedMap = session.getMap(\"customers\"); Whereas the code below gets the same \"customers\" cache but with Binary keys (serializer Integer instances in this example) and Binary values (serialized Customer instances in this example). <markup lang=\"java\" >NamedMap&lt;Binary, Binary&gt; namedMap = session.getMap(\"customers\", WithClassLoader.using(NullImplementation.getClassLoader())); Load Data to a Binary NamedMap or NamedCache Now we can obtain a binary cache to use to load decorated binary values, we can put everything together to load data from the data source, convert it to decorated binaty values, and call putAll . The method below from the AbstractBinaryJdbcPreloadTask class in the example source loads a batch of decorated values into a cache. The method is passed a batch of values to load (in a Map ), and a binary NamedMap to load the batch of data into, <markup lang=\"java\" title=\"AbstractBinaryJdbcPreloadTask.java\" >private void load(Map&lt;K, V&gt; batch, NamedMap&lt;Binary, Binary&gt; namedMap) { BackingMapManagerContext context = namedMap.getService().getBackingMapManager().getContext(); Converter&lt;K, Binary&gt; keyConverter = context.getKeyToInternalConverter(); Converter&lt;V, Binary&gt; valueConverter = context.getValueToInternalConverter(); Map&lt;Binary, Binary&gt; decoratedBatch = new HashMap&lt;&gt;(); for (Map.Entry&lt;K, V&gt; entry : batch.entrySet()) { Binary binary = valueConverter.convert(entry.getValue()); Binary decorated = ExternalizableHelper.decorate(binary, decorationId, Binary.NO_BINARY); decoratedBatch.put(keyConverter.convert(entry.getKey()), decorated); } namedMap.putAll(decoratedBatch); batch.clear(); } First the BackingMapManagerContext is obtained from the NamedMap . This will allow access to the converters to use to serialize the keys and values into Binary values. Obtain the key converter to serialize keys to Binary . Coherence uses different converters to serialize the key and value, because different logic is used internally to decorate a serialized key. If a key is converted to a Binary incorrectly it will not be possible to get the value back out again with something like a get() call. Obtain the converter to use to serialize values to Binary Create a Map to hold the Binary keys and value we will put into the cache, them iterate over the values in the batch. Convert the value to a Binary and add the decoration to it. Put the serialized key and decorated value into the decoratedBatch map After converting all th keys and value to Binary keys and decorated Binary values the map of binaries can be passed to the namedMap.putAll method. As all the data is already serialized, Coherence will send it unchanged to the storage enabled cluster members that own those entries. The SmartCacheStore works around the caveats of a ControllableCacheStore As the decoration on the value controls whether it is written, this method will work with write-behind. There is no need to turn on or off the cache stores If application code updates caches during the preload, those updated values will not be decorated and will be stored by the cache store If a new cluster member joins and becomes the owner of a number of entries, those entries will still have the decoration present and will not be written by the cache store. Summary This guide has shown a few solutions to the preload use case. Which one is suitable depends on your applications requirements. The example code has been built in such a way that it can be taken as a starting framework for a preloader and controllable/smart cache stores and expanded to suit application use cases. ",
            "title": "CacheStore Complications"
        },
        {
            "location": "/coherence-mp/README",
            "text": " Coherence provides a number of additional modules that provide support for different Microprofile APIs. Microprofile Config Using Coherence as a Microprofile config source. Microprofile Metrics Configure Coherence to publish metrics via the Microprofile metrics API. ",
            "title": "Coherence MP"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " What You Will Build What You Need Building The Example Code Example Data Model Why use Coherence*Extend? Connect via the Name Service Using Proxy Load Balancing Setting Host and Port Explicitly Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The example code is written as a set of JUnit integration tests, showing how you can use Coherence*Extend. For our test cases we will also use Oracle Bedrock to start server instances of Oracle Coherence for testing purposes. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Although recommended, it may not always be possible that your application can be directly part of a Coherence cluster using the Tangosol Cluster Management Protocol (TCMP). For example, your application may be located in a different network, you need to access Oracle Coherence from desktop applications, or you need to use languages other than Java, e.g. C++ or .NET. Another alternative is to use the gRPC integration . ",
            "title": "Why use Coherence*Extend?"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Running the test should be fairly uneventful. If successful, you will see Bedrock starting up the Coherence server with 1 instance followed by the client starting up and connecting. Let&#8217;s do a quick test of the request-timeout and see what happens when the Coherence Server is not available. Comment out the setup() method, and re-run the test. After the specified request-timeout of 5 seconds, you should get a stacktrace with an exception similar to the following: <markup lang=\"bash\" >com.tangosol.net.messaging.ConnectionException: Unable to locate cluster 'myCluster' while looking for its ProxyService 'MyCountryExtendService' In the next section we will see how we can use multiple Coherence servers, and thus taking advantage of proxy load-balancing. ",
            "title": "Run the Test"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " When connecting to a Coherence Cluster via Coherence*Extend, we recommend the use of the Name Service. The use of the name service simplifies port management as the name service will look up the actual Coherence*Extend ports. That way Coherence*Extend ports can be ephemeral. For this example, let&#8217;s start with the Server Cache Configuration file at src/main/resources/name-service/server-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In the &lt;cache-mapping&gt; element, we state that the countries cache maps to the country-scheme The country-scheme then declares the &lt;proxy-scheme&gt; with the name MyCountryExtendService The MyCountryExtendService will start automatically The MyCountryExtendService will be registered with the default name service. If you wanted to customize that behavior, you would need to provide an &lt;acceptor-config&gt; element. See the load-balancing use-case below for details. We will also create a corresponding Client Cache Configuration file at src/main/resources/name-service/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The cache mapping for the client will look similar to the server one, but we name the scheme remote-country-scheme The client specifies a &lt;remote-cache-scheme&gt; element The service name MyCountryExtendService must match the name we use in the server cache configuration file We also define a request-timeout of 5 seconds. This means that if a connection cannot be established within that time, an exception is raised The client will be using the default name service port of 7574 to lookup the proxy endpoint for the MyCountryExtendService . You could customize that configuration by providing an &lt;initiator-config&gt; element. See the firewall example below for details. In the test case itself, we will use Oracle Bedrock to boostrap the Coherence server using the Server Cache Configuration file: <markup lang=\"java\" >static CoherenceClusterMember server; @BeforeAll static void setup() { final LocalPlatform platform = LocalPlatform.get(); // Start the Coherence server server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"name-service/server-coherence-cache-config.xml\"), IPv4Preferred.yes(), SystemProperty.of(\"coherence.wka\", \"127.0.0.1\"), ClusterName.of(\"myCluster\"), DisplayName.of(\"server\")); // Wait for Coherence to start Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); } Specify the server cache configuration file Give the Server Cluster an explicit name myCluster Make sure that we wait until the MyCountryExtendService proxy service is available Then we configure and start the Coherence client. <markup lang=\"java\" >@Test void testNameServiceUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.NAME_SERVICE_INSTANCE_NAME, \"name-service/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.NAME_SERVICE_INSTANCE_NAME,\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Disable TCMP to ensure that we only connect via Coherence*Extend Set the cluster name of the client to the same name as the server Specify the client cache configuration file Get the NamedCache and add a new country Important When configuring your Coherence*Extend client, it is important that your client&#8217;s Cluster Name match the name of the Coherence Server Cluster. Tip Java-based clients located on the same network as the Coherence server should disable TCMP communication in order to ensure that the client connect to clustered services exclusively using extend proxies. This can be achieved by setting System property coherence.tcmp.enabled to false . Please see the reference documentation for more detailed information. Run the Test Running the test should be fairly uneventful. If successful, you will see Bedrock starting up the Coherence server with 1 instance followed by the client starting up and connecting. Let&#8217;s do a quick test of the request-timeout and see what happens when the Coherence Server is not available. Comment out the setup() method, and re-run the test. After the specified request-timeout of 5 seconds, you should get a stacktrace with an exception similar to the following: <markup lang=\"bash\" >com.tangosol.net.messaging.ConnectionException: Unable to locate cluster 'myCluster' while looking for its ProxyService 'MyCountryExtendService' In the next section we will see how we can use multiple Coherence servers, and thus taking advantage of proxy load-balancing. ",
            "title": "Connect via the Name Service"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " When running the test you should notice the logging to the console from our CustomProxyServiceLoadBalancer : <markup lang=\"bash\" >[server-1:out:44488] 2: Local Member Id: 1 (Total # of Members: 4) - Connection Count: 1 [server-3:out:44488] 2: Local Member Id: 4 (Total # of Members: 4) - Connection Count: 1 [server-2:out:44488] 2: Local Member Id: 2 (Total # of Members: 4) - Connection Count: 1 [server-4:out:44488] 2: Local Member Id: 3 (Total # of Members: 4) - Connection Count: 0 As we have 4 Cluster Servers but only 3 clients, 1 Cluster Server will have 0 client connections, while each other server has 1 client connection each. ",
            "title": "Run the Test"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " When you have multiple Coherence servers that you are connecting to via Coherence*Extend, connection load-balancing is automatically applied. The default load-balancing behavior is based on the load of each Coherence server member and client connections are evenly spread across the Coherence cluster. The default load balance algorithm is called ‘proxy’, which if you were to explicitly configure that setting, your Server Cache Configuration file would add the following &lt;proxy-scheme&gt; : <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt;proxy&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; Under the covers, this configuration will use the class DefaultProxyServiceLoadBalancer . Note The other build-in load-balancing option is client for client-based load-balancing. We will use that option in the firewall use-case below. You can, however, customize the load-balancing logic depending on your needs by providing an implementation of the ProxyServiceLoadBalancer interface. As mentioned above, Coherence&#8217;s default implementation is the DefaultProxyServiceLoadBalancer . For our test-case, lets simply customize it by adding some more logging: <markup lang=\"java\" >public class CustomProxyServiceLoadBalancer extends DefaultProxyServiceLoadBalancer { @Override public int compare(ProxyServiceLoad load1, ProxyServiceLoad load2) { int result = super.compare(load1, load2); System.out.println(String.format(\"Local Member Id: %s (Total # of Members: %s) - Connection Count: %s\", super.getLocalMember().getId(), super.getMemberList(null).size(), load1.getConnectionCount())); return result; } } The Server Cache Configuration file at src/main/resources/load-balancing/server-coherence-cache-config.xml is almost the same compared to the name-service example, but we add a &lt;load-balancer&gt; element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.guides.extend.loadbalancer.CustomProxyServiceLoadBalancer&lt;/class-name&gt; &lt;/instance&gt; &lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The MyCountryExtendService also specifies a &lt;load-balancer&gt; element The load-balancer uses the customized CustomProxyServiceLoadBalancer The corresponding Client Cache Configuration file at src/main/resources/load-balancing/client-coherence-cache-config.xml will be identical to the Client Cache Configuration files used for the name-service example. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In this example, we will beef up the usage of Oracle Bedrock quite a bit. In order to demonstrate the load-balancer, we will create a Coherence Cluster with 4 nodes (members) and 3 Coherence*Extend clients that connect to those members. <markup lang=\"java\" >@Test void testCoherenceExtendConnection() throws InterruptedException { LocalPlatform platform = LocalPlatform.get(); int numberOfServers = 4; int numberOfClients = 3; List&lt;CoherenceClusterMember&gt; servers = new ArrayList&lt;&gt;(numberOfServers); List&lt;CoherenceClusterMember&gt; clients = new ArrayList&lt;&gt;(numberOfClients); try { for (int i = 1; i &lt;= numberOfServers; i++) { CoherenceClusterMember server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/server-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), ClusterName.of(\"myCluster\"), RoleName.of(\"server\"), SystemProperty.of(\"coherence.log.level\", \"5\"), DisplayName.of(\"server-\" + i)); servers.add(server); } for (CoherenceClusterMember server : servers) { Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); assertThat(server.getExtendConnectionCount(\"MyCountryExtendService\"), is(0)); } for (int i = 1; i &lt;= numberOfClients; i++) { CoherenceClusterMember client = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/client-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), SystemProperty.of(\"coherence.client\", \"remote\"), SystemProperty.of(\"coherence.tcmp.enabled\", \"false\"), SystemProperty.of(\"coherence.log.level\", \"5\"), ClusterName.of(\"myCluster\"), RoleName.of(\"client\"), DisplayName.of(\"client-\" + i)); clients.add(client); } for (CoherenceClusterMember client : clients) { Eventually.assertDeferred(client::isCoherenceRunning, is(true)); client.invoke(new Connect()); } int clientCount = servers.stream() .map(server -&gt; server.getExtendConnectionCount(\"MyCountryExtendService\")) .reduce(0, Integer::sum); assertThat(clientCount, is(3)); TimeUnit.MILLISECONDS.sleep(20000); } finally { for (CoherenceClusterMember client : clients) { client.close(); } for (CoherenceClusterMember server : servers) { server.close(); } } } First we specify the desired number of Coherence servers, 4 in this case We also need 3 Coherence*Extend clients In a loop we create the Coherence servers using the server cache configuration file For each server we make sure it is running None of the servers should have a client connected to them, yet Next we start all the clients using the client cache configuration file We also make sure that all clients are running Once running, we invoke a task on the client that establishes the Coherence*Extend connection. See the source code snippet below Let&#8217;s introspect all the started servers. For each of them, we get the Coherence*Extend connection count for the MyCountryExtendService and sum the result The client connection count should be 3 Let&#8217;s wait for 20 seconds, so you can observe the logging activity of our custom load-balancer As mentioned above, we execute a task for each Coherence*Extend client, to establish the actual connection. In Bedrock, we can submit a RemoteCallable to achieve this: <markup lang=\"java\" >public static class Connect implements RemoteCallable&lt;UUID&gt; { @Override public UUID call() { Session session = Coherence.getInstance().getSession(); NamedCache&lt;Object, Object&gt; cache = session.getCache(\"countries\"); Member member = cache.getCacheService().getInfo().getServiceMember(0); return member.getUuid(); } } Our class implements Bedrock&#8217;s RemoteCallable interface Get a Coherence session Retrieve the countries cache from the session Run the Test When running the test you should notice the logging to the console from our CustomProxyServiceLoadBalancer : <markup lang=\"bash\" >[server-1:out:44488] 2: Local Member Id: 1 (Total # of Members: 4) - Connection Count: 1 [server-3:out:44488] 2: Local Member Id: 4 (Total # of Members: 4) - Connection Count: 1 [server-2:out:44488] 2: Local Member Id: 2 (Total # of Members: 4) - Connection Count: 1 [server-4:out:44488] 2: Local Member Id: 3 (Total # of Members: 4) - Connection Count: 0 As we have 4 Cluster Servers but only 3 clients, 1 Cluster Server will have 0 client connections, while each other server has 1 client connection each. ",
            "title": "Using Proxy Load Balancing"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " The client should be able to connect the server on the explicitly defined host and port. ",
            "title": "Run the Test"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Generally we recommend using the name service to connect to Coherence, but you may have specific firewall constraints. In that case, you can configure the Coherence server to listen to a specific address and port instead. The Server Cache Configuration file at src/main/resources/firewall/server-coherence-cache-config.xml will look almost identical to the example using the name service. However, here we add an &lt;acceptor-config&gt; XML element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;address-provider&gt; &lt;local-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/local-address&gt; &lt;/address-provider&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Add a &lt;acceptor-config&gt; element Define an explicit Coherence*Extend host address, in this case 127.0.0.1 Define the port 7077 on which we will be listening for Coherence*Extend clients We need to set load-balancing to client We will also create a corresponding Client Cache Configuration file at src/main/resources/firewall/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Define the &lt;initiator-config&gt; element Specify the Coherence*Extend server address and port under the &lt;remote-addresses&gt; element The test case itself is identical to the name service test above: <markup lang=\"java\" >@Test void testFirewallUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.FIREWALL_INSTANCE_NAME, \"firewall/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.FIREWALL_INSTANCE_NAME, \"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Run the Test The client should be able to connect the server on the explicitly defined host and port. ",
            "title": "Setting Specific Host and Port"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " In this guide, we gave a few deeper examples of how to set up Coherence*Extend to connect clients to a Coherence Cluster. As part of the Coherence reference documentation, we provide an entire guide on Developing Remote Clients for Oracle Coherence . Part I of that guide provides not only an introduction to Coherence*Extend but also covers advanced topics as well as best practices. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " Bootstrap Coherence Run Coherence as an Extend Client Configure an Extend Client Introduction to Coherence*Extend Configuring Extend Clients Advanced Extend Configuration Best Practices for Coherence*Extend ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/070-coherence-extend/README",
            "text": " In the previous guide Bootstrap Coherence we briefly talked about connecting to a Coherence Cluster via a Coherence*Extend client using the default cache configuration file. This guide will go a bit deeper in regard to using Coherence*Extend and cover the following use-cases: Connect using the name service using a custom cache configuration file Demonstrate Proxy load balancing Setting specific host &amp; port (Firewall use-case) In all 3 use-cases we will use custom cache configuration files. Table of Contents What You Will Build What You Need Building The Example Code Example Data Model Why use Coherence*Extend? Connect via the Name Service Using Proxy Load Balancing Setting Host and Port Explicitly Summary See Also What You Will Build The example code is written as a set of JUnit integration tests, showing how you can use Coherence*Extend. For our test cases we will also use Oracle Bedrock to start server instances of Oracle Coherence for testing purposes. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . Why use Coherence*Extend? Although recommended, it may not always be possible that your application can be directly part of a Coherence cluster using the Tangosol Cluster Management Protocol (TCMP). For example, your application may be located in a different network, you need to access Oracle Coherence from desktop applications, or you need to use languages other than Java, e.g. C++ or .NET. Another alternative is to use the gRPC integration . Connect via the Name Service When connecting to a Coherence Cluster via Coherence*Extend, we recommend the use of the Name Service. The use of the name service simplifies port management as the name service will look up the actual Coherence*Extend ports. That way Coherence*Extend ports can be ephemeral. For this example, let&#8217;s start with the Server Cache Configuration file at src/main/resources/name-service/server-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In the &lt;cache-mapping&gt; element, we state that the countries cache maps to the country-scheme The country-scheme then declares the &lt;proxy-scheme&gt; with the name MyCountryExtendService The MyCountryExtendService will start automatically The MyCountryExtendService will be registered with the default name service. If you wanted to customize that behavior, you would need to provide an &lt;acceptor-config&gt; element. See the load-balancing use-case below for details. We will also create a corresponding Client Cache Configuration file at src/main/resources/name-service/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The cache mapping for the client will look similar to the server one, but we name the scheme remote-country-scheme The client specifies a &lt;remote-cache-scheme&gt; element The service name MyCountryExtendService must match the name we use in the server cache configuration file We also define a request-timeout of 5 seconds. This means that if a connection cannot be established within that time, an exception is raised The client will be using the default name service port of 7574 to lookup the proxy endpoint for the MyCountryExtendService . You could customize that configuration by providing an &lt;initiator-config&gt; element. See the firewall example below for details. In the test case itself, we will use Oracle Bedrock to boostrap the Coherence server using the Server Cache Configuration file: <markup lang=\"java\" >static CoherenceClusterMember server; @BeforeAll static void setup() { final LocalPlatform platform = LocalPlatform.get(); // Start the Coherence server server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"name-service/server-coherence-cache-config.xml\"), IPv4Preferred.yes(), SystemProperty.of(\"coherence.wka\", \"127.0.0.1\"), ClusterName.of(\"myCluster\"), DisplayName.of(\"server\")); // Wait for Coherence to start Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); } Specify the server cache configuration file Give the Server Cluster an explicit name myCluster Make sure that we wait until the MyCountryExtendService proxy service is available Then we configure and start the Coherence client. <markup lang=\"java\" >@Test void testNameServiceUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.NAME_SERVICE_INSTANCE_NAME, \"name-service/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.NAME_SERVICE_INSTANCE_NAME,\"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Disable TCMP to ensure that we only connect via Coherence*Extend Set the cluster name of the client to the same name as the server Specify the client cache configuration file Get the NamedCache and add a new country Important When configuring your Coherence*Extend client, it is important that your client&#8217;s Cluster Name match the name of the Coherence Server Cluster. Tip Java-based clients located on the same network as the Coherence server should disable TCMP communication in order to ensure that the client connect to clustered services exclusively using extend proxies. This can be achieved by setting System property coherence.tcmp.enabled to false . Please see the reference documentation for more detailed information. Run the Test Running the test should be fairly uneventful. If successful, you will see Bedrock starting up the Coherence server with 1 instance followed by the client starting up and connecting. Let&#8217;s do a quick test of the request-timeout and see what happens when the Coherence Server is not available. Comment out the setup() method, and re-run the test. After the specified request-timeout of 5 seconds, you should get a stacktrace with an exception similar to the following: <markup lang=\"bash\" >com.tangosol.net.messaging.ConnectionException: Unable to locate cluster 'myCluster' while looking for its ProxyService 'MyCountryExtendService' In the next section we will see how we can use multiple Coherence servers, and thus taking advantage of proxy load-balancing. Using Proxy Load Balancing When you have multiple Coherence servers that you are connecting to via Coherence*Extend, connection load-balancing is automatically applied. The default load-balancing behavior is based on the load of each Coherence server member and client connections are evenly spread across the Coherence cluster. The default load balance algorithm is called ‘proxy’, which if you were to explicitly configure that setting, your Server Cache Configuration file would add the following &lt;proxy-scheme&gt; : <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt;proxy&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; Under the covers, this configuration will use the class DefaultProxyServiceLoadBalancer . Note The other build-in load-balancing option is client for client-based load-balancing. We will use that option in the firewall use-case below. You can, however, customize the load-balancing logic depending on your needs by providing an implementation of the ProxyServiceLoadBalancer interface. As mentioned above, Coherence&#8217;s default implementation is the DefaultProxyServiceLoadBalancer . For our test-case, lets simply customize it by adding some more logging: <markup lang=\"java\" >public class CustomProxyServiceLoadBalancer extends DefaultProxyServiceLoadBalancer { @Override public int compare(ProxyServiceLoad load1, ProxyServiceLoad load2) { int result = super.compare(load1, load2); System.out.println(String.format(\"Local Member Id: %s (Total # of Members: %s) - Connection Count: %s\", super.getLocalMember().getId(), super.getMemberList(null).size(), load1.getConnectionCount())); return result; } } The Server Cache Configuration file at src/main/resources/load-balancing/server-coherence-cache-config.xml is almost the same compared to the name-service example, but we add a &lt;load-balancer&gt; element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;load-balancer&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.guides.extend.loadbalancer.CustomProxyServiceLoadBalancer&lt;/class-name&gt; &lt;/instance&gt; &lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The MyCountryExtendService also specifies a &lt;load-balancer&gt; element The load-balancer uses the customized CustomProxyServiceLoadBalancer The corresponding Client Cache Configuration file at src/main/resources/load-balancing/client-coherence-cache-config.xml will be identical to the Client Cache Configuration files used for the name-service example. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; In this example, we will beef up the usage of Oracle Bedrock quite a bit. In order to demonstrate the load-balancer, we will create a Coherence Cluster with 4 nodes (members) and 3 Coherence*Extend clients that connect to those members. <markup lang=\"java\" >@Test void testCoherenceExtendConnection() throws InterruptedException { LocalPlatform platform = LocalPlatform.get(); int numberOfServers = 4; int numberOfClients = 3; List&lt;CoherenceClusterMember&gt; servers = new ArrayList&lt;&gt;(numberOfServers); List&lt;CoherenceClusterMember&gt; clients = new ArrayList&lt;&gt;(numberOfClients); try { for (int i = 1; i &lt;= numberOfServers; i++) { CoherenceClusterMember server = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/server-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), ClusterName.of(\"myCluster\"), RoleName.of(\"server\"), SystemProperty.of(\"coherence.log.level\", \"5\"), DisplayName.of(\"server-\" + i)); servers.add(server); } for (CoherenceClusterMember server : servers) { Eventually.assertDeferred(() -&gt; server.invoke( new IsServiceRunning(\"MyCountryExtendService\")), is(true)); assertThat(server.getExtendConnectionCount(\"MyCountryExtendService\"), is(0)); } for (int i = 1; i &lt;= numberOfClients; i++) { CoherenceClusterMember client = platform.launch(CoherenceClusterMember.class, CacheConfig.of(\"load-balancing/client-coherence-cache-config.xml\"), ClassName.of(Coherence.class), LocalHost.only(), Logging.atInfo(), IPv4Preferred.yes(), SystemProperty.of(\"coherence.client\", \"remote\"), SystemProperty.of(\"coherence.tcmp.enabled\", \"false\"), SystemProperty.of(\"coherence.log.level\", \"5\"), ClusterName.of(\"myCluster\"), RoleName.of(\"client\"), DisplayName.of(\"client-\" + i)); clients.add(client); } for (CoherenceClusterMember client : clients) { Eventually.assertDeferred(client::isCoherenceRunning, is(true)); client.invoke(new Connect()); } int clientCount = servers.stream() .map(server -&gt; server.getExtendConnectionCount(\"MyCountryExtendService\")) .reduce(0, Integer::sum); assertThat(clientCount, is(3)); TimeUnit.MILLISECONDS.sleep(20000); } finally { for (CoherenceClusterMember client : clients) { client.close(); } for (CoherenceClusterMember server : servers) { server.close(); } } } First we specify the desired number of Coherence servers, 4 in this case We also need 3 Coherence*Extend clients In a loop we create the Coherence servers using the server cache configuration file For each server we make sure it is running None of the servers should have a client connected to them, yet Next we start all the clients using the client cache configuration file We also make sure that all clients are running Once running, we invoke a task on the client that establishes the Coherence*Extend connection. See the source code snippet below Let&#8217;s introspect all the started servers. For each of them, we get the Coherence*Extend connection count for the MyCountryExtendService and sum the result The client connection count should be 3 Let&#8217;s wait for 20 seconds, so you can observe the logging activity of our custom load-balancer As mentioned above, we execute a task for each Coherence*Extend client, to establish the actual connection. In Bedrock, we can submit a RemoteCallable to achieve this: <markup lang=\"java\" >public static class Connect implements RemoteCallable&lt;UUID&gt; { @Override public UUID call() { Session session = Coherence.getInstance().getSession(); NamedCache&lt;Object, Object&gt; cache = session.getCache(\"countries\"); Member member = cache.getCacheService().getInfo().getServiceMember(0); return member.getUuid(); } } Our class implements Bedrock&#8217;s RemoteCallable interface Get a Coherence session Retrieve the countries cache from the session Run the Test When running the test you should notice the logging to the console from our CustomProxyServiceLoadBalancer : <markup lang=\"bash\" >[server-1:out:44488] 2: Local Member Id: 1 (Total # of Members: 4) - Connection Count: 1 [server-3:out:44488] 2: Local Member Id: 4 (Total # of Members: 4) - Connection Count: 1 [server-2:out:44488] 2: Local Member Id: 2 (Total # of Members: 4) - Connection Count: 1 [server-4:out:44488] 2: Local Member Id: 3 (Total # of Members: 4) - Connection Count: 0 As we have 4 Cluster Servers but only 3 clients, 1 Cluster Server will have 0 client connections, while each other server has 1 client connection each. Setting Specific Host and Port Generally we recommend using the name service to connect to Coherence, but you may have specific firewall constraints. In that case, you can configure the Coherence server to listen to a specific address and port instead. The Server Cache Configuration file at src/main/resources/firewall/server-coherence-cache-config.xml will look almost identical to the example using the name service. However, here we add an &lt;acceptor-config&gt; XML element. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;country-scheme&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;proxy-scheme&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;address-provider&gt; &lt;local-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/local-address&gt; &lt;/address-provider&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;load-balancer&gt;client&lt;/load-balancer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/proxy-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Add a &lt;acceptor-config&gt; element Define an explicit Coherence*Extend host address, in this case 127.0.0.1 Define the port 7077 on which we will be listening for Coherence*Extend clients We need to set load-balancing to client We will also create a corresponding Client Cache Configuration file at src/main/resources/firewall/client-coherence-cache-config.xml . <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;countries&lt;/cache-name&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote-country-scheme&lt;/scheme-name&gt; &lt;service-name&gt;MyCountryExtendService&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;127.0.0.1&lt;/address&gt; &lt;port&gt;7077&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;outgoing-message-handler&gt; &lt;request-timeout&gt;5s&lt;/request-timeout&gt; &lt;/outgoing-message-handler&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Define the &lt;initiator-config&gt; element Specify the Coherence*Extend server address and port under the &lt;remote-addresses&gt; element The test case itself is identical to the name service test above: <markup lang=\"java\" >@Test void testFirewallUseCase() { System.setProperty(\"coherence.tcmp.enabled\", \"false\"); System.setProperty(\"coherence.cluster\", \"myCluster\"); System.setProperty(\"coherence.wka\", \"127.0.0.1\"); CoherenceHelper.startCoherenceClient( CoherenceHelper.FIREWALL_INSTANCE_NAME, \"firewall/client-coherence-cache-config.xml\"); NamedCache&lt;String, Country&gt; countries = CoherenceHelper.getMap(CoherenceHelper.FIREWALL_INSTANCE_NAME, \"countries\"); countries.put(\"de\", new Country(\"Germany\", \"Berlin\", 83.2)); } Run the Test The client should be able to connect the server on the explicitly defined host and port. Summary In this guide, we gave a few deeper examples of how to set up Coherence*Extend to connect clients to a Coherence Cluster. As part of the Coherence reference documentation, we provide an entire guide on Developing Remote Clients for Oracle Coherence . Part I of that guide provides not only an introduction to Coherence*Extend but also covers advanced topics as well as best practices. See Also Bootstrap Coherence Run Coherence as an Extend Client Configure an Extend Client Introduction to Coherence*Extend Configuring Extend Clients Advanced Extend Configuration Best Practices for Coherence*Extend ",
            "title": "Coherence*Extend"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " What You Will Build What You Need Building The Example Code Example Data Model Why use Entry Processors? Creating an Entry Processor Using Lambda Expressions Process Single Map Keys Using Lambda Expressions Update all Map Entries Specify Cache Entry Expiration Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " The example code is written as a set of unit tests, showing how you can use Entry Processors with Coherence. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " In our example, we do have several countries loaded into the cache. Let&#8217;s assume we want to increase the population of several countries by a million each. More specifically, we only want to increase the population for those countries that have a population of 60 million or more. The obvious choice would be to query the cache using a GreaterEqualsFilter as we have done in the previous example on Views , iterate over the results and update the respective countries. <markup lang=\"java\" >@Test void testIncreasePopulationWithoutEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Set&lt;String&gt; filteredKeys = map.keySet(filter); assertThat(filteredKeys).hasSize(2); for (String key : filteredKeys) { map.lock(key, 0); try { Country country = map.get(key); country.setPopulation(country.getPopulation() + 1); map.put(key, country); } finally { map.unlock(key); } } assertThat(map).hasSize(5); Country germany = map.get(\"de\"); Country france = map.get(\"fr\"); assertThat(germany.getPopulation()).isEqualTo(84.2d); assertThat(france.getPopulation()).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking keySet(filter) on the NamedCache that will return a Set of keys Assert that the Set of filtered keys only contains 2 keys Loop over the keys Make sure we lock the cache entry Increment the population by 1 million Update the map This is an example of how NOT to do this! While this works, it will be inefficient in use-cases where you have to update high number of cache entries.This approach would cause a lot of data to be moved over the wire, first for the retrieval of countries and then when pushing the updated countries back into the cluster. This is where Entry Processors come into play. Entry Processors allow us to perform data grid processing inside the Coherence cluster. You can either apply Entry Processors for single cache keys or you can perform parallel processing against a collection of cache entries (map-reduce functionality). For a more in-depth introduction to Entry Processors, please refer to the respective chapter Processing Data In a Cache in the Oracle Coherence reference guide. ",
            "title": "Why use Entry Processors?"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Let&#8217;s rewrite the inefficient example above to use an Entry Processor. First, we will create a class called IncrementingEntryProcessor that implements InvocableMap.EntryProcessor : <markup lang=\"java\" >import com.oracle.coherence.guides.entryprocessors.model.Country; import com.tangosol.util.InvocableMap; /** * @author Gunnar Hillert 2022.02.25 */ public class IncrementingEntryProcessor implements InvocableMap.EntryProcessor&lt;String, Country, Double&gt; { @Override public Double process(InvocableMap.Entry&lt;String, Country&gt; entry) { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); } } The Entry Processor implements Coherence&#8217;s InvocableMap.EntryProcessor class. The type parameters represent the key, the value and the return type of the Entry Processor. The process() method gives us access to the value of the countries Map Increment the population by 1 million Return the incremented population The IncrementingEntryProcessor contains one method process() that provides us with access to the Country via the InvocableMap.Entry argument. We will increase the population and the return the population. Now it is time to use the IncrementingEntryProcessor . <markup lang=\"java\" >@Test void testIncreasePopulationWithCustomEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, new IncrementingEntryProcessor()); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map, passing in both the filter and the IncrementingEntryProcessor The result should be the Map containing the key and the new population value for the 2 affected countries In this example we are processing multiple map entries at once. You can of course apply Entry Processors to single map keys as well by using: <markup lang=\"java\" >@Test void testIncreasePopulationForSingleEntry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double result = map.invoke(\"de\", new IncrementingEntryProcessor()); assertThat(result).isEqualTo(84.2d); } Get the countries Map Call invoke on the countries Map, passing in the key (instead of the filter) and the IncrementingEntryProcessor The result should be the double value representing the new population value of Germany Tip Before writing your own Entry Processor from scratch, please also check if one of the built-in Entry Processors may already solve your requirement. In the next section we will see how we can simplify the example even further using lambda expressions. ",
            "title": "Creating an Entry Processor"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Instead of creating dedicated Entry Processor classes, it may be advisable to pass in lambda expressions instead. Especially in use-cases such as our very simple contrived example, lambda expressions simplify the code noticeably. <markup lang=\"java\" >@Test void testIncreasePopulationUsingLambdaExpression() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map passing in the filter and the function that increments the population The result should be the Map containing the key and the new population value for the 2 affected countries ",
            "title": "Using Lambda Expressions"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " When using lambda expressions for single map keys, you can use Coherence&#8217;s invoke() as well as Java&#8217;s Map.compute() method. Let&#8217;s see the code for Coherence&#8217;s invoke() method first: <markup lang=\"java\" >@Test void testIncreasePopulationUsingInvokeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double results = map.invoke(\"de\", entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); entry.setValue(country); return country.getPopulation(); }); assertThat(results).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call invoke on the NamedCache , passing the key for Germany and the lambda expression It is important to explicitly call setValue on the cache entry If using compute() , the code will look like this: <markup lang=\"java\" >@Test void testIncreasePopulationUsingComputeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Country results = map.compute(\"de\", (key, country) -&gt; { country.setPopulation(country.getPopulation() + 1); return country; }); assertThat(results.getPopulation()).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call compute on the NamedCache , passing the key for Germany and the lambda expression. Set the new population but as you can see that there is no need to set the country explicitly on the cache entry. The code when using compute looks a little simpler, as compute implicitly updates the value to whatever you return. When using invoke , you have to explicitly call entry.setValue(country) . On the other hand, compute will return the entire country, whereas with invoke you can return any data object. This is advantageous in situations where you need to minimize the amount of data passed over the wire. ",
            "title": "Process Single Map Keys Using Lambda Expressions"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Sometimes we may need to update all entries in a Coherence Map. In that use-case we simply change the passed-in Filter. The lambda expression on the other hand remains the same. All we need to do is to pass in an instance of the AlwaysFilter : <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountries() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map Get an instance of the AlwaysFilter that will select all entries in the countries Map Call invokeAll on the countries Map passing in the AlwaysFilter and the function that increments the population The result should be the Map containing the key and the new population value for all 5 countries in the Map ",
            "title": "Update all Map Entries"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " When adding values to a NamedCache , without using an Entry Processor, you have the optional ability to also set an expiration value for the cache value. For example, in the following code snippet, we add a new country with an expiration value of 5 seconds (Values are specified in milliseconds). <markup lang=\"java\" >NamedCache&lt;String, Country&gt; countries = getMap(\"countries\"); Country country = new Country(\"Germany\", \"Berlin\", 83.2); countries.put(\"de\", country , 5000); Note Cache expiration can also be configured per-cache using the &lt;expiry-delay&gt; cache configuration element in the coherence-cache-config.xml file. See the reference documentation for details. You can do the same when mutating cache entries via Entry Processors. Let&#8217;s update the previous example so that all countries we are updating will expire within 2 seconds. As the code listed below is very similar to the previous example, we shall highlight the changes only: <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountriesWithExpiration() throws InterruptedException { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { BinaryEntry&lt;String, Double&gt; binEntry = (BinaryEntry) entry; binEntry.expire(2000); Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(map).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); Thread.sleep(4000); assertThat(results).hasSize(5); assertThat(map).hasSize(0); } We need to cast the InvocableMap.Entry to a BinaryEntry . BinaryEntry has a method expire that specifies when the cache entry will expire. We set it to 2 seconds. The parameter is an integer value representing milliseconds. The returned Map of invokeAll will have 5 countries . This Map will not \"expire\". There should be 5 entries in the NamedCache map. The NamedCache map will expire. The Thread will sleep for 4 seconds. Assert that the returned Map of invokeAll still has 5 countries Assert that the NamedCache map is empty. The values have expired. The important piece of information to remember is that the underlying BinaryEntry has the relevant property to specify the expiration of the cache entry, and we need to cast InvocableMap.Entry to a BinaryEntry . Important The expiration property is defined as an integer and is expressed in milliseconds. Therefore, the maximum amount of time cannot exceed approximately 24 days. ",
            "title": "Specify Cache Entry Expiration"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " In this guide we showed how you can easily create Entry Processors to perform data grid processing across a cluster. Please see the Coherence reference guide, specifically the chapter Processing Data In a Cache for more details. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " Processing Data In a Cache Querying Caches Views ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/128-entry-processors/README",
            "text": " This guide walks you through the concepts of creating Entry Processors . Entry Processors allow you to perform data grid processing across a cluster. That means without moving cache entries across the wire, you can process one or more cache entries locally on the storage node. Table of Contents What You Will Build What You Need Building The Example Code Example Data Model Why use Entry Processors? Creating an Entry Processor Using Lambda Expressions Process Single Map Keys Using Lambda Expressions Update all Map Entries Specify Cache Entry Expiration Summary See Also What You Will Build The example code is written as a set of unit tests, showing how you can use Entry Processors with Coherence. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . Why use Entry Processors? In our example, we do have several countries loaded into the cache. Let&#8217;s assume we want to increase the population of several countries by a million each. More specifically, we only want to increase the population for those countries that have a population of 60 million or more. The obvious choice would be to query the cache using a GreaterEqualsFilter as we have done in the previous example on Views , iterate over the results and update the respective countries. <markup lang=\"java\" >@Test void testIncreasePopulationWithoutEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Set&lt;String&gt; filteredKeys = map.keySet(filter); assertThat(filteredKeys).hasSize(2); for (String key : filteredKeys) { map.lock(key, 0); try { Country country = map.get(key); country.setPopulation(country.getPopulation() + 1); map.put(key, country); } finally { map.unlock(key); } } assertThat(map).hasSize(5); Country germany = map.get(\"de\"); Country france = map.get(\"fr\"); assertThat(germany.getPopulation()).isEqualTo(84.2d); assertThat(france.getPopulation()).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking keySet(filter) on the NamedCache that will return a Set of keys Assert that the Set of filtered keys only contains 2 keys Loop over the keys Make sure we lock the cache entry Increment the population by 1 million Update the map This is an example of how NOT to do this! While this works, it will be inefficient in use-cases where you have to update high number of cache entries.This approach would cause a lot of data to be moved over the wire, first for the retrieval of countries and then when pushing the updated countries back into the cluster. This is where Entry Processors come into play. Entry Processors allow us to perform data grid processing inside the Coherence cluster. You can either apply Entry Processors for single cache keys or you can perform parallel processing against a collection of cache entries (map-reduce functionality). For a more in-depth introduction to Entry Processors, please refer to the respective chapter Processing Data In a Cache in the Oracle Coherence reference guide. Creating an Entry Processor Let&#8217;s rewrite the inefficient example above to use an Entry Processor. First, we will create a class called IncrementingEntryProcessor that implements InvocableMap.EntryProcessor : <markup lang=\"java\" >import com.oracle.coherence.guides.entryprocessors.model.Country; import com.tangosol.util.InvocableMap; /** * @author Gunnar Hillert 2022.02.25 */ public class IncrementingEntryProcessor implements InvocableMap.EntryProcessor&lt;String, Country, Double&gt; { @Override public Double process(InvocableMap.Entry&lt;String, Country&gt; entry) { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); } } The Entry Processor implements Coherence&#8217;s InvocableMap.EntryProcessor class. The type parameters represent the key, the value and the return type of the Entry Processor. The process() method gives us access to the value of the countries Map Increment the population by 1 million Return the incremented population The IncrementingEntryProcessor contains one method process() that provides us with access to the Country via the InvocableMap.Entry argument. We will increase the population and the return the population. Now it is time to use the IncrementingEntryProcessor . <markup lang=\"java\" >@Test void testIncreasePopulationWithCustomEntryProcessor() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, new IncrementingEntryProcessor()); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map, passing in both the filter and the IncrementingEntryProcessor The result should be the Map containing the key and the new population value for the 2 affected countries In this example we are processing multiple map entries at once. You can of course apply Entry Processors to single map keys as well by using: <markup lang=\"java\" >@Test void testIncreasePopulationForSingleEntry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double result = map.invoke(\"de\", new IncrementingEntryProcessor()); assertThat(result).isEqualTo(84.2d); } Get the countries Map Call invoke on the countries Map, passing in the key (instead of the filter) and the IncrementingEntryProcessor The result should be the double value representing the new population value of Germany Tip Before writing your own Entry Processor from scratch, please also check if one of the built-in Entry Processors may already solve your requirement. In the next section we will see how we can simplify the example even further using lambda expressions. Using Lambda Expressions Instead of creating dedicated Entry Processor classes, it may be advisable to pass in lambda expressions instead. Especially in use-cases such as our very simple contrived example, lambda expressions simplify the code noticeably. <markup lang=\"java\" >@Test void testIncreasePopulationUsingLambdaExpression() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(2); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Call invokeAll on the countries Map passing in the filter and the function that increments the population The result should be the Map containing the key and the new population value for the 2 affected countries Process Single Map Keys Using Lambda Expressions When using lambda expressions for single map keys, you can use Coherence&#8217;s invoke() as well as Java&#8217;s Map.compute() method. Let&#8217;s see the code for Coherence&#8217;s invoke() method first: <markup lang=\"java\" >@Test void testIncreasePopulationUsingInvokeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Double results = map.invoke(\"de\", entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); entry.setValue(country); return country.getPopulation(); }); assertThat(results).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call invoke on the NamedCache , passing the key for Germany and the lambda expression It is important to explicitly call setValue on the cache entry If using compute() , the code will look like this: <markup lang=\"java\" >@Test void testIncreasePopulationUsingComputeForSingleCountry() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Country results = map.compute(\"de\", (key, country) -&gt; { country.setPopulation(country.getPopulation() + 1); return country; }); assertThat(results.getPopulation()).isEqualTo(84.2d); assertThat(map.get(\"de\").getPopulation()).isEqualTo(84.2d); } Get the countries Map Call compute on the NamedCache , passing the key for Germany and the lambda expression. Set the new population but as you can see that there is no need to set the country explicitly on the cache entry. The code when using compute looks a little simpler, as compute implicitly updates the value to whatever you return. When using invoke , you have to explicitly call entry.setValue(country) . On the other hand, compute will return the entire country, whereas with invoke you can return any data object. This is advantageous in situations where you need to minimize the amount of data passed over the wire. Update all Map Entries Sometimes we may need to update all entries in a Coherence Map. In that use-case we simply change the passed-in Filter. The lambda expression on the other hand remains the same. All we need to do is to pass in an instance of the AlwaysFilter : <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountries() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); } Get the countries Map Get an instance of the AlwaysFilter that will select all entries in the countries Map Call invokeAll on the countries Map passing in the AlwaysFilter and the function that increments the population The result should be the Map containing the key and the new population value for all 5 countries in the Map Specify Cache Entry Expiration When adding values to a NamedCache , without using an Entry Processor, you have the optional ability to also set an expiration value for the cache value. For example, in the following code snippet, we add a new country with an expiration value of 5 seconds (Values are specified in milliseconds). <markup lang=\"java\" >NamedCache&lt;String, Country&gt; countries = getMap(\"countries\"); Country country = new Country(\"Germany\", \"Berlin\", 83.2); countries.put(\"de\", country , 5000); Note Cache expiration can also be configured per-cache using the &lt;expiry-delay&gt; cache configuration element in the coherence-cache-config.xml file. See the reference documentation for details. You can do the same when mutating cache entries via Entry Processors. Let&#8217;s update the previous example so that all countries we are updating will expire within 2 seconds. As the code listed below is very similar to the previous example, we shall highlight the changes only: <markup lang=\"java\" >@Test void testIncreasePopulationForAllCountriesWithExpiration() throws InterruptedException { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = AlwaysFilter.INSTANCE(); Map&lt;String, Double&gt; results = map.invokeAll(filter, entry -&gt; { BinaryEntry&lt;String, Double&gt; binEntry = (BinaryEntry) entry; binEntry.expire(2000); Country country = entry.getValue(); country.setPopulation(country.getPopulation() + 1); return country.getPopulation(); }); assertThat(results).hasSize(5); assertThat(map).hasSize(5); assertThat(results.get(\"ua\")).isEqualTo(42.2d); assertThat(results.get(\"co\")).isEqualTo(51.4d); assertThat(results.get(\"au\")).isEqualTo(27d); assertThat(results.get(\"de\")).isEqualTo(84.2d); assertThat(results.get(\"fr\")).isEqualTo(68.4d); Thread.sleep(4000); assertThat(results).hasSize(5); assertThat(map).hasSize(0); } We need to cast the InvocableMap.Entry to a BinaryEntry . BinaryEntry has a method expire that specifies when the cache entry will expire. We set it to 2 seconds. The parameter is an integer value representing milliseconds. The returned Map of invokeAll will have 5 countries . This Map will not \"expire\". There should be 5 entries in the NamedCache map. The NamedCache map will expire. The Thread will sleep for 4 seconds. Assert that the returned Map of invokeAll still has 5 countries Assert that the NamedCache map is empty. The values have expired. The important piece of information to remember is that the underlying BinaryEntry has the relevant property to specify the expiration of the cache entry, and we need to cast InvocableMap.Entry to a BinaryEntry . Important The expiration property is defined as an integer and is expressed in milliseconds. Therefore, the maximum amount of time cannot exceed approximately 24 days. Summary In this guide we showed how you can easily create Entry Processors to perform data grid processing across a cluster. Please see the Coherence reference guide, specifically the chapter Processing Data In a Cache for more details. See Also Processing Data In a Cache Querying Caches Views ",
            "title": "Entry Processors"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " Note The documentation on this site covers new features and improvements that are currently only available in the open source Coherence Community Edition (CE). For complete documentation covering all the features that are available both in the latest commercial editions (Enterprise and Grid Edition) and the Community Edition, please refer to the Official Documentation . Coherence is scalable, fault-tolerant, cloud-ready, distributed platform for building grid-based applications and reliably storing data. The product is used at scale, for both compute and raw storage, in a vast array of industries such as critical financial trading systems, high performance telecommunication products, and eCommerce applications. Typically, these deployments do not tolerate any downtime and Coherence is chosen due its novel features in death detection, application data evolvability, and the robust, battle-hardened core of the product that enables it to be seamlessly deployed and adapted within any ecosystem. At a high level, Coherence provides an implementation of the familiar Map&lt;K,V&gt; interface but rather than storing the associated data in the local process, it is partitioned (or sharded) across a number of designated remote nodes. This partitioning enables applications to not only distribute (and therefore scale) their storage across multiple processes, machines, racks, and data centers, but also to perform grid-based processing to truly harness the CPU resources of the machines. The Coherence interface NamedMap&lt;K,V&gt; (an extension of Map&lt;K,V&gt; provides methods to query, aggregate (map/reduce style), and compute (send functions to storage nodes for locally executed mutations) the data set. These capabilities, in addition to numerous other features, enable Coherence to be used as a framework to write robust, distributed applications. Please see here for the latest release notes. ",
            "title": "Overview"
        },
        {
            "location": "/docs/about/01_overview",
            "text": " assistant Coherence What is Oracle Coherence? fa-exclamation-circle Important Changes Important changes in this release of Coherence. fa-rocket Quick Start A quick-start guide to using Coherence. fa-graduation-cap Guides & Tutorials Guides, examples and tutorial about Coherence features and best practice. import_contacts Docs Oracle Coherence commercial edition product documentation. library_books API Docs Browse the Coherence CE API Docs. fa-th Container Images Example Coherence OCI container (Docker) images. ",
            "title": "Get Going"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Federation Configuration Build and Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " You will review the operational and cache configuration required to set up Federated Coherence clusters and carry out the following: Start one or more cache servers for PrimaryCluster Start one or more cache servers for SecondaryCluster Start a CohQL session for PrimaryCluster Start a CohQL session for SecondaryCluster Carry out various data operations on each cluster and observe the data being replicated The default configuration for this example runs the following clusters: PrimaryCluster on 127.0.0.1:7574 SecondaryCluster on 127.0.0.1:7575 You can change these hosts/ports by changing the following in the pom.xml : <markup lang=\"xml\" >&lt;primary.cluster.host&gt;127.0.0.1&lt;/primary.cluster.host&gt; &lt;primary.cluster.port&gt;7574&lt;/primary.cluster.port&gt; &lt;secondary.cluster.host&gt;127.0.0.1&lt;/secondary.cluster.host&gt; &lt;secondary.cluster.port&gt;7575&lt;/secondary.cluster.port&gt; If you wish to know more about Coherence Federation, please see the Coherence Documentation . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Important Because Coherence Federation is only available in Grid Edition, you must carry out the following changes to the project before building and running: Update the coherence.version property in your pom.xml and gradle.properties to the Coherence Grid Edition version you are going to use. Change the coherence.group.id in the above files to com.oracle.coherence . Install Coherence Grid Edition into your local Maven repository by running the following: This example assumes you have Coherence 14.1.1. Please adjust for your Coherence version. <markup lang=\"bas\" >mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/14.1.1/coherence.14.1.1.pom Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Important Because Coherence Federation is only available in Grid Edition, you must carry out the following changes to the project before building and running: Update the coherence.version property in your pom.xml and gradle.properties to the Coherence Grid Edition version you are going to use. Change the coherence.group.id in the above files to com.oracle.coherence . Install Coherence Grid Edition into your local Maven repository by running the following: This example assumes you have Coherence 14.1.1. Please adjust for your Coherence version. <markup lang=\"bas\" >mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/14.1.1/coherence.14.1.1.pom Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. primary-storage - Runs a DefaultCacheServer for the PrimaryCluster primary-cohql - Runs a CohQL session for the PrimaryCluster secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster secondary-cohql - Runs a CohQL session for the SecondaryCluster primary-storage - Runs a DefaultCacheServer for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=PrimaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;PrimaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${primary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${primary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=SecondaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.log.level&lt;/key&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;SecondaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${secondary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${secondary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Federated caching is configured using Coherence configuration files and requires no changes to application code. There are two areas that require configuration for Federation: An operational override file is used to configure federation participants and the federation topology. A cache configuration file is used to create federated caches schemes. A federated cache is a type of partitioned cache service and is managed by a federated cache service instance. The following cache configuration file is used to define the Federated service: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;key-type&gt;java.lang.Integer&lt;/key-type&gt; &lt;value-type&gt;java.lang.String&lt;/value-type&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;federated-scheme&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;service-name&gt;FederatedPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;topologies&gt; &lt;topology&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;/topology&gt; &lt;/topologies&gt; &lt;/federated-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A cache-mapping for all caches (*) to map to a scheme called federated The federated-scheme in a similar way to a distributed-scheme A topology for the federated-scheme. The default topology is active-active so this element is not required and just included for completeness. The following operational configuration file is used to define the participants and topology: <markup lang=\"xml\" >&lt;federation-config&gt; &lt;participants&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"primary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"primary.cluster.port\"&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;SecondaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"secondary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"secondary.cluster.port\"&gt;7575&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;/participants&gt; &lt;topology-definitions&gt; &lt;active-active&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;active system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/active&gt; &lt;active system-property=\"secondary.cluster\"&gt;SecondaryCluster&lt;/active&gt; &lt;/active-active&gt; &lt;/topology-definitions&gt; &lt;/federation-config&gt; PrimaryCluster participant with its host and port for the cluster Name Service SecondaryCluster participant with its host and port for the cluster Name Service Topology that defines an active-active configuration between clusters. This is the default and not strictly required. ",
            "title": "Federation Configuration"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. primary-storage - Runs a DefaultCacheServer for the PrimaryCluster primary-cohql - Runs a CohQL session for the PrimaryCluster secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster secondary-cohql - Runs a CohQL session for the SecondaryCluster primary-storage - Runs a DefaultCacheServer for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=PrimaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;PrimaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${primary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${primary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=SecondaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.log.level&lt;/key&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;SecondaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${secondary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${secondary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Federation Configuration Federated caching is configured using Coherence configuration files and requires no changes to application code. There are two areas that require configuration for Federation: An operational override file is used to configure federation participants and the federation topology. A cache configuration file is used to create federated caches schemes. A federated cache is a type of partitioned cache service and is managed by a federated cache service instance. The following cache configuration file is used to define the Federated service: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;key-type&gt;java.lang.Integer&lt;/key-type&gt; &lt;value-type&gt;java.lang.String&lt;/value-type&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;federated-scheme&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;service-name&gt;FederatedPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;topologies&gt; &lt;topology&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;/topology&gt; &lt;/topologies&gt; &lt;/federated-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A cache-mapping for all caches (*) to map to a scheme called federated The federated-scheme in a similar way to a distributed-scheme A topology for the federated-scheme. The default topology is active-active so this element is not required and just included for completeness. The following operational configuration file is used to define the participants and topology: <markup lang=\"xml\" >&lt;federation-config&gt; &lt;participants&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"primary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"primary.cluster.port\"&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;SecondaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"secondary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"secondary.cluster.port\"&gt;7575&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;/participants&gt; &lt;topology-definitions&gt; &lt;active-active&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;active system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/active&gt; &lt;active system-property=\"secondary.cluster\"&gt;SecondaryCluster&lt;/active&gt; &lt;/active-active&gt; &lt;/topology-definitions&gt; &lt;/federation-config&gt; PrimaryCluster participant with its host and port for the cluster Name Service SecondaryCluster participant with its host and port for the cluster Name Service Topology that defines an active-active configuration between clusters. This is the default and not strictly required. ",
            "title": "Review the Project"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P primary-storage and <markup lang=\"bash\" >./mvnw exec:exec -P secondary-storage Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:java -P primary-cohql and <markup lang=\"bash\" >./mvnw exec:java -P secondary-cohql ",
            "title": "Maven"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runServerPrimary and <markup lang=\"bash\" >./gradlew runServerSecondary Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runCohQLPrimary --console=plain and <markup lang=\"bash\" >./gradlew runCohQLSecondary --console=plain ",
            "title": "Gradle"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " In each of the CohQL sessions, run the following command to verify the caches are empty in each cluster: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 In the first (PrimaryCluster) CohQL session, add an entry to the cache test : <markup lang=\"bash\" >CohQL&gt; insert into test key(1) value(\"Tim\") CohQL&gt; select key(), value() from test Results [1, \"Tim\"] In the second (SecondaryCluster) CohQL session, verify the entry was sent from the PrimaryCluster and then update the name to Timothy . As the clusters are active-active , the changes will be sent back to the primary cluster. <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Tim\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] In the first (PrimaryCluster) CohQL session, verify the entry was changed via the change in the SecondaryCluster , then delete the entry and confirm it was deleted in the SecondaryCluster <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; delete from test Results CohQL&gt; select key(), value() from test Results Continue experimenting: You can continue to experiment by inserting, updating or removing data using various CohQL commands. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. Monitor Federation If you want to monitor Federation, you can do this via the Coherence VisualVM Plugin. See here for how to install the Plugin if you have VisualVM already, otherwise visit https://visualvm.github.io/ to download and install VisualVM . Once you have installed the plugin, you can click on one of the DefaultCacheServer process, and you will see the Federation tab as shown below: ",
            "title": "Run the following commands to exercise Federation"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " After you have built the project as described earlier in this document you can run via Maven or Gradle. Maven Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P primary-storage and <markup lang=\"bash\" >./mvnw exec:exec -P secondary-storage Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:java -P primary-cohql and <markup lang=\"bash\" >./mvnw exec:java -P secondary-cohql Gradle Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runServerPrimary and <markup lang=\"bash\" >./gradlew runServerSecondary Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runCohQLPrimary --console=plain and <markup lang=\"bash\" >./gradlew runCohQLSecondary --console=plain Run the following commands to exercise Federation In each of the CohQL sessions, run the following command to verify the caches are empty in each cluster: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 In the first (PrimaryCluster) CohQL session, add an entry to the cache test : <markup lang=\"bash\" >CohQL&gt; insert into test key(1) value(\"Tim\") CohQL&gt; select key(), value() from test Results [1, \"Tim\"] In the second (SecondaryCluster) CohQL session, verify the entry was sent from the PrimaryCluster and then update the name to Timothy . As the clusters are active-active , the changes will be sent back to the primary cluster. <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Tim\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] In the first (PrimaryCluster) CohQL session, verify the entry was changed via the change in the SecondaryCluster , then delete the entry and confirm it was deleted in the SecondaryCluster <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; delete from test Results CohQL&gt; select key(), value() from test Results Continue experimenting: You can continue to experiment by inserting, updating or removing data using various CohQL commands. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. Monitor Federation If you want to monitor Federation, you can do this via the Coherence VisualVM Plugin. See here for how to install the Plugin if you have VisualVM already, otherwise visit https://visualvm.github.io/ to download and install VisualVM . Once you have installed the plugin, you can click on one of the DefaultCacheServer process, and you will see the Federation tab as shown below: ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " In this guide you walked through the steps to use Coherence Federation by using Coherence Query Language (CohQL) to insert, update and remove data in Federated clusters. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " Federation Documentation Using Coherence Query Language ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/200-federation/README",
            "text": " This guide walks through the steps to use Coherence Federation by using Coherence Query Language (CohQL) to insert, update and remove data in Federated clusters. Federated caching federates cache data asynchronously across multiple geographically dispersed clusters. Cached data is federated across clusters to provide redundancy, off-site backup, and multiple points of access for application users in different geographical locations. Federated caching supports multiple federation topologies. These include: active-active, active-passive, hub-spoke, and central-federation. The topologies define common federation strategies between clusters and support a wide variety of use cases. Custom federation topologies can also be created as required. Federated caching provides applications with the ability to accept, reject, or modify cache entries being stored locally or remotely. Conflict resolution is application specific to allow the greatest amount of flexibility when defining federation rules. Federation is only available when using Coherence Grid Edition (GE) 12.2.1.4.X and above, and is not available in the open-source Coherence Community Edition (CE). As Coherence Grid Edition JAR&#8217;s and not available in Maven central, to build and run this example you, must first install the Coherence JAR into your Maven Repository from your local Grid Edition Install. See here for instructions on how to complete this. Table of Contents What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Federation Configuration Build and Run the Example Summary See Also What You Will Build You will review the operational and cache configuration required to set up Federated Coherence clusters and carry out the following: Start one or more cache servers for PrimaryCluster Start one or more cache servers for SecondaryCluster Start a CohQL session for PrimaryCluster Start a CohQL session for SecondaryCluster Carry out various data operations on each cluster and observe the data being replicated The default configuration for this example runs the following clusters: PrimaryCluster on 127.0.0.1:7574 SecondaryCluster on 127.0.0.1:7575 You can change these hosts/ports by changing the following in the pom.xml : <markup lang=\"xml\" >&lt;primary.cluster.host&gt;127.0.0.1&lt;/primary.cluster.host&gt; &lt;primary.cluster.port&gt;7574&lt;/primary.cluster.port&gt; &lt;secondary.cluster.host&gt;127.0.0.1&lt;/secondary.cluster.host&gt; &lt;secondary.cluster.port&gt;7575&lt;/secondary.cluster.port&gt; If you wish to know more about Coherence Federation, please see the Coherence Documentation . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Important Because Coherence Federation is only available in Grid Edition, you must carry out the following changes to the project before building and running: Update the coherence.version property in your pom.xml and gradle.properties to the Coherence Grid Edition version you are going to use. Change the coherence.group.id in the above files to com.oracle.coherence . Install Coherence Grid Edition into your local Maven repository by running the following: This example assumes you have Coherence 14.1.1. Please adjust for your Coherence version. <markup lang=\"bas\" >mvn install:install-file -Dfile=$COHERENCE_HOME/lib/coherence.jar \\ -DpomFile=$COHERENCE_HOME/plugins/maven/com/oracle/coherence/coherence/14.1.1/coherence.14.1.1.pom Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Project Maven Configuration The initial project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. primary-storage - Runs a DefaultCacheServer for the PrimaryCluster primary-cohql - Runs a CohQL session for the PrimaryCluster secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster secondary-cohql - Runs a CohQL session for the SecondaryCluster primary-storage - Runs a DefaultCacheServer for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=PrimaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the PrimaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;primary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;primary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;PrimaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${primary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${primary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; secondary-storage - Runs a DefaultCacheServer for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-storage&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-storage&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt; &lt;classpath/&gt; &lt;argument&gt;-Dcoherence.cacheconfig=federation-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.port=${primary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.port=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;-Dprimary.cluster.host=${primary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dsecondary.cluster.host=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=${secondary.cluster.host}&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cluster=SecondaryCluster&lt;/argument&gt; &lt;argument&gt;-Dcoherence.clusterport=${secondary.cluster.port}&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; primary-cohql - Runs a CohQL session for the SecondaryCluster <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;secondary-cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;secondary-cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;federation-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.log.level&lt;/key&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.cluster&lt;/key&gt; &lt;value&gt;SecondaryCluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;${secondary.cluster.host}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.clusterport&lt;/key&gt; &lt;value&gt;${secondary.cluster.port}&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Federation Configuration Federated caching is configured using Coherence configuration files and requires no changes to application code. There are two areas that require configuration for Federation: An operational override file is used to configure federation participants and the federation topology. A cache configuration file is used to create federated caches schemes. A federated cache is a type of partitioned cache service and is managed by a federated cache service instance. The following cache configuration file is used to define the Federated service: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;key-type&gt;java.lang.Integer&lt;/key-type&gt; &lt;value-type&gt;java.lang.String&lt;/value-type&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;federated-scheme&gt; &lt;scheme-name&gt;federated&lt;/scheme-name&gt; &lt;service-name&gt;FederatedPartitionedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;topologies&gt; &lt;topology&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;/topology&gt; &lt;/topologies&gt; &lt;/federated-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A cache-mapping for all caches (*) to map to a scheme called federated The federated-scheme in a similar way to a distributed-scheme A topology for the federated-scheme. The default topology is active-active so this element is not required and just included for completeness. The following operational configuration file is used to define the participants and topology: <markup lang=\"xml\" >&lt;federation-config&gt; &lt;participants&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"primary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"primary.cluster.port\"&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;participant&gt; &lt;name system-property=\"primary.cluster\"&gt;SecondaryCluster&lt;/name&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"secondary.cluster.host\"&gt;127.0.0.1&lt;/address&gt; &lt;port system-property=\"secondary.cluster.port\"&gt;7575&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/participant&gt; &lt;/participants&gt; &lt;topology-definitions&gt; &lt;active-active&gt; &lt;name&gt;MyTopology&lt;/name&gt; &lt;active system-property=\"primary.cluster\"&gt;PrimaryCluster&lt;/active&gt; &lt;active system-property=\"secondary.cluster\"&gt;SecondaryCluster&lt;/active&gt; &lt;/active-active&gt; &lt;/topology-definitions&gt; &lt;/federation-config&gt; PrimaryCluster participant with its host and port for the cluster Name Service SecondaryCluster participant with its host and port for the cluster Name Service Topology that defines an active-active configuration between clusters. This is the default and not strictly required. Run the Example After you have built the project as described earlier in this document you can run via Maven or Gradle. Maven Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P primary-storage and <markup lang=\"bash\" >./mvnw exec:exec -P secondary-storage Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./mvnw exec:java -P primary-cohql and <markup lang=\"bash\" >./mvnw exec:java -P secondary-cohql Gradle Start a DefaultCache server for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runServerPrimary and <markup lang=\"bash\" >./gradlew runServerSecondary Start a CohQL session for the primary and secondary clusters in separate terminals. <markup lang=\"bash\" >./gradlew runCohQLPrimary --console=plain and <markup lang=\"bash\" >./gradlew runCohQLSecondary --console=plain Run the following commands to exercise Federation In each of the CohQL sessions, run the following command to verify the caches are empty in each cluster: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 In the first (PrimaryCluster) CohQL session, add an entry to the cache test : <markup lang=\"bash\" >CohQL&gt; insert into test key(1) value(\"Tim\") CohQL&gt; select key(), value() from test Results [1, \"Tim\"] In the second (SecondaryCluster) CohQL session, verify the entry was sent from the PrimaryCluster and then update the name to Timothy . As the clusters are active-active , the changes will be sent back to the primary cluster. <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Tim\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] In the first (PrimaryCluster) CohQL session, verify the entry was changed via the change in the SecondaryCluster , then delete the entry and confirm it was deleted in the SecondaryCluster <markup lang=\"bash\" >CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; update 'test' set value() = \"Timothy\" Results 1: true CohQL&gt; select key(), value() from test Results [1, \"Timothy\"] CohQL&gt; delete from test Results CohQL&gt; select key(), value() from test Results Continue experimenting: You can continue to experiment by inserting, updating or removing data using various CohQL commands. For detailed information on how to use CohQL, please visit the chapter Using Coherence Query Language in the Coherence reference guide. Monitor Federation If you want to monitor Federation, you can do this via the Coherence VisualVM Plugin. See here for how to install the Plugin if you have VisualVM already, otherwise visit https://visualvm.github.io/ to download and install VisualVM . Once you have installed the plugin, you can click on one of the DefaultCacheServer process, and you will see the Federation tab as shown below: Summary In this guide you walked through the steps to use Coherence Federation by using Coherence Query Language (CohQL) to insert, update and remove data in Federated clusters. See Also Federation Documentation Using Coherence Query Language ",
            "title": "Federation"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " An exact topic mapping maps a specific topic name to a topic scheme definition. An application must provide the exact name as specified in the mapping to use a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below creates a single topic mapping that maps the topic name exampleTopic to a paged-topic-scheme definition with the scheme name `example-topic-scheme . <markup lang=\"xml\" title=\"Sample Exact Topic Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;example-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Using Exact Topic Mappings"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Name pattern topic mappings allow applications to use patterns when specifying a topic name. Patterns use the asterisk ( * ) wildcard. Name patterns alleviate an application from having to know the exact name of a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below a topic mappings using the wildcard ( \\* ) to map any topic name with the prefix account- to a paged-topic-scheme definition with the scheme name account-topic-scheme . <markup lang=\"xml\" title=\"Sample Topic Name Pattern Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;account-*&lt;/topic-name&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;AccountDistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Using Named Pattern Topic Mappings"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " A topic can have zero, one, or more durable subscriber groups defined in the topic-mapping for the topic. The subscriber group(s) are created along with the topic and are therefore ensured to exist before any data is published to the topic. A subscriber group does not have to be defined on a topic’s topic-mapping for a subscriber to be able to join its group. Groups can be created and destroyed dynamically at runtime in application code. The example below adds the subscriber group durableSubscription to the exampleTopic mapping. The subscriber-groups element can contain multiple subscriber-group elements to add as many groups as the application requires. <markup lang=\"xml\" title=\"Sample Durable Subscriber Group\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;durableSubscription&lt;/name&gt; &lt;/subscriber-group&gt; &lt;subscriber-groups&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Subscriber Group"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " A topic mapping maps a topic name to a paged topic scheme definition.The mappings provide a level of separation between applications and the underlying topic definitions. The separation allows topic implementations to be changed as required without having to change application code. Topic mappings have optional initialization parameters that are applied to the underlying paged topic scheme definition. Topic mappings are defined using a &lt;topic-mapping&gt; element within the &lt;topic-scheme-mapping&gt; node. Any number of topic mappings can be created. The topic mapping must include the topic name, and the scheme name to which the topic name is mapped. Using Exact Topic Mappings An exact topic mapping maps a specific topic name to a topic scheme definition. An application must provide the exact name as specified in the mapping to use a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below creates a single topic mapping that maps the topic name exampleTopic to a paged-topic-scheme definition with the scheme name `example-topic-scheme . <markup lang=\"xml\" title=\"Sample Exact Topic Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;example-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Using Named Pattern Topic Mappings Name pattern topic mappings allow applications to use patterns when specifying a topic name. Patterns use the asterisk ( * ) wildcard. Name patterns alleviate an application from having to know the exact name of a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below a topic mappings using the wildcard ( \\* ) to map any topic name with the prefix account- to a paged-topic-scheme definition with the scheme name account-topic-scheme . <markup lang=\"xml\" title=\"Sample Topic Name Pattern Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;account-*&lt;/topic-name&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;AccountDistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Subscriber Group A topic can have zero, one, or more durable subscriber groups defined in the topic-mapping for the topic. The subscriber group(s) are created along with the topic and are therefore ensured to exist before any data is published to the topic. A subscriber group does not have to be defined on a topic’s topic-mapping for a subscriber to be able to join its group. Groups can be created and destroyed dynamically at runtime in application code. The example below adds the subscriber group durableSubscription to the exampleTopic mapping. The subscriber-groups element can contain multiple subscriber-group elements to add as many groups as the application requires. <markup lang=\"xml\" title=\"Sample Durable Subscriber Group\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;durableSubscription&lt;/name&gt; &lt;/subscriber-group&gt; &lt;subscriber-groups&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; ",
            "title": "Defining Topic Mappings"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The &lt;paged-topic-scheme&gt; element is used to define distributed topics. A distributed topic utilizes a distributed (partitioned) topic service instance. Any number of distributed topics can be defined in a cache configuration file. The example below defines a basic distributed topic that uses distributed-topic as the scheme name and is mapped to the topic name example-topic . The &lt;autostart&gt; element is set to true to start the service on a cache server node. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;example-topic&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A paged-topic-scheme has various configuration elements, discussed further below. ",
            "title": "Sample Distributed Topic Definition"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. ",
            "title": "Very Small Channel Count"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. ",
            "title": "Vary Large Channel Count"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Channels are used by topics both as a way to increase parallel processing of messages and also to retain published ordering. The number of channels in a topic is configurable using the &lt;channel-count&gt; sub-element of the &lt;paged-topic-scheme&gt; . The default number of channels is based on the partition count of the underlying cache service used by the topic. With the Coherence default partition count of 257 giving a default topic channel count of 17. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;channel-count&gt;3&lt;/channel-count&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to have a channel count of 3 . Whether increasing or decreasing the channel count makes sense depends on how an application will publish messages, and the ordering guarantees required. To help show the pros and cons we&#8217;ll look at both the very small and the very big. Very Small Channel Count The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. Vary Large Channel Count Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. ",
            "title": "Channel Count"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " By default, messages in a topic are removed after all the currently connected anonymous subscribers and all the subscriber groups have processed and committed a message. This means a message can only be read once. When an anonymous subscriber connects to a topic it starts receiving messages with the next message published to the topic after it connects. The subscribers in a new subscriber group will also receive messages that were published after the group was created. Sometimes it is desirable to have a topic behave more like a persistent log structure, where a subscriber, or subscriber group, in an application can receive the ordered messages from the log, then go back and re-read them as required. The &lt;retain-consumed&gt; sub-element of the &lt;paged-topic-scheme&gt; element controls this behaviour. The &lt;retain-consumed&gt; sub-element&#8217;s value is a boolean, true to retain elements, false to remove consumed elements. In topics configured with &lt;retain-consumed&gt; set to true , new anonymous subscribers will start to receive messages from the beginning (head) of the topic, rather than the tail; new subscriber groups will also start from the head of the topic. Messages in a retained topic are never deleted (unless the topic is also configured with expiry). <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-consumed&gt;true&lt;/retain-consumed&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to retain messages. Topics configuration, like cache configuration, supports parameterizing certain configuration elements on a per-topic basis using parameter macros. For example, the configuration below has tow topic mappings, topic-one and topic-two . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-one&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;keep-messages&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-two&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-values&gt;{keep-messages false}&lt;/retain-values&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The topic-one mapping contains an init-param named keep-messages with a value of true The topic-two mapping contains no init-params The topic scheme contains the retain-values sub-element, but instead of a simple boolean value it uses a macro (a value inside curly brackets). The {keep-messages false} macro says to use the keep-messages parameter for the value of the retain-values sub-element, and default to false if keep-messages is not set. So, topic-one , which sets keep-values to true will use a configuration that retains messages, whereas topic-two has no init-params so keep-values will not be set and will default to `false. ",
            "title": "Retaining Messages (Topics as Persistent Logs)"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The &lt;serializer&gt; sub-element of &lt;paged-topic-scheme&gt; element enables specifying predefined serializers pof or java (default), or a custom Serializer implementation. The serializer is used to serialize and deserialize the message payload. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;serializer&gt;pof&lt;/serializer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets the serializer for all topics mapped to the distributed-topic scheme to POF. ",
            "title": "Topic Values Serializer"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The subscriber-timeout sub-element configures the maximum amount of time that can elapse after a subscribers that is part of a subscriber group polls for messages before that subscriber is considered dead. Each time a subscriber in a group calls on of the receive methods it sends a heartbeat to the server (heartbeats can also be sent manually by application code during long-running processing). If the server does not receive a heartbeat within the timeout the subscriber is considered dead and any channels that it owned will be redistributed to any remaining subscribers. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;subscriber-timeout&gt;1m&lt;/subscriber-timeout&gt; &lt;/paged-topic-scheme&gt; The subscriber timeout has been set to 1 minute. The purpose of timing out subscribers is to stop channels being starved of subscriptions due to badly behaved, dead, or deadlocked subscribers. If a dead subscriber stayed connected and its channels were not redistributed, any messages published to those channels would never be processed. A timed-out subscriber is not closed, application code that calls receive on a timed-ot subscriber will cause that subscriber to reconnect and be re-initialised with new channel ownership. The default value for the subscriber timeout is five minutes. This should be sufficient for most applications unless the message processing code takes a very long time, for example it talks to other external slow system. ",
            "title": "Subscriber Timeout"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " The &lt;storage&gt; sub-element allows specification of on-heap , ramjournal and flashjournal to store the messages and metadata for a topic. The default is to use on-heap storage. The ramjournal and flashjournal options use the Elastic Data Feature to, which is a commercial only Coherence feature. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;on-heap&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to on-heap , so topic data is stored in memory. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;flashjournal&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to flashjournal , so topic data is stored on disc, using Coherence&#8217;s commercial Elastic Data feature. ",
            "title": "Storage Options for Topic Values"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " A paged topic scheme configures a topic that stores data in pages. This is how Coherence scales topic data across the cluster, by distributing pages across storage enabled members of the cluster. Each channel has pages and each page holds a number of messages. The page size can be configured to determine how many messages can fit into a page. Publishers publish messages to the tail page in a channel, and when that page is full the page is sealed, and the next page becomes the tail. Page size is configured in the &lt;page-size&gt; element of the &lt;paged-topic-scheme&gt; in the cache configuration file. The format of the page size value is a positive integer, optionally followed by a byte size suffix (B, KB, MB). When the page size is configured with a value using a byte size suffix, the size of the serialized message payload is used to determine the page size. In this case different pages may contain different numbers of messages if the serialized message size is different. In the example below, the page size is set to 10 mega-bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;10MB&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; If a page size element has a value without a byte size suffix it is treated as a number of messages rather than a binary size. For example, using the configuration below, each page will have a maximum size of 100 messages, regardless of the message&#8217;s size in bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;100&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; Pages are used by subscribers as a way to batch receive calls. When receive is called on a subscriber, the server can return a batch of messages, upto a whole page, in the response. The subscriber then stores those messages locally and uses them to respond to further receive calls without needing to make a remote request back to the page. The default maximum batch size used in responses is the minimum of the page size, and the MTU of the network interface being used by Coherence, so that a batch fits into a network packet. As with many configuration elements, there are pros and cons to making the value too small or too large. A page that is too large will store a lot of data in a single page on a single storage member before moving to the next page, causing lumpy data distribution. Pages that are too small will cause an excessive number of pages wasting storage where the data structures created per page may exceed the storage used by the messages themselves. The default page size is 1Mb. Page numbers are stored in a Java long value, so an application is unlikely to consume all 9,223,372,036,854,775,807 pages. Using the default page size of 1Mb, that is 9 peta-bytes of messages. Due to the way publishing works, a full page usually slightly exceeds the configured page size. This is because during publishing, messages are accepted into a page until the pages size is &gt;= the configured maximum. For example, if a page has been configured with a maximum size of 1Mb and currently has 900kb of messages, so is below the maximum size. If the next message published that is 150kb, that message will still be accepted, as the page has some free space, pushing the total size to 1.5Mb. The page would then be considered sealed and accept no more messages, the next message published would go to the next page. ",
            "title": "Page Size"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Adding a &lt;high-units&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the storage size of the values retained for the topic. The topic is considered full if this storage limit is reached. Not exceeding this high water-mark is managed by using flow control. When subscriber(s) are lagging in processing outstanding values retained on the topic, the publishers are throttled until there is space available. See Managing the Publisher Flow Control to Place Upper Bound on Topics Storage. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;high-units&gt;100MB&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The &lt;high-units&gt; has been set to 100Mb so the total size of the topic will not exceed 100 mega-bytes. ",
            "title": "Size Limited Topics"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Adding a &lt;expiry-delay&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the length of time that the published messages live in a topic, waiting to be received by a Subscriber. Once the expiry delay is past, those expired messages will never be received by subscribers. The default expiry-delay is zero, meaning elements never expire. Messages will be expired regardless of the type of subscriber used. For example, even with a durable subscriber group, expired messaged will not be received after their expiry delay has passed. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;expiry-delay&gt;30d&lt;/expiry-delay&gt; &lt;/paged-topic-scheme&gt; The &lt;expiry-delay&gt; has been set to 30 days, so messages will be removed from the topic 30 days after publishing, regardless of whether they have been received by subscribers. ",
            "title": "Topics with Expiring Values"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Topic schemes are used to define the topic services that are available to an application.Topic schemes provide a declarative mechanism that allows topics to be defined independent of the applications that use them. This removes the responsibility of defining topics from the application and allows topics to change without having to change an application&#8217;s code. Topic schemes also promote topic definition reuse by allowing many applications to use the same topic definition. Topic schemes are defined within the &lt;caching-schemes&gt; element. A &lt;paged-topic-scheme&gt; scheme element and its properties are used to define a topic of that type. Sample Distributed Topic Definition The &lt;paged-topic-scheme&gt; element is used to define distributed topics. A distributed topic utilizes a distributed (partitioned) topic service instance. Any number of distributed topics can be defined in a cache configuration file. The example below defines a basic distributed topic that uses distributed-topic as the scheme name and is mapped to the topic name example-topic . The &lt;autostart&gt; element is set to true to start the service on a cache server node. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;example-topic&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A paged-topic-scheme has various configuration elements, discussed further below. Channel Count Channels are used by topics both as a way to increase parallel processing of messages and also to retain published ordering. The number of channels in a topic is configurable using the &lt;channel-count&gt; sub-element of the &lt;paged-topic-scheme&gt; . The default number of channels is based on the partition count of the underlying cache service used by the topic. With the Coherence default partition count of 257 giving a default topic channel count of 17. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;channel-count&gt;3&lt;/channel-count&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to have a channel count of 3 . Whether increasing or decreasing the channel count makes sense depends on how an application will publish messages, and the ordering guarantees required. To help show the pros and cons we&#8217;ll look at both the very small and the very big. Very Small Channel Count The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. Vary Large Channel Count Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. Retaining Messages (Topics as Persistent Logs) By default, messages in a topic are removed after all the currently connected anonymous subscribers and all the subscriber groups have processed and committed a message. This means a message can only be read once. When an anonymous subscriber connects to a topic it starts receiving messages with the next message published to the topic after it connects. The subscribers in a new subscriber group will also receive messages that were published after the group was created. Sometimes it is desirable to have a topic behave more like a persistent log structure, where a subscriber, or subscriber group, in an application can receive the ordered messages from the log, then go back and re-read them as required. The &lt;retain-consumed&gt; sub-element of the &lt;paged-topic-scheme&gt; element controls this behaviour. The &lt;retain-consumed&gt; sub-element&#8217;s value is a boolean, true to retain elements, false to remove consumed elements. In topics configured with &lt;retain-consumed&gt; set to true , new anonymous subscribers will start to receive messages from the beginning (head) of the topic, rather than the tail; new subscriber groups will also start from the head of the topic. Messages in a retained topic are never deleted (unless the topic is also configured with expiry). <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-consumed&gt;true&lt;/retain-consumed&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to retain messages. Topics configuration, like cache configuration, supports parameterizing certain configuration elements on a per-topic basis using parameter macros. For example, the configuration below has tow topic mappings, topic-one and topic-two . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-one&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;keep-messages&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-two&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-values&gt;{keep-messages false}&lt;/retain-values&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The topic-one mapping contains an init-param named keep-messages with a value of true The topic-two mapping contains no init-params The topic scheme contains the retain-values sub-element, but instead of a simple boolean value it uses a macro (a value inside curly brackets). The {keep-messages false} macro says to use the keep-messages parameter for the value of the retain-values sub-element, and default to false if keep-messages is not set. So, topic-one , which sets keep-values to true will use a configuration that retains messages, whereas topic-two has no init-params so keep-values will not be set and will default to `false. Topic Values Serializer The &lt;serializer&gt; sub-element of &lt;paged-topic-scheme&gt; element enables specifying predefined serializers pof or java (default), or a custom Serializer implementation. The serializer is used to serialize and deserialize the message payload. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;serializer&gt;pof&lt;/serializer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets the serializer for all topics mapped to the distributed-topic scheme to POF. Subscriber Timeout The subscriber-timeout sub-element configures the maximum amount of time that can elapse after a subscribers that is part of a subscriber group polls for messages before that subscriber is considered dead. Each time a subscriber in a group calls on of the receive methods it sends a heartbeat to the server (heartbeats can also be sent manually by application code during long-running processing). If the server does not receive a heartbeat within the timeout the subscriber is considered dead and any channels that it owned will be redistributed to any remaining subscribers. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;subscriber-timeout&gt;1m&lt;/subscriber-timeout&gt; &lt;/paged-topic-scheme&gt; The subscriber timeout has been set to 1 minute. The purpose of timing out subscribers is to stop channels being starved of subscriptions due to badly behaved, dead, or deadlocked subscribers. If a dead subscriber stayed connected and its channels were not redistributed, any messages published to those channels would never be processed. A timed-out subscriber is not closed, application code that calls receive on a timed-ot subscriber will cause that subscriber to reconnect and be re-initialised with new channel ownership. The default value for the subscriber timeout is five minutes. This should be sufficient for most applications unless the message processing code takes a very long time, for example it talks to other external slow system. Storage Options for Topic Values The &lt;storage&gt; sub-element allows specification of on-heap , ramjournal and flashjournal to store the messages and metadata for a topic. The default is to use on-heap storage. The ramjournal and flashjournal options use the Elastic Data Feature to, which is a commercial only Coherence feature. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;on-heap&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to on-heap , so topic data is stored in memory. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;flashjournal&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to flashjournal , so topic data is stored on disc, using Coherence&#8217;s commercial Elastic Data feature. Page Size A paged topic scheme configures a topic that stores data in pages. This is how Coherence scales topic data across the cluster, by distributing pages across storage enabled members of the cluster. Each channel has pages and each page holds a number of messages. The page size can be configured to determine how many messages can fit into a page. Publishers publish messages to the tail page in a channel, and when that page is full the page is sealed, and the next page becomes the tail. Page size is configured in the &lt;page-size&gt; element of the &lt;paged-topic-scheme&gt; in the cache configuration file. The format of the page size value is a positive integer, optionally followed by a byte size suffix (B, KB, MB). When the page size is configured with a value using a byte size suffix, the size of the serialized message payload is used to determine the page size. In this case different pages may contain different numbers of messages if the serialized message size is different. In the example below, the page size is set to 10 mega-bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;10MB&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; If a page size element has a value without a byte size suffix it is treated as a number of messages rather than a binary size. For example, using the configuration below, each page will have a maximum size of 100 messages, regardless of the message&#8217;s size in bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;100&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; Pages are used by subscribers as a way to batch receive calls. When receive is called on a subscriber, the server can return a batch of messages, upto a whole page, in the response. The subscriber then stores those messages locally and uses them to respond to further receive calls without needing to make a remote request back to the page. The default maximum batch size used in responses is the minimum of the page size, and the MTU of the network interface being used by Coherence, so that a batch fits into a network packet. As with many configuration elements, there are pros and cons to making the value too small or too large. A page that is too large will store a lot of data in a single page on a single storage member before moving to the next page, causing lumpy data distribution. Pages that are too small will cause an excessive number of pages wasting storage where the data structures created per page may exceed the storage used by the messages themselves. The default page size is 1Mb. Page numbers are stored in a Java long value, so an application is unlikely to consume all 9,223,372,036,854,775,807 pages. Using the default page size of 1Mb, that is 9 peta-bytes of messages. Due to the way publishing works, a full page usually slightly exceeds the configured page size. This is because during publishing, messages are accepted into a page until the pages size is &gt;= the configured maximum. For example, if a page has been configured with a maximum size of 1Mb and currently has 900kb of messages, so is below the maximum size. If the next message published that is 150kb, that message will still be accepted, as the page has some free space, pushing the total size to 1.5Mb. The page would then be considered sealed and accept no more messages, the next message published would go to the next page. Size Limited Topics Adding a &lt;high-units&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the storage size of the values retained for the topic. The topic is considered full if this storage limit is reached. Not exceeding this high water-mark is managed by using flow control. When subscriber(s) are lagging in processing outstanding values retained on the topic, the publishers are throttled until there is space available. See Managing the Publisher Flow Control to Place Upper Bound on Topics Storage. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;high-units&gt;100MB&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The &lt;high-units&gt; has been set to 100Mb so the total size of the topic will not exceed 100 mega-bytes. Topics with Expiring Values Adding a &lt;expiry-delay&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the length of time that the published messages live in a topic, waiting to be received by a Subscriber. Once the expiry delay is past, those expired messages will never be received by subscribers. The default expiry-delay is zero, meaning elements never expire. Messages will be expired regardless of the type of subscriber used. For example, even with a durable subscriber group, expired messaged will not be received after their expiry delay has passed. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;expiry-delay&gt;30d&lt;/expiry-delay&gt; &lt;/paged-topic-scheme&gt; The &lt;expiry-delay&gt; has been set to 30 days, so messages will be removed from the topic 30 days after publishing, regardless of whether they have been received by subscribers. ",
            "title": "Defining a Distributed Topic Scheme"
        },
        {
            "location": "/docs/topics/02_configuring_topics",
            "text": " Coherence topics are configured in the cache configuration file. This section includes the following topics: Defining Topic Mappings - A topic mapping maps a topic name to a paged topic scheme definition. Define Subscriber Groups - Subscriber groups can be defined in topic mappings Defining a Distributed Topic Scheme - Topic schemes are used to define the topic services that are available to an application. Channel Count - configure the number of channels in a topic Retaining Messages (Topics as Persistent Logs) - retain messages after consumption (rewindable topics) Configure the message Serializer Configure the Subscriber Timeout - configure the group subscriber timout Storage Options for Topic Values Page Size - configure the size of a page in a paged topic Size Limited Topics Expiring Messages - expiring messages Defining Topic Mappings A topic mapping maps a topic name to a paged topic scheme definition.The mappings provide a level of separation between applications and the underlying topic definitions. The separation allows topic implementations to be changed as required without having to change application code. Topic mappings have optional initialization parameters that are applied to the underlying paged topic scheme definition. Topic mappings are defined using a &lt;topic-mapping&gt; element within the &lt;topic-scheme-mapping&gt; node. Any number of topic mappings can be created. The topic mapping must include the topic name, and the scheme name to which the topic name is mapped. Using Exact Topic Mappings An exact topic mapping maps a specific topic name to a topic scheme definition. An application must provide the exact name as specified in the mapping to use a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below creates a single topic mapping that maps the topic name exampleTopic to a paged-topic-scheme definition with the scheme name `example-topic-scheme . <markup lang=\"xml\" title=\"Sample Exact Topic Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;example-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Using Named Pattern Topic Mappings Name pattern topic mappings allow applications to use patterns when specifying a topic name. Patterns use the asterisk ( * ) wildcard. Name patterns alleviate an application from having to know the exact name of a topic. The slash (/) and colon (:) are reserved characters and cannot be used in topic names. The example below a topic mappings using the wildcard ( \\* ) to map any topic name with the prefix account- to a paged-topic-scheme definition with the scheme name account-topic-scheme . <markup lang=\"xml\" title=\"Sample Topic Name Pattern Mapping\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;account-*&lt;/topic-name&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;account-topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;AccountDistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Subscriber Group A topic can have zero, one, or more durable subscriber groups defined in the topic-mapping for the topic. The subscriber group(s) are created along with the topic and are therefore ensured to exist before any data is published to the topic. A subscriber group does not have to be defined on a topic’s topic-mapping for a subscriber to be able to join its group. Groups can be created and destroyed dynamically at runtime in application code. The example below adds the subscriber group durableSubscription to the exampleTopic mapping. The subscriber-groups element can contain multiple subscriber-group elements to add as many groups as the application requires. <markup lang=\"xml\" title=\"Sample Durable Subscriber Group\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;exampleTopic&lt;/topic-name&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;subscriber-groups&gt; &lt;subscriber-group&gt; &lt;name&gt;durableSubscription&lt;/name&gt; &lt;/subscriber-group&gt; &lt;subscriber-groups&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Defining a Distributed Topic Scheme Topic schemes are used to define the topic services that are available to an application.Topic schemes provide a declarative mechanism that allows topics to be defined independent of the applications that use them. This removes the responsibility of defining topics from the application and allows topics to change without having to change an application&#8217;s code. Topic schemes also promote topic definition reuse by allowing many applications to use the same topic definition. Topic schemes are defined within the &lt;caching-schemes&gt; element. A &lt;paged-topic-scheme&gt; scheme element and its properties are used to define a topic of that type. Sample Distributed Topic Definition The &lt;paged-topic-scheme&gt; element is used to define distributed topics. A distributed topic utilizes a distributed (partitioned) topic service instance. Any number of distributed topics can be defined in a cache configuration file. The example below defines a basic distributed topic that uses distributed-topic as the scheme name and is mapped to the topic name example-topic . The &lt;autostart&gt; element is set to true to start the service on a cache server node. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;example-topic&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; A paged-topic-scheme has various configuration elements, discussed further below. Channel Count Channels are used by topics both as a way to increase parallel processing of messages and also to retain published ordering. The number of channels in a topic is configurable using the &lt;channel-count&gt; sub-element of the &lt;paged-topic-scheme&gt; . The default number of channels is based on the partition count of the underlying cache service used by the topic. With the Coherence default partition count of 257 giving a default topic channel count of 17. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;channel-count&gt;3&lt;/channel-count&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to have a channel count of 3 . Whether increasing or decreasing the channel count makes sense depends on how an application will publish messages, and the ordering guarantees required. To help show the pros and cons we&#8217;ll look at both the very small and the very big. Very Small Channel Count The smallest channel count possible would be one. With one channel all messages published will go to this single channel. The channel will have a single tail location where messages are published to and subscribed from. If there is only a single publisher that requires ordering of all messages it publishes then a single channel would work. If there are multiple publishers then with a single channel all publishers will publish to this one channel and there will be contention on the tail of the topic if the publishers all try to publish at the same time. If the multiple publishers require global ordering of messages across publishers then one channel will give this at the cost of performance due to contention. If the publishers are not publishing very often, the contention would be reduced. Multiple anonymous subscribers can all subscribe to a single channel and receive messages in order. Using a subscriber group on a single channel topic does not allow multiple subscribers in the group to scale out message processing. IN a subscriber group, subscribers own channels that they subscribe to, so with only a single channel, only one subscriber in the group can receive messages. Vary Large Channel Count Setting a very large channel count (100s or 1000s) whilst possible would be impractical. One reason is that various data structures used by topics are created per-channel (or per-partition per-channel) so having a very large number of channels will use more resources, such as heap, to maintain these structures. A larger number of channels would have less contention where many publishers are publishing messages at the same time. A larger number of channels would allow more subscribers in a group to process messages in parallel, assuming that either there are enough publishers publishing to all of those channels, or the publishers are configured to publish to multiple channels. In most cases the default channel count should be about right. Applications may wish to slightly reduce or increase this and there are use-cases where one channel may be suitable. An excessively large number of channels is probably not justifiable. Retaining Messages (Topics as Persistent Logs) By default, messages in a topic are removed after all the currently connected anonymous subscribers and all the subscriber groups have processed and committed a message. This means a message can only be read once. When an anonymous subscriber connects to a topic it starts receiving messages with the next message published to the topic after it connects. The subscribers in a new subscriber group will also receive messages that were published after the group was created. Sometimes it is desirable to have a topic behave more like a persistent log structure, where a subscriber, or subscriber group, in an application can receive the ordered messages from the log, then go back and re-read them as required. The &lt;retain-consumed&gt; sub-element of the &lt;paged-topic-scheme&gt; element controls this behaviour. The &lt;retain-consumed&gt; sub-element&#8217;s value is a boolean, true to retain elements, false to remove consumed elements. In topics configured with &lt;retain-consumed&gt; set to true , new anonymous subscribers will start to receive messages from the beginning (head) of the topic, rather than the tail; new subscriber groups will also start from the head of the topic. Messages in a retained topic are never deleted (unless the topic is also configured with expiry). <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-consumed&gt;true&lt;/retain-consumed&gt; &lt;/paged-topic-scheme&gt; The example above configures the topics mapped to the distributed-topic scheme to retain messages. Topics configuration, like cache configuration, supports parameterizing certain configuration elements on a per-topic basis using parameter macros. For example, the configuration below has tow topic mappings, topic-one and topic-two . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;topic-scheme-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-one&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;keep-messages&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/topic-mapping&gt; &lt;topic-mapping&gt; &lt;topic-name&gt;topic-two&lt;/topic-name&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;/topic-mapping&gt; &lt;/topic-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;distributed-topic&lt;/scheme-name&gt; &lt;service-name&gt;DistributedTopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;retain-values&gt;{keep-messages false}&lt;/retain-values&gt; &lt;/paged-topic-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The topic-one mapping contains an init-param named keep-messages with a value of true The topic-two mapping contains no init-params The topic scheme contains the retain-values sub-element, but instead of a simple boolean value it uses a macro (a value inside curly brackets). The {keep-messages false} macro says to use the keep-messages parameter for the value of the retain-values sub-element, and default to false if keep-messages is not set. So, topic-one , which sets keep-values to true will use a configuration that retains messages, whereas topic-two has no init-params so keep-values will not be set and will default to `false. Topic Values Serializer The &lt;serializer&gt; sub-element of &lt;paged-topic-scheme&gt; element enables specifying predefined serializers pof or java (default), or a custom Serializer implementation. The serializer is used to serialize and deserialize the message payload. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;serializer&gt;pof&lt;/serializer&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets the serializer for all topics mapped to the distributed-topic scheme to POF. Subscriber Timeout The subscriber-timeout sub-element configures the maximum amount of time that can elapse after a subscribers that is part of a subscriber group polls for messages before that subscriber is considered dead. Each time a subscriber in a group calls on of the receive methods it sends a heartbeat to the server (heartbeats can also be sent manually by application code during long-running processing). If the server does not receive a heartbeat within the timeout the subscriber is considered dead and any channels that it owned will be redistributed to any remaining subscribers. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;subscriber-timeout&gt;1m&lt;/subscriber-timeout&gt; &lt;/paged-topic-scheme&gt; The subscriber timeout has been set to 1 minute. The purpose of timing out subscribers is to stop channels being starved of subscriptions due to badly behaved, dead, or deadlocked subscribers. If a dead subscriber stayed connected and its channels were not redistributed, any messages published to those channels would never be processed. A timed-out subscriber is not closed, application code that calls receive on a timed-ot subscriber will cause that subscriber to reconnect and be re-initialised with new channel ownership. The default value for the subscriber timeout is five minutes. This should be sufficient for most applications unless the message processing code takes a very long time, for example it talks to other external slow system. Storage Options for Topic Values The &lt;storage&gt; sub-element allows specification of on-heap , ramjournal and flashjournal to store the messages and metadata for a topic. The default is to use on-heap storage. The ramjournal and flashjournal options use the Elastic Data Feature to, which is a commercial only Coherence feature. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;on-heap&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to on-heap , so topic data is stored in memory. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" > &lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;storage&gt;flashjournal&lt;/storage&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/paged-topic-scheme&gt; the example above sets storage to flashjournal , so topic data is stored on disc, using Coherence&#8217;s commercial Elastic Data feature. Page Size A paged topic scheme configures a topic that stores data in pages. This is how Coherence scales topic data across the cluster, by distributing pages across storage enabled members of the cluster. Each channel has pages and each page holds a number of messages. The page size can be configured to determine how many messages can fit into a page. Publishers publish messages to the tail page in a channel, and when that page is full the page is sealed, and the next page becomes the tail. Page size is configured in the &lt;page-size&gt; element of the &lt;paged-topic-scheme&gt; in the cache configuration file. The format of the page size value is a positive integer, optionally followed by a byte size suffix (B, KB, MB). When the page size is configured with a value using a byte size suffix, the size of the serialized message payload is used to determine the page size. In this case different pages may contain different numbers of messages if the serialized message size is different. In the example below, the page size is set to 10 mega-bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;10MB&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; If a page size element has a value without a byte size suffix it is treated as a number of messages rather than a binary size. For example, using the configuration below, each page will have a maximum size of 100 messages, regardless of the message&#8217;s size in bytes: <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;page-size&gt;100&lt;/page-size&gt; &lt;/paged-topic-scheme&gt; Pages are used by subscribers as a way to batch receive calls. When receive is called on a subscriber, the server can return a batch of messages, upto a whole page, in the response. The subscriber then stores those messages locally and uses them to respond to further receive calls without needing to make a remote request back to the page. The default maximum batch size used in responses is the minimum of the page size, and the MTU of the network interface being used by Coherence, so that a batch fits into a network packet. As with many configuration elements, there are pros and cons to making the value too small or too large. A page that is too large will store a lot of data in a single page on a single storage member before moving to the next page, causing lumpy data distribution. Pages that are too small will cause an excessive number of pages wasting storage where the data structures created per page may exceed the storage used by the messages themselves. The default page size is 1Mb. Page numbers are stored in a Java long value, so an application is unlikely to consume all 9,223,372,036,854,775,807 pages. Using the default page size of 1Mb, that is 9 peta-bytes of messages. Due to the way publishing works, a full page usually slightly exceeds the configured page size. This is because during publishing, messages are accepted into a page until the pages size is &gt;= the configured maximum. For example, if a page has been configured with a maximum size of 1Mb and currently has 900kb of messages, so is below the maximum size. If the next message published that is 150kb, that message will still be accepted, as the page has some free space, pushing the total size to 1.5Mb. The page would then be considered sealed and accept no more messages, the next message published would go to the next page. Size Limited Topics Adding a &lt;high-units&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the storage size of the values retained for the topic. The topic is considered full if this storage limit is reached. Not exceeding this high water-mark is managed by using flow control. When subscriber(s) are lagging in processing outstanding values retained on the topic, the publishers are throttled until there is space available. See Managing the Publisher Flow Control to Place Upper Bound on Topics Storage. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;high-units&gt;100MB&lt;/high-units&gt; &lt;/paged-topic-scheme&gt; The &lt;high-units&gt; has been set to 100Mb so the total size of the topic will not exceed 100 mega-bytes. Topics with Expiring Values Adding a &lt;expiry-delay&gt; sub-element to &lt;paged-topic-scheme&gt; element limits the length of time that the published messages live in a topic, waiting to be received by a Subscriber. Once the expiry delay is past, those expired messages will never be received by subscribers. The default expiry-delay is zero, meaning elements never expire. Messages will be expired regardless of the type of subscriber used. For example, even with a durable subscriber group, expired messaged will not be received after their expiry delay has passed. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;paged-topic-scheme&gt; &lt;scheme-name&gt;topic-scheme&lt;/scheme-name&gt; &lt;service-name&gt;TopicService&lt;/service-name&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;expiry-delay&gt;30d&lt;/expiry-delay&gt; &lt;/paged-topic-scheme&gt; The &lt;expiry-delay&gt; has been set to 30 days, so messages will be removed from the topic 30 days after publishing, regardless of whether they have been received by subscribers. ",
            "title": "Configure Coherence Topics"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " In this example you will utilize the built-in aggregators such as count , sum , min , average and top on orders and customers maps. You will also use the Aggregators class and its helpers to simplify aggregator usage. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " The data model consists of the following classes in two maps, customers and orders Customer - Represents a customer Order - Represents and order for a customer and contains order lines OrderLine - Represents an individual order line for an order Address - Represents an address for a customer ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " In this example we will show a number of the built-in aggregation functions in action. The full list is: count distinct average max min top sum All the above aggregators can be implemented using static helpers in the Aggregators class, for example Aggregators.count() . The helpers create the right aggregator type based on the type of method reference/extractor that is passed as an argument. They all return the EntryAggregator implementations which allows them to be passed as argument to the aggregate methods below. public default &lt;R&gt; R aggregate(EntryAggregator&lt;? super K, ? super V, R&gt; aggregator) - Aggregate across all entries in a cache public &lt;R&gt; R aggregate(Collection&lt;? extends K&gt; collKeys, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defined by the keys public &lt;R&gt; R aggregate(Filter filter, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defines by the filter The SimpleAggregationExample runs various aggregations using a number of the above functions. Example Details The runExample() method contains the code that exercises the above aggregators. Refer to the inline code comments for explanations of what each aggregator is doing. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Customer&gt; customers = getCustomers(); NamedMap&lt;Integer, Order&gt; orders = getOrders(); // count the customers using the Aggregators helper int customerCount = customers.aggregate(Aggregators.count()); Logger.info(\"Customer Count = \" + customerCount); // count the orders int orderCount = orders.aggregate(Aggregators.count()); Logger.info(\"Order Count = \" + orderCount); // get the total value of all orders - requires index on Order::getOrderTotal to be efficient Double totalOrders = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Total Order Value \" + formatMoney(totalOrders)); // get the average order value across all orders - requires index to be efficient Double averageOrderValue = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Average Order Value \" + formatMoney(averageOrderValue)); // get the minimum order value where then is only 1 order line - requires index on Order::getOrderLineCount to be efficient Double minOrderValue1Line = orders.aggregate(Filters.equal(Order::getOrderLineCount, 1), Aggregators.min(Order::getOrderTotal)); Logger.info(\"Min Order Value for orders with 1 line \" + formatMoney(minOrderValue1Line)); // get the outstanding balances by state - requires index on the full ValueExtractor to be efficient ValueExtractor&lt;Customer, String&gt; officeState = ValueExtractor.of(Customer::getOfficeAddress).andThen(Address::getState); Map&lt;String, BigDecimal&gt; mapOutstandingByState = customers.aggregate( GroupAggregator.createInstance(officeState, Aggregators.sum(Customer::getOutstandingBalance))); mapOutstandingByState.forEach((k, v) -&gt; Logger.info(\"State: \" + k + \", outstanding total is \" + formatMoney(v))); // get the top 5 order totals by value Logger.info(\"Top 5 orders by value\"); Object[] topOrderValues = orders.aggregate(Aggregators.topN(Order::getOrderTotal, 5)); for (Object value : topOrderValues) { Logger.info(formatMoney((Double) value)); } } ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Run the following to load the data and run the example. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Creating 10000 customers &lt;Info&gt; (thread=main, member=1): Creating orders for customers &lt;Info&gt; (thread=main, member=1): Orders created &lt;Info&gt; (thread=main, member=1): Customer Count = 10000 &lt;Info&gt; (thread=main, member=1): Order Count = 29848 &lt;Info&gt; (thread=main, member=1): Total Order Value $89,689,872.00 &lt;Info&gt; (thread=main, member=1): Average Order Value $3,004.89 &lt;Info&gt; (thread=main, member=1): Min Order Value for orders with 1 line $500.08 &lt;Info&gt; (thread=main, member=1): State: QLD, outstanding total is $567,600.00 &lt;Info&gt; (thread=main, member=1): State: WA, outstanding total is $585,800.00 &lt;Info&gt; (thread=main, member=1): State: SA, outstanding total is $561,900.00 &lt;Info&gt; (thread=main, member=1): State: VIC, outstanding total is $556,500.00 &lt;Info&gt; (thread=main, member=1): State: NT, outstanding total is $528,700.00 &lt;Info&gt; (thread=main, member=1): State: ACT, outstanding total is $566,800.00 &lt;Info&gt; (thread=main, member=1): State: TAS, outstanding total is $563,900.00 &lt;Info&gt; (thread=main, member=1): State: NSW, outstanding total is $530,900.00 &lt;Info&gt; (thread=main, member=1): Top 5 orders by value &lt;Info&gt; (thread=main, member=1): $8,304.27 &lt;Info&gt; (thread=main, member=1): $8,273.82 &lt;Info&gt; (thread=main, member=1): $8,229.51 &lt;Info&gt; (thread=main, member=1): $8,197.35 &lt;Info&gt; (thread=main, member=1): $8,194.63 ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " You have seen how to use built-in aggregators which include count , sum , min , average and top on orders and customers maps. You also used the Aggregators class and its helpers to simplify aggregator usage. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " Performing Data Grid Operations Streams ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/120-built-in-aggregators/README",
            "text": " This guide walks you through how to use built-in aggregators such as including count, sum, min, average and top which allow you to process data stored in Coherence in parallel. Coherence supports entry aggregators that perform operations against all, or a subset of entries to obtain a single result. This aggregation is carried out in parallel across the cluster and is a map-reduce type of operation which can be performed efficiently across large amounts of data. See the Coherence Documentation for detailed information on Aggregations. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build In this example you will utilize the built-in aggregators such as count , sum , min , average and top on orders and customers maps. You will also use the Aggregators class and its helpers to simplify aggregator usage. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but to best demonstrate the functionality you should run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the following classes in two maps, customers and orders Customer - Represents a customer Order - Represents and order for a customer and contains order lines OrderLine - Represents an individual order line for an order Address - Represents an address for a customer Review the Example Code In this example we will show a number of the built-in aggregation functions in action. The full list is: count distinct average max min top sum All the above aggregators can be implemented using static helpers in the Aggregators class, for example Aggregators.count() . The helpers create the right aggregator type based on the type of method reference/extractor that is passed as an argument. They all return the EntryAggregator implementations which allows them to be passed as argument to the aggregate methods below. public default &lt;R&gt; R aggregate(EntryAggregator&lt;? super K, ? super V, R&gt; aggregator) - Aggregate across all entries in a cache public &lt;R&gt; R aggregate(Collection&lt;? extends K&gt; collKeys, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defined by the keys public &lt;R&gt; R aggregate(Filter filter, EntryAggregator&lt;? super K, ? super V, R&gt; aggregator); - Aggregate across a set of entries defines by the filter The SimpleAggregationExample runs various aggregations using a number of the above functions. Example Details The runExample() method contains the code that exercises the above aggregators. Refer to the inline code comments for explanations of what each aggregator is doing. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Customer&gt; customers = getCustomers(); NamedMap&lt;Integer, Order&gt; orders = getOrders(); // count the customers using the Aggregators helper int customerCount = customers.aggregate(Aggregators.count()); Logger.info(\"Customer Count = \" + customerCount); // count the orders int orderCount = orders.aggregate(Aggregators.count()); Logger.info(\"Order Count = \" + orderCount); // get the total value of all orders - requires index on Order::getOrderTotal to be efficient Double totalOrders = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Total Order Value \" + formatMoney(totalOrders)); // get the average order value across all orders - requires index to be efficient Double averageOrderValue = orders.aggregate(Aggregators.sum(Order::getOrderTotal)); Logger.info(\"Average Order Value \" + formatMoney(averageOrderValue)); // get the minimum order value where then is only 1 order line - requires index on Order::getOrderLineCount to be efficient Double minOrderValue1Line = orders.aggregate(Filters.equal(Order::getOrderLineCount, 1), Aggregators.min(Order::getOrderTotal)); Logger.info(\"Min Order Value for orders with 1 line \" + formatMoney(minOrderValue1Line)); // get the outstanding balances by state - requires index on the full ValueExtractor to be efficient ValueExtractor&lt;Customer, String&gt; officeState = ValueExtractor.of(Customer::getOfficeAddress).andThen(Address::getState); Map&lt;String, BigDecimal&gt; mapOutstandingByState = customers.aggregate( GroupAggregator.createInstance(officeState, Aggregators.sum(Customer::getOutstandingBalance))); mapOutstandingByState.forEach((k, v) -&gt; Logger.info(\"State: \" + k + \", outstanding total is \" + formatMoney(v))); // get the top 5 order totals by value Logger.info(\"Top 5 orders by value\"); Object[] topOrderValues = orders.aggregate(Aggregators.topN(Order::getOrderTotal, 5)); for (Object value : topOrderValues) { Logger.info(formatMoney((Double) value)); } } Run the Example Carry out the following to run this example: Start 2 cache servers using the method described above: E.g. for Maven use: <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Run the following to load the data and run the example. E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Creating 10000 customers &lt;Info&gt; (thread=main, member=1): Creating orders for customers &lt;Info&gt; (thread=main, member=1): Orders created &lt;Info&gt; (thread=main, member=1): Customer Count = 10000 &lt;Info&gt; (thread=main, member=1): Order Count = 29848 &lt;Info&gt; (thread=main, member=1): Total Order Value $89,689,872.00 &lt;Info&gt; (thread=main, member=1): Average Order Value $3,004.89 &lt;Info&gt; (thread=main, member=1): Min Order Value for orders with 1 line $500.08 &lt;Info&gt; (thread=main, member=1): State: QLD, outstanding total is $567,600.00 &lt;Info&gt; (thread=main, member=1): State: WA, outstanding total is $585,800.00 &lt;Info&gt; (thread=main, member=1): State: SA, outstanding total is $561,900.00 &lt;Info&gt; (thread=main, member=1): State: VIC, outstanding total is $556,500.00 &lt;Info&gt; (thread=main, member=1): State: NT, outstanding total is $528,700.00 &lt;Info&gt; (thread=main, member=1): State: ACT, outstanding total is $566,800.00 &lt;Info&gt; (thread=main, member=1): State: TAS, outstanding total is $563,900.00 &lt;Info&gt; (thread=main, member=1): State: NSW, outstanding total is $530,900.00 &lt;Info&gt; (thread=main, member=1): Top 5 orders by value &lt;Info&gt; (thread=main, member=1): $8,304.27 &lt;Info&gt; (thread=main, member=1): $8,273.82 &lt;Info&gt; (thread=main, member=1): $8,229.51 &lt;Info&gt; (thread=main, member=1): $8,197.35 &lt;Info&gt; (thread=main, member=1): $8,194.63 Summary You have seen how to use built-in aggregators which include count , sum , min , average and top on orders and customers maps. You also used the Aggregators class and its helpers to simplify aggregator usage. See Also Performing Data Grid Operations Streams ",
            "title": "Built-In Aggregators"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " The POF Maven Plugin provides automated instrumentation of classes with the @PortableType annotation to generate consistent (and correct) implementations of Evolvable POF serialization methods. It is a far from a trivial exercise to manually write serialization methods that support serializing inheritance hierarchies that support the Evolvable concept. However, with static type analysis these methods can be deterministically generated. This allows developers to focus on business logic rather than implementing boilerplate code for the above-mentioned methods. Please see Portable Types documentation for more information and detailed instructions on Portable Types creation and usage. ",
            "title": "POF Maven Plugin"
        },
        {
            "location": "/plugins/maven/pof-maven-plugin/README",
            "text": " In order to use the POF Maven Plugin, you need to declare it as a plugin dependency in your pom.xml : <markup lang=\"xml\" > &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;pof-maven-plugin&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;instrument&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;instrument-tests&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;instrument-tests&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; An example Person class (below) when processed with the plugin is below results in the bytecode shown below. <markup lang=\"java\" >@PortableType(id=1000) public class Person { public Person() { } public Person(int id, String name, Address address) { super(); this.id = id; this.name = name; this.address = address; } int id; String name; Address address; // getters and setters omitted for brevity } Generated bytecode: <markup lang=\"bash\" >$ javap Person.class Compiled from \"Person.java\" public class demo.Person implements com.tangosol.io.pof.PortableObject,com.tangosol.io.pof.EvolvableObject { int id; java.lang.String name; demo.Address address; public demo.Person(); public demo.Person(int, java.lang.String, demo.Address); public int getId(); public void setId(int); public java.lang.String getName(); public void setName(java.lang.String); public demo.Address getAddress(); public void setAddress(demo.Address); public java.lang.String toString(); public int hashCode(); public boolean equals(java.lang.Object); public void readExternal(com.tangosol.io.pof.PofReader) throws java.io.IOException; public void writeExternal(com.tangosol.io.pof.PofWriter) throws java.io.IOException; public com.tangosol.io.Evolvable getEvolvable(int); public com.tangosol.io.pof.EvolvableHolder getEvolvableHolder(); } Additional methods generated by Coherence POF plugin. ",
            "title": "Usage"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " The simplest way to create a Publisher is from the Coherence Session API, by calling the createPublisher method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\"); The code snippet above creates an anonymous Publisher that publishes to String messages to the topic names test-topic . Alternatively, a Publisher can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Publisher&lt;String&gt; publisher = topic.createPublisher(\"test-topic\"); Both the Session.createPublisher() and NamedTopic.createPublisher() methods also take a var-args array of Publisher.Option instances to further configure the behaviour of the publisher.Some of these options are described below. ",
            "title": "Creating Publishers"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Publishers should ideally be closed when application code finishes with them so that any resources associated with them are also closed and cleaned up. Pubishers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); try (Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\")) { // ... publish messages ... } In the above example, the publisher is used to publish messages inside the try/catch block.Once the try/catch block exits, the publisher is closed. When a publisher is closed, it can no longer be used.Calls to publish methods after closing will throw an IllegalStateException . ",
            "title": "Closing a Publisher"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " The default behaviour in Coherence Topics is that messages published by a publisher on a specific JVM thread are received in order they were published.Messages published on different threads, or by different publishers could be received interleaved with each other but always in order from the viewpoint of the publishing thread. The OrderBy.thread() option can be specified explicitly as shown below: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.thread()); ",
            "title": "OrderBy Thread"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " It is possible to publish messages to a channel based on a function that derives a channel from the value being published.This is achieved using the OrderBy.value(ToIntFunction&lt;? super V&gt; orderIdFunction) option, where the orderIdFunction is a java.util.function.ToIntFunction that takes the value being published and returns an int .The publisher channel the value is published to will be derived by modding the returned int with the number of channels available for the topic.For example, if there were 17 channels (0 - 16), and the function returned 8, the message would be published to channel 8, if the function returned 19, the message would be published to 19 % 17 - which is 2. If the int returned is negative, a positive channel number is calculated equal to n % channelCount + channelCount ) For example, suppose a Publisher is publishing Order messages, and we only care about ordering by the orders customerId (which coincidentally is an int ).We could do something like this: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); OrderBy orderBy = OrderBy.value(order -&gt; order.getCustomerId()); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", orderBy); ",
            "title": "OrderBy Value"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Messages can be published to a fixed channel by using the OrderBy.id(int nOrderId) option, where the nOrderId parameter is used to determine the channel.The exact channel is worked out in the same way that it is for the OrderBy.value() option, by modding the int value used in the OrderBy.id() option with the number of channels.For example, if there were 17 channels, and the option used was OrderBy.id(8) then all messages would be published to channel 8, if the OrderBy.id(19) was used all messages would go to channel 19 % 17 , which is channel 2. For example, to publish all messages from a Publisher to channel 5: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.is(5)); Using the OrderBy.id() option with multiple publishers with the same Id value will cause all those publishers to publish to the same channel.This would mean that message ordering would be global across all of those publishers, with the caveat that there would be more contention with publishers against the tail of the channel. ",
            "title": "OrderBy Id"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Finally, using OrderBy.none() will guarantee no ordering, each message would be published to a random channel.This would allow the least contention in use cases where the order of message processing by subscribers did not matter. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.none()); ",
            "title": "OrderBy None"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " It is possible to use the values themselves to determine ordering by making the message value implement the interface com.tangosol.net.topic.Publisher.Orderable .This interface has a single getOrderId() method that returns an int that is used in the same way as other options above to determine the channel to publish to. Publishing Orderable values will override any OrderBy option that might have been specified for the publisher. This is an similar to the OrderBy.value() option, but in this case the code that creates the publisher does not need to know how to determine the order, this can be different for each type of message that the publisher publishes. For example, if there is a Transaction class with a String customer identifier that should be used to order published Transactions , we might implement it like this: <markup lang=\"java\" title=\"Transaction.java\" >public class Transaction implements Publisher.Orderable { private String customerId; @Override public int getOrderId() { return Objects.hashCode(customerId); } } Now we can configure a publisher: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Transaction&gt; publisher = session.createPublisher(\"test-topic\"); The publisher above did not specify an OrderBy option, so the default or OrderBy.thread() will be used, but as the Transaction class implements Publisher.Orderable then it&#8217;s getOrderId method wil be used to determine message ordering. ",
            "title": "Publish Orderable Messages"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " In a lot of use cases it is important that messages are processed by subscribers in a guaranteed order.The publisher can be configured when it is created to use different ordering guarantees by using the Publisher.OrderBy option. The OrderBy options controls which channel (or channels) in a topic a publisher publishes messages to.Subscribers receive messages from a specific channel in the order that they were published. OrderBy Thread The default behaviour in Coherence Topics is that messages published by a publisher on a specific JVM thread are received in order they were published.Messages published on different threads, or by different publishers could be received interleaved with each other but always in order from the viewpoint of the publishing thread. The OrderBy.thread() option can be specified explicitly as shown below: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.thread()); OrderBy Value It is possible to publish messages to a channel based on a function that derives a channel from the value being published.This is achieved using the OrderBy.value(ToIntFunction&lt;? super V&gt; orderIdFunction) option, where the orderIdFunction is a java.util.function.ToIntFunction that takes the value being published and returns an int .The publisher channel the value is published to will be derived by modding the returned int with the number of channels available for the topic.For example, if there were 17 channels (0 - 16), and the function returned 8, the message would be published to channel 8, if the function returned 19, the message would be published to 19 % 17 - which is 2. If the int returned is negative, a positive channel number is calculated equal to n % channelCount + channelCount ) For example, suppose a Publisher is publishing Order messages, and we only care about ordering by the orders customerId (which coincidentally is an int ).We could do something like this: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); OrderBy orderBy = OrderBy.value(order -&gt; order.getCustomerId()); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", orderBy); OrderBy Id Messages can be published to a fixed channel by using the OrderBy.id(int nOrderId) option, where the nOrderId parameter is used to determine the channel.The exact channel is worked out in the same way that it is for the OrderBy.value() option, by modding the int value used in the OrderBy.id() option with the number of channels.For example, if there were 17 channels, and the option used was OrderBy.id(8) then all messages would be published to channel 8, if the OrderBy.id(19) was used all messages would go to channel 19 % 17 , which is channel 2. For example, to publish all messages from a Publisher to channel 5: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.is(5)); Using the OrderBy.id() option with multiple publishers with the same Id value will cause all those publishers to publish to the same channel.This would mean that message ordering would be global across all of those publishers, with the caveat that there would be more contention with publishers against the tail of the channel. OrderBy None Finally, using OrderBy.none() will guarantee no ordering, each message would be published to a random channel.This would allow the least contention in use cases where the order of message processing by subscribers did not matter. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.none()); Publish Orderable Messages It is possible to use the values themselves to determine ordering by making the message value implement the interface com.tangosol.net.topic.Publisher.Orderable .This interface has a single getOrderId() method that returns an int that is used in the same way as other options above to determine the channel to publish to. Publishing Orderable values will override any OrderBy option that might have been specified for the publisher. This is an similar to the OrderBy.value() option, but in this case the code that creates the publisher does not need to know how to determine the order, this can be different for each type of message that the publisher publishes. For example, if there is a Transaction class with a String customer identifier that should be used to order published Transactions , we might implement it like this: <markup lang=\"java\" title=\"Transaction.java\" >public class Transaction implements Publisher.Orderable { private String customerId; @Override public int getOrderId() { return Objects.hashCode(customerId); } } Now we can configure a publisher: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Transaction&gt; publisher = session.createPublisher(\"test-topic\"); The publisher above did not specify an OrderBy option, so the default or OrderBy.thread() will be used, but as the Transaction class implements Publisher.Orderable then it&#8217;s getOrderId method wil be used to determine message ordering. ",
            "title": "Configure Ordering Guarantees"
        },
        {
            "location": "/docs/topics/03_publishers",
            "text": " Publishers are used to publish messages to a Coherence topic, a publisher publishes to a single topic. Creating Publishers Closing a Publisher Configure Ordering Guarantees Ordering by publishing thread Ordering by message value Ordering by fixed channel Orderable messages Creating Publishers The simplest way to create a Publisher is from the Coherence Session API, by calling the createPublisher method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\"); The code snippet above creates an anonymous Publisher that publishes to String messages to the topic names test-topic . Alternatively, a Publisher can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Publisher&lt;String&gt; publisher = topic.createPublisher(\"test-topic\"); Both the Session.createPublisher() and NamedTopic.createPublisher() methods also take a var-args array of Publisher.Option instances to further configure the behaviour of the publisher.Some of these options are described below. Closing a Publisher Publishers should ideally be closed when application code finishes with them so that any resources associated with them are also closed and cleaned up. Pubishers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; Session session = Coherence.getSession(); try (Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\")) { // ... publish messages ... } In the above example, the publisher is used to publish messages inside the try/catch block.Once the try/catch block exits, the publisher is closed. When a publisher is closed, it can no longer be used.Calls to publish methods after closing will throw an IllegalStateException . Configure Ordering Guarantees In a lot of use cases it is important that messages are processed by subscribers in a guaranteed order.The publisher can be configured when it is created to use different ordering guarantees by using the Publisher.OrderBy option. The OrderBy options controls which channel (or channels) in a topic a publisher publishes messages to.Subscribers receive messages from a specific channel in the order that they were published. OrderBy Thread The default behaviour in Coherence Topics is that messages published by a publisher on a specific JVM thread are received in order they were published.Messages published on different threads, or by different publishers could be received interleaved with each other but always in order from the viewpoint of the publishing thread. The OrderBy.thread() option can be specified explicitly as shown below: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;String&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.thread()); OrderBy Value It is possible to publish messages to a channel based on a function that derives a channel from the value being published.This is achieved using the OrderBy.value(ToIntFunction&lt;? super V&gt; orderIdFunction) option, where the orderIdFunction is a java.util.function.ToIntFunction that takes the value being published and returns an int .The publisher channel the value is published to will be derived by modding the returned int with the number of channels available for the topic.For example, if there were 17 channels (0 - 16), and the function returned 8, the message would be published to channel 8, if the function returned 19, the message would be published to 19 % 17 - which is 2. If the int returned is negative, a positive channel number is calculated equal to n % channelCount + channelCount ) For example, suppose a Publisher is publishing Order messages, and we only care about ordering by the orders customerId (which coincidentally is an int ).We could do something like this: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); OrderBy orderBy = OrderBy.value(order -&gt; order.getCustomerId()); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", orderBy); OrderBy Id Messages can be published to a fixed channel by using the OrderBy.id(int nOrderId) option, where the nOrderId parameter is used to determine the channel.The exact channel is worked out in the same way that it is for the OrderBy.value() option, by modding the int value used in the OrderBy.id() option with the number of channels.For example, if there were 17 channels, and the option used was OrderBy.id(8) then all messages would be published to channel 8, if the OrderBy.id(19) was used all messages would go to channel 19 % 17 , which is channel 2. For example, to publish all messages from a Publisher to channel 5: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.is(5)); Using the OrderBy.id() option with multiple publishers with the same Id value will cause all those publishers to publish to the same channel.This would mean that message ordering would be global across all of those publishers, with the caveat that there would be more contention with publishers against the tail of the channel. OrderBy None Finally, using OrderBy.none() will guarantee no ordering, each message would be published to a random channel.This would allow the least contention in use cases where the order of message processing by subscribers did not matter. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Order&gt; publisher = session.createPublisher(\"test-topic\", OrderBy.none()); Publish Orderable Messages It is possible to use the values themselves to determine ordering by making the message value implement the interface com.tangosol.net.topic.Publisher.Orderable .This interface has a single getOrderId() method that returns an int that is used in the same way as other options above to determine the channel to publish to. Publishing Orderable values will override any OrderBy option that might have been specified for the publisher. This is an similar to the OrderBy.value() option, but in this case the code that creates the publisher does not need to know how to determine the order, this can be different for each type of message that the publisher publishes. For example, if there is a Transaction class with a String customer identifier that should be used to order published Transactions , we might implement it like this: <markup lang=\"java\" title=\"Transaction.java\" >public class Transaction implements Publisher.Orderable { private String customerId; @Override public int getOrderId() { return Objects.hashCode(customerId); } } Now we can configure a publisher: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Publisher; import com.tangosol.net.topic.Publisher.OrderBy; Session session = Coherence.getSession(); Publisher&lt;Transaction&gt; publisher = session.createPublisher(\"test-topic\"); The publisher above did not specify an OrderBy option, so the default or OrderBy.thread() will be used, but as the Transaction class implements Publisher.Orderable then it&#8217;s getOrderId method wil be used to determine message ordering. ",
            "title": "Publishers"
        },
        {
            "location": "/examples/README",
            "text": " These guides and tutorials are designed to help you be productive as quickly as possible in whatever use-case you are building with Coherence. Coherence has a long history and having been around for twenty years its APIs have evolved over that time. Occasionally there are multiple ways to implement a specific use-case, typically because to remain backwards compatible with older releases, features cannot be removed from the product. For that reason these guides use the latest Coherence versions and best practice and approaches recommended by the Coherence team for that version. explore Simple Guides fa-graduation-cap Tutorials ",
            "title": "Overview"
        },
        {
            "location": "/examples/README",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor if you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Bootstrap Coherence This guide walks you through various methods to configure and bootstrap a Coherence instance. Coherence*Extend Provides a guide for clients to connect to a Coherence Cluster via Coherence*Extend. Put Get and Remove This guide walks you through basic CRUD put , get , and remove operations on a NamedMap . Querying Caches This guide walks you through the basic concepts of querying Coherence caches. Built-in Aggregators This guide walks you through how to use built-in aggregators within Coherence. Custom Aggregators This guide walks you through how to create custom aggregators within Coherence. Views Learn about the basic concepts of working with views using the ContinuousQueryCache . Streams This guide walks you through how to use the Streams API with Coherence. Entry Processors This guide walks you through how to use Entry Processors with Coherence. Federation This guide walks you through how to use Federation within Coherence. Topics This guide walks you through how to use Topics within Coherence. Near Caching This guide walks you through how to use near caching within Coherence. Client Events This guide walks you through how to use client events within Coherence. Server-Side Events This guide walks you through how to use server-side events within Coherence. Durable Events This guide walks you through how to use durable events within Coherence. Cache Stores This guide walks you through how to use and configure Cache Stores. Bulk Loading Caches This guide shows approaches to bluk load data into caches, typically this would be loading data into caches from a DB when applications start. Securing with SSL This guide walks you through how to secure Coherence using SSL/TLS. Key Association This guide walks you through a use case for key association in Coherence. Multi-Cluster Client An example of how to connect an Extend or gRPC client to multiple Coherence clusters. ",
            "title": "Guides"
        },
        {
            "location": "/examples/README",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. Persistence This tutorial shows you how to use Persistence from CohQL and how to monitor Persistence events. ",
            "title": "Tutorials"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Access Metrics Run the Completed Tutorial Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " About 30-45 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. ",
            "title": "Getting Started"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... ",
            "title": "Review the Initial Project"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Configure MicroProfile GraphQL"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. ",
            "title": "Follow the Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") @Counted public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Include the @Counted microprofile metrics annotation to count the number of invocations Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@starwars.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders() { return orders.values(); } Include the @Timed microprofile metrics annotation to time the query In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" ",
            "title": "Create Queries to Show Customer and Orders"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed, and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally, add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@starwars.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally, add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... ",
            "title": "Inject Related Objects"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation @Timed public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } Include the @Timed microprofile metrics annotation to time the mutation In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation @Timed public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } Include the @Timed microprofile metrics annotation to time the mutation The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the the {@link Order} was not found */ @Mutation @Timed public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Include the @Timed microprofile metrics annotation to time the mutation Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. ",
            "title": "Add Mutations"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Finally, we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } ",
            "title": "Add a Dynamic Where Clause"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As we can see from the above examples, metrics can be easily enabled for queries and mutations by including the @Counted or @Timed annotations. After running a number of queries and mutations you can access the metrics end point using the following curl command: The base metrics endpoint is http://localhost:7001/metrics , but we have added the /application path to restrict the metrics returned. <markup lang=\"bash\" >curl -H 'Accept: application/json' http://127.0.0.1:7001/metrics/application | jq { \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.addOrderLineToOrder\": { \"count\": 1, \"meanRate\": 0.020786416474669184, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 63082260, \"max\": 63082260, \"mean\": 63082260, \"stddev\": 0, \"p50\": 63082260, \"p75\": 63082260, \"p95\": 63082260, \"p98\": 63082260, \"p99\": 63082260, \"p999\": 63082260 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createCustomer\": { \"count\": 1, \"meanRate\": 0.02078651489493201, \"oneMinRate\": 0.013536188363841833, \"fiveMinRate\": 0.0031973351962583784, \"fifteenMinRate\": 0.001095787094460976, \"min\": 4184923, \"max\": 4184923, \"mean\": 4184923, \"stddev\": 0, \"p50\": 4184923, \"p75\": 4184923, \"p95\": 4184923, \"p98\": 4184923, \"p99\": 4184923, \"p999\": 4184923 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createOrder\": { \"count\": 1, \"meanRate\": 0.020786437087696893, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 5411268, \"max\": 5411268, \"mean\": 5411268, \"stddev\": 0, \"p50\": 5411268, \"p75\": 5411268, \"p95\": 5411268, \"p98\": 5411268, \"p99\": 5411268, \"p999\": 5411268 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getCustomers\": 1, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getOrders\": { \"count\": 3, \"meanRate\": 0.06235852925082789, \"oneMinRate\": 0.04423984338571901, \"fiveMinRate\": 0.009754115099857198, \"fifteenMinRate\": 0.003305709235676515, \"min\": 6507371, \"max\": 47080043, \"mean\": 20945553.135424484, \"stddev\": 19245930.056725293, \"p50\": 7014199, \"p75\": 47080043, \"p95\": 47080043, \"p98\": 47080043, \"p99\": 47080043, \"p999\": 47080043 } } jq has been used to format the JSON output. This can be downloaded from https://stedolan.github.io/jq/download/ or you can format the output with an alternate utility. ",
            "title": "Access Metrics"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Example Code"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } ",
            "title": "Run the Completed Tutorial"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. ",
            "title": "Summary"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "See Also"
        },
        {
            "location": "/examples/tutorials/500-graphql/README",
            "text": " This tutorial walks through the steps to enable access to Coherence data from GraphQL using Helidon’s MicroProfile (MP) GraphQL support and Coherence CDI . Table of Contents What You Will Build What You Need Getting Started Follow the Tutorial Review the Initial Project Configure MicroProfile GraphQL Create Queries to Show Customer and Orders Inject Related Objects Add Mutations Add a Dynamic Where Clause Access Metrics Run the Completed Tutorial Summary See Also What You Will Build You will build on an existing mock sample Coherence data model and create an application that will expose a GraphQL endpoint to perform various queries and mutations against the data model. If you wish to read more about GraphQL or Helidon&#8217;s support in GraphQL, please see this Medium post . What You Need About 30-45 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code Whenever you are asked to build the code, please refer to the instructions below. The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Getting Started This tutorial contains both the completed codebase as well as the initial state from which you build the complete the tutorial on. If you would like to run the completed example, please follow the instructions here otherwise continue below for the tutorial. Follow the Tutorial Ensure you have the project in tutorials/500-graphql/initial imported into your IDE. Review the Initial Project Maven Configuration The initial project is a Coherence-CDI and Helidon project and imports the coherence-bom , helidon-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon&lt;/groupId&gt; &lt;artifactId&gt;helidon-bom&lt;/artifactId&gt; &lt;version&gt;${helidon.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; helidon-microprofile-cdi , coherence-cdi-server and helidon-microprofile-metrics are also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.cdi&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-cdi&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.metrics&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-metrics&lt;/artifactId&gt; &lt;/dependency&gt; The POM also includes the jandex-maven-plugin to build an index, which is required by Helidon&#8217;s implementation. <markup lang=\"xml\" >&lt;plugin&gt; &lt;groupId&gt;org.jboss.jandex&lt;/groupId&gt; &lt;artifactId&gt;jandex-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.jandex.plugin.version}&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-index&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jandex&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; Data Model The data model consists of the following classes: Customer - contains customer details and keyed by customer id Order - contains orders for a customer and is keyed by order number Order Lines - contains order line information which is included directly within Order object The Objects to be used must conform to the naming conventions for fields and their getters and setters according to the Java Bean Spec to ensure full functionality works correctly in Helidon&#8217;s MicroProfile GraphQL implementation. Coherence Bootstrap The Bootstrap class is used to initialize Coherence and includes the following NamedMaps : <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; The class is ApplicationScoped and init method is called on application startup. <markup lang=\"java\" >/** * Initialize the Coherence {@link NamedMap}s with data. * * @param init init */ private void init(@Observes @Initialized(ApplicationScoped.class) Object init) { Build and Run the Initial State Build and run using either of the following: Commands to build and run for the rest of the tutorial Build Tool Build Command Run Comments Maven ./mvnw clean package ./mvnw exec:exec Gradle ./gradlew build ./gradlew runApp Running the application will output, amongst other things, messages indicating Coherence has started and the following to show the data was loaded: <markup lang=\"text\" >===CUSTOMERS=== Customer{customerId=1, name='Billy Joel', email='billy@billyjoel.com', address='Address 1', balance=0.0} Customer{customerId=4, name='Tom Jones', email='tom@jones.com', address='Address 4', balance=0.0} Customer{customerId=2, name='James Brown', email='soul@jamesbrown.net', address='Address 2', balance=100.0} Customer{customerId=3, name='John Williams', email='john@starwars.com', address='Address 3', balance=0.0} ===ORDERS=== .... Configure MicroProfile GraphQL Add Helidon MP GraphQL Add the following dependency to the project POM: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.graphql&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-graphql-server&lt;/artifactId&gt; &lt;/dependency&gt; or if you are using Gradle, then add the following to build.gradle : <markup lang=\"properties\" >implementation (\"io.helidon.microprofile.graphql:helidon-microprofile-graphql-server\") Add MicroProfile Properties Add the following to src/main/resources/META-INF/microprofile-config.properties : <markup lang=\"java\" >server.static.classpath.context=/ui server.static.classpath.location=/web graphql.cors=Access-Control-Allow-Origin mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException The server.static.classpath.context=/ui defines the URL to serve the contents found in resources location server.static.classpath.location=/web . E.g. src/main/resources/web . The setting graphql.cors=Access-Control-Allow-Origin allows the GraphiQL UI to use CORS. We will explain the mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException later. As the GraphiQL UI client used in this example is not included in this repository, you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html before you continue. Create Queries to Show Customer and Orders Create the CustomerApi Class Firstly we need to create a class to expose our GraphQL endpoint. Create a new Class called CustomerApi in the package com.oracle.coherence.tutorials.graphql.api . Add the GraphQLApi annotation to mark this class as a GraphQL Endpoint and make it application scoped. <markup lang=\"java\" >@ApplicationScoped @GraphQLApi public class CustomerApi { Inject the Coherence `NamedMap`s for customers and orders <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private NamedMap&lt;Integer, Customer&gt; customers; /** * The {@link NamedMap} for orders. */ @Inject private NamedMap&lt;Integer, Order&gt; orders; Add a Query to return all customers Add the following code to CustomerApi to create a query to return all customers: <markup lang=\"java\" >/** * Returns all of the {@link Customer}s. * * @return all of the {@link Customer}s. */ @Query @Description(\"Displays customers\") @Counted public Collection&lt;Customer&gt; getCustomers() { return customers.values(); } Include the @Counted microprofile metrics annotation to count the number of invocations Ensure you import the Query and Description annotations from org.eclipse.microprofile.graphql Build and run the project. Issue the following to display the automatically generated schema: <markup lang=\"bash\" >curl http://localhost:7001/graphql/schema.graphql type Customer { address: String balance: String! customerId: Int! email: String name: String orders: [Order] } type Query { \"Displays customers\" customers: [Customer] } Open the URL http://localhost:7001/ui . You should see the GraphiQL UI. Notice the Documentation Explorer on the right, which will allow you to explore the generated schema. Enter the following in the left-hand pane and click the Play button. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance } } This will result in the following JSON output: <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": 0 }, { \"customerId\": 4, \"name\": \"Tom Jones\", \"address\": \"Address 4\", \"email\": \"tom@jones.com\", \"balance\": 0 }, { \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": 100 }, { \"customerId\": 3, \"name\": \"John Williams\", \"address\": \"Address 3\", \"email\": \"john@starwars.com\", \"balance\": 0 } ] } } Add a Query to return all Orders Add the following code to CustomerApi to create a query to return all orders: <markup lang=\"java\" >@Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders() { return orders.values(); } Include the @Timed microprofile metrics annotation to time the query In this case we are overriding the default name for the query, which would be orders , with displayOrders . Stop the running project, rebuild and re-run. Refresh GraphiQL and enter the following in the left-hand pane and click the Play button and choose orders . <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } This will result in the following JSON output. The output below has been shortened. Notice that because we included the orderLines field and it is an object, then we must specify the individual fields to return. <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": 12163.024674447412, \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"Samsung TU8000 55 inch Crystal UHD 4K Smart TV [2020]\", \"itemCount\": 1, \"costPerItem\": 1695.3084188228172, \"orderLineTotal\": 1695.3084188228172 }, { \"lineNumber\": 4, \"productDescription\": \"Sony X7000G 49 inch 4k Ultra HD HDR Smart TV\", \"itemCount\": 2, \"costPerItem\": 2003.1246529714456, \"orderLineTotal\": 4006.249305942891 }, { \"lineNumber\": 3, \"productDescription\": \"TCL S615 40 inch Full HD Android TV\", \"itemCount\": 2, \"costPerItem\": 1171.4274805289924, \"orderLineTotal\": 2342.854961057985 }, { \"lineNumber\": 2, \"productDescription\": \"Samsung Q80T 85 inch QLED Ultra HD 4K Smart TV [2020]\", \"itemCount\": 2, \"costPerItem\": 2059.305994311859, \"orderLineTotal\": 4118.611988623718 } ] }, { \"orderId\": 102, \"customerId\": 2, ... Format currency fields We can see from the above output that a number of the currency fields are not formatted correctly. We will use the GraphQL annotation NumberFormat to format this as currency. You may also use the JsonbNumberFormat annotation as well. Add the NumberFormat to getBalance on the Customer class. <markup lang=\"java\" >/** * Returns the customer's balance. * * @return the customer's balance */ @NumberFormat(\"$###,##0.00\") public double getBalance() { return balance; } By adding the NumberFormat to the get method, the format will be applied to the output type only. If we add the NumberFormat to the set method it will be applied to the input type only. E.g. when Customer is used as a parameter. If it is added to the attribute it will apply to both input and output types. Add the NumberFormat to getOrderTotal on the Order class. <markup lang=\"java\" >/** * Returns the order total. * * @return the order total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderTotal() { return orderLines.stream().mapToDouble(OrderLine::getOrderLineTotal).sum(); } Add the NumberFormat to getCostPerItem and getOrderLineTotal on the OrderLine class. <markup lang=\"java\" >/** * Return the cost per item. * * @return the cost per item */ @NumberFormat(\"$###,###,##0.00\") public double getCostPerItem() { return costPerItem; } <markup lang=\"java\" >/** * Returns the order line total. * * @return he order line total */ @NumberFormat(\"$###,###,##0.00\") public double getOrderLineTotal() { return itemCount * costPerItem; } Stop the running project, rebuild and re-run. Refresh GraphiQL and run the customers and orders queries and you will see the number values formatted as shown below: <markup lang=\"json\" >{ \"customerId\": 2, \"name\": \"James Brown\", \"address\": \"Address 2\", \"email\": \"soul@jamesbrown.net\", \"balance\": \"$100.00\" } <markup lang=\"json\" >... \"orderTotal\": \"$13,029.54\", ... \"costPerItem\": \"$2,456.27\", \"orderLineTotal\": \"$2,456.27\" Inject Related Objects From the above output for orders, we can see we have customerId field only. It would be useful to also be able to return any attributes for the customer customer. Conversely it would be useful to be able to show the order details for a customer. We can achieve this using Coherence by making the class implement Injectable . When the class is deserialized on the client, any @Inject statements are processed, and we will use this to inject the NamedMap for customer and use to retrieve the customer details if required. Return the Customer for the Order Make the Order class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Order implements Serializable, Injectable { Inject the customer NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for customers. */ @Inject private transient NamedMap&lt;Integer, Customer&gt; customers; Finally, add the getCustomer method. <markup lang=\"java\" >/** * Returns the {@link Customer} for this {@link Order}. * * @return the {@link Customer} for this {@link Order} */ public Customer getCustomer() { return customers.get(customerId); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Order object in the Documentation Explorer . You will see a customer field that returns a Customer object. Change the orders query to the following and execute. You will notice the customers name and email returned. <markup lang=\"graphql\" >query orders { displayOrders { orderId customerId orderDate orderTotal customer { name email } orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } } <markup lang=\"json\" > \"data\": { \"displayOrders\": [ { \"orderId\": 104, \"customerId\": 3, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$7,946.81\", \"customer\": { \"name\": \"John Williams\", \"email\": \"john@starwars.com\" }, ... Return the Orders for a Customer Make the Customer class implement com.oracle.coherence.inject.Injectable . <markup lang=\"java\" >public class Customer implements Serializable, Injectable { Inject the orders NamedMap . <markup lang=\"java\" >/** * The {@link NamedMap} for orders. */ @Inject private transient NamedMap&lt;Integer, Order&gt; orders; Finally, add the getOrders method to get the orders for the current customer by specifying a Coherence filter. <markup lang=\"java\" >/** * Returns the {@link Order}s for a {@link Customer}. * * @return the {@link Order}s for a {@link Customer} */ public Collection&lt;Order&gt; getOrders() { return orders.values(Filters.equal(Order::getCustomerId, customerId)); } Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the Customer object in the Documentation Explorer . You will see an orders field that returns an array of Customer objects. Change the customers query to add the orders for a customer and execute. You will notice the orders for the customers returned. <markup lang=\"graphql\" >query customers { customers { customerId name address email balance orders { orderId orderDate orderTotal } } } <markup lang=\"json\" >{ \"data\": { \"customers\": [ { \"customerId\": 1, \"name\": \"Billy Joel\", \"address\": \"Address 1\", \"email\": \"billy@billyjoel.com\", \"balance\": \"$0.00\", \"orders\": [ { \"orderId\": 100, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$1,572.23\" }, { \"orderId\": 101, \"orderDate\": \"2021-01-28\", \"orderTotal\": \"$2,201.91\" } ] }, ... Add Mutations In this section we will add mutations to create or update data. Create a Customer Add the following to the CustomerApi class to create a customer: <markup lang=\"java\" >/** * Creates and saves a {@link Customer}. * * @param customer and saves a {@link Customer} * * @return the new {@link Customer} */ @Mutation @Timed public Customer createCustomer(@Name(\"customer\") Customer customer) { if (customers.containsKey(customer.getCustomerId())) { throw new IllegalArgumentException(\"Customer \" + customer.getCustomerId() + \" already exists\"); } customers.put(customer.getCustomerId(), customer); return customers.get(customer.getCustomerId()); } Include the @Timed microprofile metrics annotation to time the mutation In the above code we throw an IllegalArgumentException if the customer already exists. By default in the MicroProfile GraphQL specification, messages from unchecked exceptions are hidden from the client and \"Server Error\" is returned. In this case we have overridden this behaviour in the META-INF/microprofile-config.properties as shown below: <markup lang=\"java\" >mp.graphql.exceptionsWhiteList=java.lang.IllegalArgumentException Checked exceptions, which we will show below will return the message back to the client by default and the message can be hidden as well if required. Stop the running project, rebuild and re-run. Refresh GraphiQL and create a fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } You can also update your existing customers query to use this fragment. Execute the following mutation: <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } <markup lang=\"json\" >{ \"data\": { \"createCustomer\": { \"customerId\": 12, \"name\": \"Tim\", \"address\": null, \"email\": null, \"balance\": \"$1,000.00\", \"orders\": [] } } } Making Attributes Mandatory If you execute the following query, you will notice that a customer is created with a null name. This is because in MP GraphQL any primitive is mandatory and all Objects are optional. Name is a String and therefore is optional. <markup lang=\"graphql\" >mutation createNewCustomer { createCustomer(customer: { customerId: 11 balance: 1000}) { ...customer } } View the Documentation Explorer and note that the createCustomer mutation has the following schema: <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer CustomerInput has the following structure: <markup lang=\"graphql\" >input CustomerInput { address: String balance: Float! customerId: Int! email: String name: String orders: [OrderInput] } Add the NonNull annotation to the name field in the Customer object: <markup lang=\"java\" >/** * Name. */ @NonNull private String name; Stop the running project, rebuild and re-run. Refresh GraphiQL and try to execute the following mutation again. You will notice the UI will show an error indicating that name is now mandatory. <markup lang=\"graphql\" >createCustomer(customer: CustomerInput): Customer Create an Order Add the following to the CustomerApi class to create an order: <markup lang=\"java\" >/** * Creates and saves an {@link Order} for a given customer id. * * @param customerId customer id to create the {@link Order} for * @param orderId order id * * @return the new {@link Order} * * @throws CustomerNotFoundException if the {@link Customer} was not found */ @Mutation @Timed public Order createOrder(@Name(\"customerId\") int customerId, @Name(\"orderId\") int orderId) throws CustomerNotFoundException { if (!customers.containsKey(customerId)) { throw new CustomerNotFoundException(\"Customer id \" + customerId + \" was not found\"); } if (orders.containsKey(orderId)) { throw new IllegalArgumentException(\"Order \" + orderId + \" already exists\"); } Order order = new Order(orderId, customerId); orders.put(orderId, order); return orders.get(orderId); } Include the @Timed microprofile metrics annotation to time the mutation The validation ensures that we have a valid customer and the order id does not already exist. Create a new checked exception called CustomerNotFoundException in the api package. By default in MP GraphQL the messages from checked exceptions will be automatically returned to the client. <markup lang=\"java\" >public class CustomerNotFoundException extends Exception { /** * Constructs a new exception to indicate that a customer was not found. * * @param message the detail message. */ public CustomerNotFoundException(String message) { super(message); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and add the following fragment to avoid having to repeat fields: <markup lang=\"graphql\" >fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } You can also update the orders query to use the new fragment: <markup lang=\"graphql\" >query orders { displayOrders { ...order } } Try to create an order with a non-existent customer number 12. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 100) { ...order } } This shows the following message from the CustomerNotFoundException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Customer id 12 was not found\" } ] } Try to create an order with an already existing order id 100. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 100) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": null }, \"errors\": [ { \"path\": [ \"createOrder\" ], \"locations\": [ { \"column\": 3, \"line\": 58 } ], \"message\": \"Order 100 already exists\" } ] } Create a new order with valid values: <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } This shows the following message from the IllegalArgumentException : <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } Add an OrderLine to an Order Add the following to the CustomerApi class to add an OrderLine to an Order: <markup lang=\"java\" >/** * Adds an {@link OrderLine} to an existing {@link Order}. * * @param orderId order id to add to * @param orderLine {@link OrderLine} to add * * @return the updates {@link Order} * * @throws OrderNotFoundException the the {@link Order} was not found */ @Mutation @Timed public Order addOrderLineToOrder(@Name(\"orderId\") int orderId, @Name(\"orderLine\") OrderLine orderLine) throws OrderNotFoundException { if (!orders.containsKey(orderId)) { throw new OrderNotFoundException(\"Order number \" + orderId + \" was not found\"); } if (orderLine.getProductDescription() == null || orderLine.getProductDescription().equals(\"\") || orderLine.getItemCount() &lt;= 0 || orderLine.getCostPerItem() &lt;= 0) { throw new IllegalArgumentException(\"Supplied Order Line is invalid: \" + orderLine); } return orders.compute(orderId, (k, v)-&gt;{ v.addOrderLine(orderLine); return v; }); } Include the @Timed microprofile metrics annotation to time the mutation Create a new checked exception called OrderNotFoundException in the api package. <markup lang=\"java\" >public class OrderNotFoundException extends Exception { /** * Constructs a new exception to indicate that an order was not found. * * @param message the detail message. */ public OrderNotFoundException(String message) { super(message); } } To make input easier, we can add DefaultValue annotations to the setLineNumber method and setItemCount methods in the OrderLine` class. Ensure you import DefaultValue from the org.eclipse.microprofile.graphql package. <markup lang=\"java\" >@DefaultValue(\"1\") public void setLineNumber(int lineNumber) { this.lineNumber = lineNumber; } <markup lang=\"java\" >@DefaultValue(\"1\") public void setItemCount(int itemCount) { this.itemCount = itemCount; } By placing the DefaultValue on the setter methods only, it applies to input types only. If we wanted the DefaultValue to apply to output type only we would apply to the getters. If we wish to appy to both input and output we can place on the field. Stop the running project, rebuild and re-run. Refresh GraphiQL and run view the OrderLineInput object in the Documentation Explorer . You will see the default values applied. They are also no longer mandatory as they have a default value. <markup lang=\"graphql\" >lineNumber: Int = 1 itemCount: Int = 1 Create a new order 200 for customer 1 and then add a new order line. <markup lang=\"graphql\" >mutation createOrderForCustomer { createOrder(customerId: 1 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } This shows the following output for the new order. <markup lang=\"json\" >{ \"data\": { \"createOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$0.00\", \"orderLines\": [] } } } And the result of the new order line. <markup lang=\"json\" >{ \"data\": { \"addOrderLineToOrder\": { \"orderId\": 200, \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" }, \"orderDate\": \"2021-01-29\", \"orderTotal\": \"$1,500.00\", \"orderLines\": [ { \"lineNumber\": 1, \"productDescription\": \"iPhone 12\", \"itemCount\": 1, \"costPerItem\": \"$1,500.00\", \"orderLineTotal\": \"$1,500.00\" } ] } } } Experiment with invalid order id and customer id as input. Add a Dynamic Where Clause Finally, we will enhance the orders query and add a dynamic where clause. Update the getOrders method in the CustomerApi to add the where clause and pass this to the QuerHelper to generate the Coherence Filter . The code will ask return an error message if the where clause is invalid. <markup lang=\"java\" >/** * Returns {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null. * * @param whereClause where clause to restrict selection of {@link Order}s * * @return {@link Order}s that match the where clause or all {@link Order}s * if the where clause is null */ @Query(\"displayOrders\") @Timed public Collection&lt;Order&gt; getOrders(@Name(\"whereClause\") String whereClause) { try { Filter filter = whereClause == null ? Filters.always() : QueryHelper.createFilter(whereClause); return orders.values(filter); } catch (Exception e) { throw new IllegalArgumentException(\"Invalid where clause: [\" + whereClause + \"]\"); } } Stop the running project, rebuild and re-run. Refresh GraphiQL and execute the following query to find all orders with a orderTotal greater than $4000. <markup lang=\"graphql\" >query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } }, { \"orderId\": 105, \"orderTotal\": \"$4,629.24\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } }, { \"orderId\": 104, \"orderTotal\": \"$8,078.11\", \"customerId\": 3, \"customer\": { \"name\": \"John Williams\" } } ] } } Use a more complex where clause: <markup lang=\"graphql\" >query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } <markup lang=\"json\" >{ \"data\": { \"displayOrders\": [ { \"orderId\": 101, \"orderTotal\": \"$4,077.69\", \"customerId\": 1, \"customer\": { \"name\": \"Billy Joel\" } } ] } } Access Metrics As we can see from the above examples, metrics can be easily enabled for queries and mutations by including the @Counted or @Timed annotations. After running a number of queries and mutations you can access the metrics end point using the following curl command: The base metrics endpoint is http://localhost:7001/metrics , but we have added the /application path to restrict the metrics returned. <markup lang=\"bash\" >curl -H 'Accept: application/json' http://127.0.0.1:7001/metrics/application | jq { \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.addOrderLineToOrder\": { \"count\": 1, \"meanRate\": 0.020786416474669184, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 63082260, \"max\": 63082260, \"mean\": 63082260, \"stddev\": 0, \"p50\": 63082260, \"p75\": 63082260, \"p95\": 63082260, \"p98\": 63082260, \"p99\": 63082260, \"p999\": 63082260 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createCustomer\": { \"count\": 1, \"meanRate\": 0.02078651489493201, \"oneMinRate\": 0.013536188363841833, \"fiveMinRate\": 0.0031973351962583784, \"fifteenMinRate\": 0.001095787094460976, \"min\": 4184923, \"max\": 4184923, \"mean\": 4184923, \"stddev\": 0, \"p50\": 4184923, \"p75\": 4184923, \"p95\": 4184923, \"p98\": 4184923, \"p99\": 4184923, \"p999\": 4184923 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.createOrder\": { \"count\": 1, \"meanRate\": 0.020786437087696893, \"oneMinRate\": 0.014712537947741825, \"fiveMinRate\": 0.0032510706679223173, \"fifteenMinRate\": 0.0011018917421948848, \"min\": 5411268, \"max\": 5411268, \"mean\": 5411268, \"stddev\": 0, \"p50\": 5411268, \"p75\": 5411268, \"p95\": 5411268, \"p98\": 5411268, \"p99\": 5411268, \"p999\": 5411268 }, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getCustomers\": 1, \"com.oracle.coherence.tutorials.graphql.api.CustomerApi.getOrders\": { \"count\": 3, \"meanRate\": 0.06235852925082789, \"oneMinRate\": 0.04423984338571901, \"fiveMinRate\": 0.009754115099857198, \"fifteenMinRate\": 0.003305709235676515, \"min\": 6507371, \"max\": 47080043, \"mean\": 20945553.135424484, \"stddev\": 19245930.056725293, \"p50\": 7014199, \"p75\": 47080043, \"p95\": 47080043, \"p98\": 47080043, \"p99\": 47080043, \"p999\": 47080043 } } jq has been used to format the JSON output. This can be downloaded from https://stedolan.github.io/jq/download/ or you can format the output with an alternate utility. Run the Completed Tutorial Building the Example Code As the GraphiQL UI client used in this example is not included in this repository, before carrying out the build instructions below you must copy the index.html file contents from https://github.com/graphql/graphiql/blob/main/packages/graphiql/README.md#cdn-bundle into the file in src/main/resources/web/index.html . The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Run with Maven <markup lang=\"bash\" >./mvnw exec:exec Run with Gradle <markup lang=\"bash\" >./gradlew runApp Run the Example Code Open the GraphiQL UI at http://localhost:7001/ui and copy the sample GraphQL queries and mutations below into the editor and use the Play button at the top to try out GraphQL against your Coherence cluster. <markup lang=\"graphql\" >fragment customer on Customer { customerId name address email balance orders { orderId orderTotal } } fragment order on Order { orderId customerId customer { name } orderDate orderTotal orderLines { lineNumber productDescription itemCount costPerItem orderLineTotal } } query customers { customers { ...customer } } query orders { displayOrders { ...order } } query ordersWithWhereClause { displayOrders(whereClause: \"orderTotal &gt; 4000.0\") { orderId orderTotal customerId customer { name } } } query ordersWithWhereClause2 { displayOrders(whereClause: \"orderTotal &gt; 4000.0 and customerId = 1\") { orderId orderTotal customerId customer { name } } } mutation createNewCustomer { createCustomer(customer: { customerId: 12 name: \"Tim\" balance: 1000}) { ...customer } } mutation createOrderForCustomer { createOrder(customerId: 12 orderId: 200) { ...order } } mutation addOrderLineToOrder { addOrderLineToOrder(orderId: 200 orderLine: {productDescription: \"iPhone 12\" costPerItem: 1500 }) { ...order } } Summary In this tutorial you have seen how easy it is to expose Coherence Data using GraphQL. See Also Helidon MP Documentation Microprofile GraphQL Specification ",
            "title": "GraphQL"
        },
        {
            "location": "/examples/tutorials/000-overview",
            "text": " These tutorials provide a deeper understanding of larger Coherence features and concepts that cannot be usually be explained with a few simple code snippets. They might, for example, require a running Coherence cluster to properly show a feature. Tutorials are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. GraphQL This tutorial shows you how to access Coherence Data using GraphQL. Persistence This tutorial shows you how to use Persistence from CohQL and how to monitor Persistence events. ",
            "title": "Tutorials"
        },
        {
            "location": "/coherence-grpc/README",
            "text": "",
            "title": "Coherence gRPC"
        },
        {
            "location": "/coherence-grpc/README",
            "text": "",
            "title": "Developing Remote Clients for Oracle Coherence"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Coherence gRPC for Java allows Java applications to access Coherence clustered services, including data, data events, and data processing from outside the Coherence cluster. Typical uses for Java gRPC clients include desktop and Web applications that require access to remote Coherence resources. This provides an alternative to using Coherence*Extend when writing client applications. Note The Coherence gRPC client and Coherence Extend client feature sets do not match exactly, some functionality in gRPC is not available in Extend and vice-versa. The Coherence gRPC for Java library connects to a Coherence clustered service instance running within the Coherence cluster using a high performance gRPC based communication layer. This library sends all client requests to the Coherence clustered gRPC proxy service which, in turn, responds to client requests by delegating to an actual Coherence clustered service (for example, a partitioned cache service). Like cache clients that are members of the cluster, Java gRPC clients use the Session API call to retrieve a resources such as NamedMap , NamedCache , etc. After it is obtained, a client accesses these resources in the same way as it would if it were part of the Coherence cluster. The fact that operations on Coherence resources are being sent to a remote cluster node (over gRPC) is completely transparent to the client application. There are two parts to Coherence gRPC, the coherence-grpc-proxy module, that provides the server-side gRPC proxy, and the coherence-java-client module that provides the gRPC client. Other non-java Coherence clients are also available that use the Coherence gRPC protocol. ",
            "title": "24 Introduction to gRPC"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } ",
            "title": "Setting Up the Coherence gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The gRPC server starts automatically when you run com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ). Typically, com.tangosol.coherence.net.Coherence class should be used as the application’s main class. Alternatively, you can start an instance of com.tangosol.coherence.net.Coherence by using the Bootstrap API. By default, the gRPC server will listen on all local addresses using an ephemeral port. Just like with Coherence*Extend, the endpoints the gRPC server has bound to can be discovered by a client using the Coherence NameService, so using ephemeral ports allows the server to start without needing to be concerned with port clashes. When reviewing the log output, two log messages appear as shown below to indicate which ports the gRPC server has bound to. <markup >In-Process GrpcAcceptor is now listening for connections using name \"default\" GrpcAcceptor now listening for connections on 0.0.0.0:55550 The service is ready to process requests from one of the Coherence gRPC client implementations. ",
            "title": "Starting the Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 ",
            "title": "Configuring the gRPC Server Listen Address and Port"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication ",
            "title": "Configuring SSL/TLS"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 ",
            "title": "Configuring the gRPC Server Thread Pool"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The Coherence gRPC proxy is configured using an internal default cache configuration file named grpc-proxy-cache-config.xml which only contains a single &lt;proxy-scheme&gt; configuration for the gRPC proxy. There is no reason to override this file as the server can be configured with System properties and environment variables. Configuring the gRPC Server Listen Address and Port The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 Configuring SSL/TLS In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication Configuring the gRPC Server Thread Pool Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 ",
            "title": "Configuring the Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the coherence-grpc-proxy module is on the class path (or module path) then the gRPC server will be started automatically. This behaviour can be disabled by setting the coherence.grpc.enabled system property or COHERENCE_GRPC_ENABLED environment variable to false . ",
            "title": "Disabling the gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The Coherence gRPC proxy is the server-side implementation of the gRPC services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. This chapter includes the following sections: Setting Up the Coherence gRPC Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. Configuring the Server Configuring the gRPC Server includes setting the server port, specifying the in-process server name, and enabling TLS. Disabling the gRPC Proxy Server The Coherence gRPC server starts automatically based on the lifecycle events of DefaultCacheServer , but it can be disabled. Deploying the Proxy Service with Helidon Microprofile gRPC Server If you use the Helidon Microprofile server with the microprofile gRPC server enabled, you can deploy the Coherence gRPC proxy into the Helidon gRPC server instead of the Coherence default gRPC server. Setting Up the Coherence gRPC Proxy Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } Starting the Server The gRPC server starts automatically when you run com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ). Typically, com.tangosol.coherence.net.Coherence class should be used as the application’s main class. Alternatively, you can start an instance of com.tangosol.coherence.net.Coherence by using the Bootstrap API. By default, the gRPC server will listen on all local addresses using an ephemeral port. Just like with Coherence*Extend, the endpoints the gRPC server has bound to can be discovered by a client using the Coherence NameService, so using ephemeral ports allows the server to start without needing to be concerned with port clashes. When reviewing the log output, two log messages appear as shown below to indicate which ports the gRPC server has bound to. <markup >In-Process GrpcAcceptor is now listening for connections using name \"default\" GrpcAcceptor now listening for connections on 0.0.0.0:55550 The service is ready to process requests from one of the Coherence gRPC client implementations. Configuring the Server The Coherence gRPC proxy is configured using an internal default cache configuration file named grpc-proxy-cache-config.xml which only contains a single &lt;proxy-scheme&gt; configuration for the gRPC proxy. There is no reason to override this file as the server can be configured with System properties and environment variables. Configuring the gRPC Server Listen Address and Port The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 Configuring SSL/TLS In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication Configuring the gRPC Server Thread Pool Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 Disabling the gRPC Proxy Server If the coherence-grpc-proxy module is on the class path (or module path) then the gRPC server will be started automatically. This behaviour can be disabled by setting the coherence.grpc.enabled system property or COHERENCE_GRPC_ENABLED environment variable to false . ",
            "title": "25 Using the Coherence gRPC Proxy Server"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " To set up and start using the Coherence gRPC Java client, you should declare it as a dependency of your project. The gRPC client is provided in the coherence-java-client module. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-java-client\" } ",
            "title": "Setting Up the Coherence gRPC Client"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. ",
            "title": "Defining a Remote gRPC Cache"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "A Minimal NameService Configuration"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "A Minimal NameService Configuration with Different Cluster Name"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Configure the NameService Endpoints"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Configure Fixed Endpoints"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication ",
            "title": "Configure SSL"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; ",
            "title": "Configuring the Client Thread Pool"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Just like Coherence*Extend, a Coherence gRPC client accesses remote clustered resources by configuring remote schemes in the applications cache configuration file. Defining a Remote gRPC Cache A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. A Minimal NameService Configuration The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; A Minimal NameService Configuration with Different Cluster Name If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure the NameService Endpoints If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure Fixed Endpoints If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure SSL To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication Configuring the Client Thread Pool Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; ",
            "title": "Configure the Coherence gRPC Client"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Using a Remote gRPC Cache as a Back Cache"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " As the gRPC client is configured as a remote scheme in the cache configuration file, Coherence resources can be accessed using the same Coherence APIs as used on cluster members or Extend clients. If the client has been started using the Coherence bootstrap API, running a com.tangosol.net.Coherence instance, a Session and NamedMap can be accessed as shown below: <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test-cache\"); Using a Remote gRPC Cache as a Back Cache A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Accessing Coherence Resources"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " The Coherence Java gRPC Client is a library that enables a Java application to connect to a Coherence gRPC proxy server. This chapter includes the following sections: Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Client, you should declare it as an application dependency. There should also be a corresponding Coherence server running the gRPC proxy to which the client can connect. Configure the Coherence gRPC Client Add the gRPC client configuration to the application&#8217;s cache configuration file. Accessing Coherence Resources The simplest way to access the remote Coherence resources, such as a NamedMap when using the gRPC client is through a Coherence Session . Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Java client, you should declare it as a dependency of your project. The gRPC client is provided in the coherence-java-client module. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-java-client\" } Configure the Coherence gRPC Client Just like Coherence*Extend, a Coherence gRPC client accesses remote clustered resources by configuring remote schemes in the applications cache configuration file. Defining a Remote gRPC Cache A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. A Minimal NameService Configuration The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; A Minimal NameService Configuration with Different Cluster Name If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure the NameService Endpoints If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure Fixed Endpoints If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure SSL To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication Configuring the Client Thread Pool Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; Accessing Coherence Resources As the gRPC client is configured as a remote scheme in the cache configuration file, Coherence resources can be accessed using the same Coherence APIs as used on cluster members or Extend clients. If the client has been started using the Coherence bootstrap API, running a com.tangosol.net.Coherence instance, a Session and NamedMap can be accessed as shown below: <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test-cache\"); Using a Remote gRPC Cache as a Back Cache A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "26 Using the Coherence Java gRPC Client"
        },
        {
            "location": "/coherence-grpc/README",
            "text": " Learn how to use the Coherence gRPC library to interact with a Coherence data management services. This part contains the following chapters: Introduction to gRPC Coherence gRPC provides the protobuf definitions necessary to interact with a Coherence data management services over gRPC. Using the Coherence gRPC Server The Coherence gRPC proxy is the server-side implementation of the services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. Using the Coherence Java gRPC Client The Coherence Java gRPC Client is a library that enables a Java application to connect to a Coherence gRPC proxy server. 24 Introduction to gRPC Coherence gRPC for Java allows Java applications to access Coherence clustered services, including data, data events, and data processing from outside the Coherence cluster. Typical uses for Java gRPC clients include desktop and Web applications that require access to remote Coherence resources. This provides an alternative to using Coherence*Extend when writing client applications. Note The Coherence gRPC client and Coherence Extend client feature sets do not match exactly, some functionality in gRPC is not available in Extend and vice-versa. The Coherence gRPC for Java library connects to a Coherence clustered service instance running within the Coherence cluster using a high performance gRPC based communication layer. This library sends all client requests to the Coherence clustered gRPC proxy service which, in turn, responds to client requests by delegating to an actual Coherence clustered service (for example, a partitioned cache service). Like cache clients that are members of the cluster, Java gRPC clients use the Session API call to retrieve a resources such as NamedMap , NamedCache , etc. After it is obtained, a client accesses these resources in the same way as it would if it were part of the Coherence cluster. The fact that operations on Coherence resources are being sent to a remote cluster node (over gRPC) is completely transparent to the client application. There are two parts to Coherence gRPC, the coherence-grpc-proxy module, that provides the server-side gRPC proxy, and the coherence-java-client module that provides the gRPC client. Other non-java Coherence clients are also available that use the Coherence gRPC protocol. 25 Using the Coherence gRPC Proxy Server The Coherence gRPC proxy is the server-side implementation of the gRPC services defined within the Coherence gRPC module. The gRPC proxy uses standard gRPC Java libraries to provide Coherence APIs over gRPC. This chapter includes the following sections: Setting Up the Coherence gRPC Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. Configuring the Server Configuring the gRPC Server includes setting the server port, specifying the in-process server name, and enabling TLS. Disabling the gRPC Proxy Server The Coherence gRPC server starts automatically based on the lifecycle events of DefaultCacheServer , but it can be disabled. Deploying the Proxy Service with Helidon Microprofile gRPC Server If you use the Helidon Microprofile server with the microprofile gRPC server enabled, you can deploy the Coherence gRPC proxy into the Helidon gRPC server instead of the Coherence default gRPC server. Setting Up the Coherence gRPC Proxy Server To set up and start using the Coherence gRPC Server, you should declare it as a dependency of your project. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-grpc-proxy&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-grpc-proxy\" } Starting the Server The gRPC server starts automatically when you run com.tangosol.coherence.net.Coherence (or com.tangosol.coherence.net.DefaultCacheServer ). Typically, com.tangosol.coherence.net.Coherence class should be used as the application’s main class. Alternatively, you can start an instance of com.tangosol.coherence.net.Coherence by using the Bootstrap API. By default, the gRPC server will listen on all local addresses using an ephemeral port. Just like with Coherence*Extend, the endpoints the gRPC server has bound to can be discovered by a client using the Coherence NameService, so using ephemeral ports allows the server to start without needing to be concerned with port clashes. When reviewing the log output, two log messages appear as shown below to indicate which ports the gRPC server has bound to. <markup >In-Process GrpcAcceptor is now listening for connections using name \"default\" GrpcAcceptor now listening for connections on 0.0.0.0:55550 The service is ready to process requests from one of the Coherence gRPC client implementations. Configuring the Server The Coherence gRPC proxy is configured using an internal default cache configuration file named grpc-proxy-cache-config.xml which only contains a single &lt;proxy-scheme&gt; configuration for the gRPC proxy. There is no reason to override this file as the server can be configured with System properties and environment variables. Configuring the gRPC Server Listen Address and Port The address and port that the gRPC server binds to when starting can be configured at runtime by setting system properties or environment variables. By default, the server binds to the address 0.0.0.0 which equates to all the local host&#8217;s network interfaces. This can be changed by setting the coherence.grpc.server.address system property or COHERENCE_GRPC_SERVER_ADDRESS environment variable. For example, if the host had a local IP address 192.168.0.25 the server could be configured to bind to just this address as follows: Using System properties <markup >-Dcoherence.grpc.server.address=192.168.0.2 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_ADDRESS=192.168.0.2 The port that the gRPC server binds to can be configured using the coherence.grpc.server.port system property or COHERENCE_GRPC_SERVER_PORT environment variable For example, to configure the server to listen on port 1408: Using System properties <markup >-Dcoherence.grpc.server.port=1408 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_PORT=1408 Configuring SSL/TLS In common with the rest of Coherence, the Coherence gRPC server can be configured to use SSL by specifying the name of a socket provider. Named socket providers are configured in the Coherence operational configuration file (override file). There are various ways to configure an SSL socket provider, which are covered in the Coherence documentation section Using SSL to Secure Communication Once a named socket provider has been configured, the gRPC server can be configured to use that provider by setting the coherence.grpc.server.socketprovider system property or COHERENCE_GRPC_SERVER_SOCKETPROVIDER environment variable. For example, if a socket provider named tls has been configured in the operational configuration file, the gRPC server can be configured to use it: <markup lang=\"xml\" title=\"tangosol-coherence-override.xml\" > &lt;socket-providers&gt; &lt;socket-provider id=\"tls\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"coherence.security.key\"&gt;server.key&lt;/key&gt; &lt;cert system-property=\"coherence.security.cert\"&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"coherence.security.ca.cert\"&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/socket-providers&gt; Using System properties <markup >-Dcoherence.grpc.server.socketprovider=tls Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_SOCKETPROVIDER=tls For more information on socket providers see Using SSL to Secure Communication Configuring the gRPC Server Thread Pool Like other Coherence services, the gRPC server uses a dynamically sized thread pool to process requests. The thread pool size can be configured if the dynamic sizing algorithm provies to not be optimal. Set the Minimum Thread Count Adjusting the minimum number of threads can be useful when dealing with bursts in load. Sometimes it can take the dynamic pool some time to increase the thread count to a suitable number to quickly deal with an increase in load. Setting the minimum size will ensure there are always a certain number of threads to service load. The minimum number of threads in the pool can be set using the coherence.grpc.server.threads.min system property, or the COHERENCE_GRPC_SERVER_THREADS_MIN environment variable. For example, the minimum thread count can be set to 10 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.min=10 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MIN=10 Set the Maximum Thread Count Adjusting the maximum number of threads can be useful to stop the dynamic pool going too high and consuming too much CPU resource. The maximum number of threads in the pool can be set using the coherence.grpc.server.threads.max system property, or the COHERENCE_GRPC_SERVER_THREADS_MAX environment variable. If both maximum and minimum thread counts are specified, the maximum thread count should obviously be set to a value higher than the minimum thread count. For example, the maximum thread count can be set to 20 as shown below: Using System properties <markup >-Dcoherence.grpc.server.threads.max=20 Using environment variables <markup lang=\"bash\" >export COHERENCE_GRPC_SERVER_THREADS_MAX=20 Disabling the gRPC Proxy Server If the coherence-grpc-proxy module is on the class path (or module path) then the gRPC server will be started automatically. This behaviour can be disabled by setting the coherence.grpc.enabled system property or COHERENCE_GRPC_ENABLED environment variable to false . 26 Using the Coherence Java gRPC Client The Coherence Java gRPC Client is a library that enables a Java application to connect to a Coherence gRPC proxy server. This chapter includes the following sections: Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Client, you should declare it as an application dependency. There should also be a corresponding Coherence server running the gRPC proxy to which the client can connect. Configure the Coherence gRPC Client Add the gRPC client configuration to the application&#8217;s cache configuration file. Accessing Coherence Resources The simplest way to access the remote Coherence resources, such as a NamedMap when using the gRPC client is through a Coherence Session . Setting Up the Coherence gRPC Client To set up and start using the Coherence gRPC Java client, you should declare it as a dependency of your project. The gRPC client is provided in the coherence-java-client module. For example: If using Maven, declare the server as follows (where coherence.groupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherence.version property is the version of Coherence being used: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.groupId}&lt;/groupId&gt; &lt;artifactId&gt;coherence-java-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependencies&gt; Or with Gradle, declare the server as follows (where coherenceGroupId is either the Coherence commercial group id, com.oracle.coherence or the CE group id com.oracle.coherence.ce , and the coherenceVersion property is the version of Coherence being used: <markup lang=\"groovy\" title=\"build.gradle\" >dependencies { implementation platform(\"${coherenceGroupId}:coherence-bom:${coherenceVersion}\") implementation \"${coherenceGroupId}:coherence\" implementation \"${coherenceGroupId}:coherence-java-client\" } Configure the Coherence gRPC Client Just like Coherence*Extend, a Coherence gRPC client accesses remote clustered resources by configuring remote schemes in the applications cache configuration file. Defining a Remote gRPC Cache A remote gRPC cache is specialized cache service that routes cache operations to a cache on the Coherence cluster via the gRPC proxy. The remote cache and the cache on the cluster must have the same cache name. Coherence gRPC clients use the NamedMap or NamedCache interfaces as normal to get an instance of the cache. At runtime, the cache operations are not executed locally but instead are sent using gRPC to a gRPC proxy service on the cluster. The fact that the cache operations are delegated to a cache on the cluster is transparent to the client. A remote gRPC cache is defined within a &lt;caching-schemes&gt; section using the &lt;remote-grpc-cache-scheme&gt; element. There are two approaches to configure a gRPC client: NameService - the gRPC client uses the Coherence NameService to discover the gRPC endpoints in the cluster. This is the simplest configuration. Coherence will discover all the endpoints in the cluster that the gRPC proxy is listening on and the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Fixed Endpoints - a fixed set of gRPC endpoints can be supplied, either hard coded or via a custom AddressProvider configuration. If multiple endpoints are provided, the gRPC Java library&#8217;s standard client-side load balancer will load balance connections from the client to those proxy endpoints. Some approaches work in some types of deployment environment and not in others, for example the NameService configurations are not suitable where the cluster is inside a containerized environment, such as Kubernetes and the client is external to this. Choose the simplest configuration that works in your environment. If both clients and cluster are inside the same containerized environment the NameService will work. In containerized environments such as Kubernetes, this is typically configured with a single ingress point which load balances connections to the Coherence cluster Pods. The address of this ingress point is then used as a single fixed address in the &lt;remote-grpc-cache-scheme&gt; configuration. A Minimal NameService Configuration The simplest configuration for a gRPC client is to use the NameService to locate the gRPC proxy endpoints, but without adding any address or port information in the &lt;remote-grpc-cache-scheme&gt; in the configuration file. This configuration will use Coherence&#8217;s default cluster discovery mechanism to locate the Coherence cluster&#8217;s NameService and look up the gRPC endpoints. This requires the client to be configured with the same cluster name and well-known-address list (or multicast configuration) as the cluster being connected to. The example below shows a &lt;remote-grpc-cache-scheme&gt; configured with just &lt;scheme-name&gt; and &lt;service-name&gt; elements. This is the absolute minimum, required configuration. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; A Minimal NameService Configuration with Different Cluster Name If the client is configured with a different cluster name to the cluster being connected to (for example the client is actually in a different Coherence cluster), then the &lt;remote-grpc-cache-scheme&gt; can be configured with a cluster name. For example, the &lt;remote-grpc-cache-scheme&gt; below is configured with &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; so Coherence will use the NameService to discover the gRPC endpoints in the Coherence cluster named test-cluster . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;cluster-name&gt;test-cluster&lt;/cluster-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure the NameService Endpoints If the client cannot use the standard Coherence cluster discovery mechanism to look up the target cluster, the NameService endpoints can be supplied in the &lt;grpc-channel&gt; section of the &lt;remote-grpc-cache-scheme&gt; configuration. The example below creates a remote cache scheme that is named RemoteGrpcCache , which connects to the Coherence NameService on 198.168.1.5:7574 , which then redirects the request to the address of the gRPC proxy service. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;name-service-addresses&gt; &lt;socket-address&gt; &lt;address&gt;198.168.1.5&lt;/address&gt; &lt;port&gt;7574&lt;/port&gt; &lt;/socket-address&gt; &lt;/name-service-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure Fixed Endpoints If the NameService cannot be used to discover the gRPC endpoints, a fixed set of addresses can be configured. In the &lt;grpc-channel&gt; section, configure a &lt;remote-addresses&gt; element containing one or more &lt;socket-address&gt; elements. For example, the client configured below will connect to a gRPC proxy listening on the endpoint test-cluster.svc:1408 . <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; Configure SSL To configure the client to use SSL a socket provider can be configured in the &lt;grpc-channel&gt; section. Socket providers are configured exactly the same way as in other parts of Coherence. The &lt;socket-provider&gt; element can either contain the name of a socket provider configured in the Operational override file, or can be configured with an inline socket provider configuration. For example, the &lt;remote-grpc-cache-scheme&gt; is configured with a reference to the socket provider named ssl that is configured in the operational override file. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt;ssl&lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; The &lt;remote-grpc-cache-scheme&gt; below is configured with an inline socket provider. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;socket-provider&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key&gt;server.key&lt;/key&gt; &lt;cert&gt;server.cert&lt;/cert&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert&gt;server-ca.cert&lt;/cert&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; For more information on socket providers see Using SSL to Secure Communication Configuring the Client Thread Pool Unlike an Extend client, the gRPC client is built on top of a gRPC asynchronous client. This is configured with a thread pool, to allow the client to process multiple parallel requests and responses. The thread pool used by the gRPC client is a standard Coherence dynamically sized thread pool, the number of threads will automatically be adjusted depending on load. Sometimes Coherence does not adjust the thread pool optimally for an application use-case, so it can be configured to set the pool size. Any of the thread count, minimum thread count and maximum thread count can be configured. Obviously the thread-count must be greater than or equal to the minimum count, and less than or equal the maximum count, and the maximum count must be greater than or equal to the minimum count. To configure a fixed size pool, just set the minimum and maximum to the same value. The example below configures all three thread counts. The pool will start with 10 threads and by automatically sized between 5 and 15 threads depending on load. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address&gt;test-cluster.svc&lt;/address&gt; &lt;port&gt;1408&lt;/port&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;thread-count&gt;10&lt;/thread-count&gt; &lt;thread-count-max&gt;15&lt;/thread-count-max&gt; &lt;thread-count-min&gt;5&lt;/thread-count-min&gt; &lt;/remote-grpc-cache-scheme&gt; Accessing Coherence Resources As the gRPC client is configured as a remote scheme in the cache configuration file, Coherence resources can be accessed using the same Coherence APIs as used on cluster members or Extend clients. If the client has been started using the Coherence bootstrap API, running a com.tangosol.net.Coherence instance, a Session and NamedMap can be accessed as shown below: <markup lang=\"java\" >Session session = Coherence.getInstance().getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test-cache\"); Using a Remote gRPC Cache as a Back Cache A remote gRPC cache can be used as the back cache of a near-cache or a view-cache in the same way as other types of caches. The example below shows a near scheme configured to use a &lt;remote-grpc-cache-scheme&gt; as the back scheme. <markup lang=\"xml\" title=\"coherence-cache-config.xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;high-units&gt;10000&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-ref&gt;remote-grpc&lt;/scheme-ref&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/back-scheme&gt; &lt;/near-scheme&gt; &lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;remote-grpc&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;/remote-grpc-cache-scheme&gt; &lt;/caching-schemes&gt; ",
            "title": "Part V Getting Started with gRPC"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " A Coherence application JVM can only be a member of zero or one Coherence cluster at any point in time. It can however, connect to zero, one or many Coherence clusters as a client. When building a Coherence client application (either an Extend client or a gRPC client) the application may need to connect to more than one Coherence cluster. Exactly how this is achieved, and the relative simplicity, depends on the version of Coherence being used. This example uses the Bootstrap API introduced in Coherence CE 20.12, and enhancements made to it in 22.06. ",
            "title": "Multi-Cluster Client"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The withParameter method on the SessionConfiguration.Builder is used to pass parameters to the cache configuration. Cache configuration files can be parameterized using the system-property attribute on elements. Typically, the values used for these elements are taken from corresponding system properties or environment variables. By using the withParameter method on a SessionConfiguration.Builder values for these elements can also be provided. For example, the &lt;remote-cache-scheme&gt; below has the &lt;address&gt; and &lt;port&gt; elements parameterized. The &lt;address&gt; element&#8217;s value will come from the coherence.extend.address System property (or COHERENCE_EXTEND_ADDRESS environment variable). The &lt;port&gt; element&#8217;s value will come from the coherence.extend.port System property (or COHERENCE_EXTEND_PORT environment variable). There are a number of alternative ways to configures the address for a remote gRPC scheme, which are covered on the Coherence documentation. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"/&gt; &lt;port system-property=\"coherence.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When creating a SessionConfiguration , those values can also be specified as configuration parameters. <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) ",
            "title": "Session Configuration Parameters"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Creating an Extend client could be as simple as the example below. This example will create an Extend client using the default Coherence cache configuration (as the withConfigUri has not been used to specify a cache configuration file). Both the session name and scope name are set to the tenant name. A number of parameters are also set in the configuration, this is explained further below. <markup lang=\"java\" >public Session getSession(String tenant) { Coherence coherence = Coherence.getInstance(); Optional&lt;Session&gt; optional = Coherence.findSession(tenant); if (optional.isPresent()) { return optional.get(); } coherence.addSessionIfAbsent(tenant, () -&gt; SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withParameter(\"coherence.client\", \"remote-fixed\") .withParameter(\"coherence.serializer\", \"java\") .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) .build()); return coherence.getSession(tenant); } Obtain the default Coherence instance Find the Session with the tenant name The Session has already been created, so use it Use the Coherence.addSessionIfAbsent() method to add a SessionConfiguration for the tenant. The addSessionIfAbsent method is used to be slightly more thread safe. Return the Session for the configuration name just added Session Configuration Parameters The withParameter method on the SessionConfiguration.Builder is used to pass parameters to the cache configuration. Cache configuration files can be parameterized using the system-property attribute on elements. Typically, the values used for these elements are taken from corresponding system properties or environment variables. By using the withParameter method on a SessionConfiguration.Builder values for these elements can also be provided. For example, the &lt;remote-cache-scheme&gt; below has the &lt;address&gt; and &lt;port&gt; elements parameterized. The &lt;address&gt; element&#8217;s value will come from the coherence.extend.address System property (or COHERENCE_EXTEND_ADDRESS environment variable). The &lt;port&gt; element&#8217;s value will come from the coherence.extend.port System property (or COHERENCE_EXTEND_PORT environment variable). There are a number of alternative ways to configures the address for a remote gRPC scheme, which are covered on the Coherence documentation. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"/&gt; &lt;port system-property=\"coherence.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When creating a SessionConfiguration , those values can also be specified as configuration parameters. <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) ",
            "title": "Create an Extend Client Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Since Coherence 22.06.2, creating a gRPC client session is a simple as creating an Extend client session. A &lt;remote-grpc-cache-scheme&gt; can be configured in a Coherence cache configuration file. The &lt;remote-grpc-cache-scheme&gt; can contain a &lt;grpc-channel&gt; element that configures the channel that the client will use to connect to the gRPC proxy in the Coherence cluster. There are a number of alternative ways to configure the &lt;remote-grpc-cache-scheme&gt; and &lt;grpc-channel&gt; elements, which are covered on the Coherence documentation. An example of a &lt;remote-grpc-cache-scheme&gt; is shown below. In this case the &lt;grpc-channel&gt; is configured with a single fixed address that the gRPC client connects to. The &lt;address&gt; and &lt;port&gt; elements below do not actually have values, the values of those elements will be supplied by the coherence.grpc.address and coherence.grpc.port system properties or by the COHERENCE_GRPC_ADDRESS and COHERENCE_GRPC_PORT environment variables, or by setting them in the Session configuration properties. <markup lang=\"xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;thin-grpc-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.grpc.address\"/&gt; &lt;port system-property=\"coherence.grpc.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; When creating a SessionConfiguration , the &lt;address&gt; and &lt;port&gt; values can also be specified as configuration parameters. For example, the SessionConfiguration below will configure the gRPC channel to connect to loopback ( 127.0.0.1 ) and port 1408 . <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.grpc.address\", \"127.0.0.1\") .withParameter(\"coherence.grpc.port\", 1408) ",
            "title": "Create a gRPC Client Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Using the bootstrap API, access to Coherence resources is via an instance of com.tangosol.net.Session where either the default Session is used, or one or more sessions are configured at start-up. In a multi-tenant client, where all the tenants are known ahead of time, this pattern is still usable, a client Session can be configured at start-up for each tenant. In this example the tenant list can be dynamic, to make the example a little more interesting and to show how to create a new Session at runtime. The bootstrap API starts one or more Coherence instances, each of which manages one or more Session instances. For the multi-tenant use case there is no need to have multiple Coherence instances. The default instance can be used, then new tenant specific client sessions are created as required. Every Session must have a unique name (and for an Extend client session, either a unique cache configuration URI, or a unique scope name). This example uses the same cache configuration file for all tenants (which is how most multi-tenant applications would work) so the session&#8217;s name and scope name are both set to the tenant name to ensure uniqueness. Create an Extend Client Session Creating an Extend client could be as simple as the example below. This example will create an Extend client using the default Coherence cache configuration (as the withConfigUri has not been used to specify a cache configuration file). Both the session name and scope name are set to the tenant name. A number of parameters are also set in the configuration, this is explained further below. <markup lang=\"java\" >public Session getSession(String tenant) { Coherence coherence = Coherence.getInstance(); Optional&lt;Session&gt; optional = Coherence.findSession(tenant); if (optional.isPresent()) { return optional.get(); } coherence.addSessionIfAbsent(tenant, () -&gt; SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withParameter(\"coherence.client\", \"remote-fixed\") .withParameter(\"coherence.serializer\", \"java\") .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) .build()); return coherence.getSession(tenant); } Obtain the default Coherence instance Find the Session with the tenant name The Session has already been created, so use it Use the Coherence.addSessionIfAbsent() method to add a SessionConfiguration for the tenant. The addSessionIfAbsent method is used to be slightly more thread safe. Return the Session for the configuration name just added Session Configuration Parameters The withParameter method on the SessionConfiguration.Builder is used to pass parameters to the cache configuration. Cache configuration files can be parameterized using the system-property attribute on elements. Typically, the values used for these elements are taken from corresponding system properties or environment variables. By using the withParameter method on a SessionConfiguration.Builder values for these elements can also be provided. For example, the &lt;remote-cache-scheme&gt; below has the &lt;address&gt; and &lt;port&gt; elements parameterized. The &lt;address&gt; element&#8217;s value will come from the coherence.extend.address System property (or COHERENCE_EXTEND_ADDRESS environment variable). The &lt;port&gt; element&#8217;s value will come from the coherence.extend.port System property (or COHERENCE_EXTEND_PORT environment variable). There are a number of alternative ways to configures the address for a remote gRPC scheme, which are covered on the Coherence documentation. <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;thin-remote-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.extend.address\"/&gt; &lt;port system-property=\"coherence.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; When creating a SessionConfiguration , those values can also be specified as configuration parameters. <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.extend.address\", \"127.0.0.1\") .withParameter(\"coherence.extend.port\", 20000) Create a gRPC Client Session Since Coherence 22.06.2, creating a gRPC client session is a simple as creating an Extend client session. A &lt;remote-grpc-cache-scheme&gt; can be configured in a Coherence cache configuration file. The &lt;remote-grpc-cache-scheme&gt; can contain a &lt;grpc-channel&gt; element that configures the channel that the client will use to connect to the gRPC proxy in the Coherence cluster. There are a number of alternative ways to configure the &lt;remote-grpc-cache-scheme&gt; and &lt;grpc-channel&gt; elements, which are covered on the Coherence documentation. An example of a &lt;remote-grpc-cache-scheme&gt; is shown below. In this case the &lt;grpc-channel&gt; is configured with a single fixed address that the gRPC client connects to. The &lt;address&gt; and &lt;port&gt; elements below do not actually have values, the values of those elements will be supplied by the coherence.grpc.address and coherence.grpc.port system properties or by the COHERENCE_GRPC_ADDRESS and COHERENCE_GRPC_PORT environment variables, or by setting them in the Session configuration properties. <markup lang=\"xml\" >&lt;remote-grpc-cache-scheme&gt; &lt;scheme-name&gt;thin-grpc-fixed&lt;/scheme-name&gt; &lt;service-name&gt;RemoteGrpcCache&lt;/service-name&gt; &lt;grpc-channel&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"coherence.grpc.address\"/&gt; &lt;port system-property=\"coherence.grpc.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/grpc-channel&gt; &lt;/remote-grpc-cache-scheme&gt; When creating a SessionConfiguration , the &lt;address&gt; and &lt;port&gt; values can also be specified as configuration parameters. For example, the SessionConfiguration below will configure the gRPC channel to connect to loopback ( 127.0.0.1 ) and port 1408 . <markup lang=\"java\" >SessionConfiguration.builder() .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.grpc.address\", \"127.0.0.1\") .withParameter(\"coherence.grpc.port\", 1408) ",
            "title": "Client Sessions"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The simplest way to run the example is to build the image and run the application web-server in a container. The example Maven and Gradle build files contain tasks to pull together all the dependencies and docker file into a directory. For Maven this will be target/docker and for Gradle this will be build/docker . The build then executes the Docker build command in that directory to build the image. Using Maven: <markup lang=\"bash\" >./mvnw clean package -DskipTests -P build-image Using Gradle <markup lang=\"bash\" >./gradlew clean buildImage Both of the commands above will create two images, one for the example server and one for the client Server image ghcr.io/coherence-community/multi-cluster-server:latest Client image ghcr.io/coherence-community/multi-cluster-client:latest ",
            "title": "Build the Example Image"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Build the Example Image The simplest way to run the example is to build the image and run the application web-server in a container. The example Maven and Gradle build files contain tasks to pull together all the dependencies and docker file into a directory. For Maven this will be target/docker and for Gradle this will be build/docker . The build then executes the Docker build command in that directory to build the image. Using Maven: <markup lang=\"bash\" >./mvnw clean package -DskipTests -P build-image Using Gradle <markup lang=\"bash\" >./gradlew clean buildImage Both of the commands above will create two images, one for the example server and one for the client Server image ghcr.io/coherence-community/multi-cluster-server:latest Client image ghcr.io/coherence-community/multi-cluster-client:latest ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The example application is a web-server that gets data to service requests from a specific Coherence cluster depending on a header value in the http request. This demonstrates a simple stateless multi-tenant web-server, where the tenant&#8217;s data is segregated into different Coherence clusters. The information about a tenant&#8217;s connection details are held in a meta-data cache in a separate admin cluster. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Build the Example Image The simplest way to run the example is to build the image and run the application web-server in a container. The example Maven and Gradle build files contain tasks to pull together all the dependencies and docker file into a directory. For Maven this will be target/docker and for Gradle this will be build/docker . The build then executes the Docker build command in that directory to build the image. Using Maven: <markup lang=\"bash\" >./mvnw clean package -DskipTests -P build-image Using Gradle <markup lang=\"bash\" >./gradlew clean buildImage Both of the commands above will create two images, one for the example server and one for the client Server image ghcr.io/coherence-community/multi-cluster-server:latest Client image ghcr.io/coherence-community/multi-cluster-client:latest ",
            "title": "Building the Example"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " So that the client can communicate with the cluster members, a Docker network is required. The command below will create a Docker network named coherence-net <markup lang=\"bash\" >docker network create --driver bridge coherence-net ",
            "title": "Create a Docker Network"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The example requires three clusters. The first is the tenant \"admin\" cluster that holds information about the tenants. Then there are two additional clusters for each tenant, in this case \"Marvel\" and \"Star Wars\" Start the admin cluster, this will hold tenant meta-data. <markup lang=\"bash\" >docker run -d --name tenants --network coherence-net \\ -e COHERENCE_CLUSTER=tenants \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Marvel tenant. <markup lang=\"bash\" >docker run -d --name marvel --network coherence-net \\ -e COHERENCE_CLUSTER=marvel \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Star Wars tenant. <markup lang=\"bash\" >docker run -d --name star-wars --network coherence-net \\ -e COHERENCE_CLUSTER=star-wars \\ ghcr.io/coherence-community/multi-cluster-server:latest After starting all three clusters, the docker ps command can be used to check their status. Eventually the STATUS colum of each container should say (healthy) . <markup >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4abdc735b7bd ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp star-wars 5df54737eb6a ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp marvel 87f9ee53dfc5 ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 3 minutes ago Up 3 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp tenants ",
            "title": "Start the Coherence Clusters"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " When all the clusters are running and healthy, the multi-tenant client can be started using the command below. This will start the webserver and expose the endpoints on http://127.0.0.1:8080 . <markup lang=\"bash\" >docker run -d --name webserver --network coherence-net \\ -e COHERENCE_EXTEND_ADDRESS=tenants \\ -e COHERENCE_EXTEND_PORT=20000 \\ -p 8080:8080 \\ ghcr.io/coherence-community/multi-cluster-client:latest Using docker ps the status of the webserver container should eventually be (healthy) too. ",
            "title": "Start the Web-Server"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Once the webserver container is healthy the /tenants endpoint can be used to create the metadata for the two tenants. The curl command below will add the meta-data for the Marvel tenant. This will connect to the Marvel cluster using Coherence Extend on port 20000. The default extend proxy port in the server container is 20000. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"marvel\",\"type\":\"extend\",\"hostName\":\"marvel\",\"port\":20000,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:15:26 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"marvel\", \"port\":20000, \"serializer\":\"java\", \"tenant\":\"marvel\", \"type\":\"extend\" } The curl command below will add the meta-data for the Star Wars tenant. This will connect to the Star Wars cluster using Coherence gRPC API on port 1408. The default gRPC port in the server container is 1408. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"star-wars\",\"type\":\"grpc\",\"hostName\":\"star-wars\",\"port\":1408,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:17:49 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"star-wars\", \"port\":1408, \"serializer\":\"java\", \"tenant\":\"star-wars\", \"type\":\"grpc\" } ",
            "title": "Create the Tenant Meta-Data"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The demo is complete so everything can be cleaned up. <markup lang=\"bash\" >docker rm -f webserver tenants marvel star-wars docker network rm coherence-net ",
            "title": "Clean-Up"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " First, try a simple GET request without a tenant header value. <markup lang=\"bash\" >curl -i -w '' -X GET http://127.0.0.1:8080/users/foo This should return a 400 bad request response as shown below <markup lang=\"bash\" >HTTP/1.1 400 Bad Request Date: Thu, 07 Jul 2022 15:33:23 GMT Transfer-encoding: chunked {\"Error\":\"Missing tenant identifier\"} Now try the same get, with a valid tenant identifier in the header. <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/foo This should return a 404, as no users have been created yet. <markup lang=\"bash\" >HTTP/1.1 404 Not Found Date: Thu, 07 Jul 2022 15:35:26 GMT Transfer-encoding: chunked {\"Error\":\"Unknown user foo\"} Create a User in the Marvel cluster with the command below, using the marvel tenant identifier in the header: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X POST http://127.0.0.1:8080/users \\ -d '{\"firstName\":\"Iron\",\"lastName\":\"Man\",\"email\":\"iron.man@marvel.com\"}' The response should be a 200 response, with the json of the user created. This will include the ID of the new user, in this case the ID is Iron.Man . <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:37:04 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.User\", \"email\":\"iron.man@marvel.com\", \"firstName\":\"Iron\", \"id\":\"Iron.Man\", \"lastName\":\"Man\" } Now get the Iron.Man user from the Marvel cluster: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/Iron.Man This should respond with a 200 response code and the same json as above. Next, try to get the Iron.Man user from the Star Wars cluster by using the star-wars tenant ID in the header <markup lang=\"bash\" >curl -i -w '' -H 'tenant: star-wars' -X GET http://127.0.0.1:8080/users/Iron.Man The response should be a 404, not-found, as the Iron.Man user is not in the Star Wars tenant&#8217;s cluster. Clean-Up The demo is complete so everything can be cleaned up. <markup lang=\"bash\" >docker rm -f webserver tenants marvel star-wars docker network rm coherence-net ",
            "title": "Access the Multi-Tenant Endpoints"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The point of this example is to show a client connecting to multiple clusters, so running the examples requires also running a number of Coherence clusters. To make running simple, each clusters will just be a single member. Create a Docker Network So that the client can communicate with the cluster members, a Docker network is required. The command below will create a Docker network named coherence-net <markup lang=\"bash\" >docker network create --driver bridge coherence-net Start the Coherence Clusters The example requires three clusters. The first is the tenant \"admin\" cluster that holds information about the tenants. Then there are two additional clusters for each tenant, in this case \"Marvel\" and \"Star Wars\" Start the admin cluster, this will hold tenant meta-data. <markup lang=\"bash\" >docker run -d --name tenants --network coherence-net \\ -e COHERENCE_CLUSTER=tenants \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Marvel tenant. <markup lang=\"bash\" >docker run -d --name marvel --network coherence-net \\ -e COHERENCE_CLUSTER=marvel \\ ghcr.io/coherence-community/multi-cluster-server:latest Start the cluster for the Star Wars tenant. <markup lang=\"bash\" >docker run -d --name star-wars --network coherence-net \\ -e COHERENCE_CLUSTER=star-wars \\ ghcr.io/coherence-community/multi-cluster-server:latest After starting all three clusters, the docker ps command can be used to check their status. Eventually the STATUS colum of each container should say (healthy) . <markup >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4abdc735b7bd ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp star-wars 5df54737eb6a ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 2 minutes ago Up 2 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp marvel 87f9ee53dfc5 ghcr.io/coherence-community/multi-cluster-server:latest \"java -cp /app/class…\" 3 minutes ago Up 3 minutes (healthy) 1408/tcp, 9612/tcp, 20000/tcp tenants Start the Web-Server When all the clusters are running and healthy, the multi-tenant client can be started using the command below. This will start the webserver and expose the endpoints on http://127.0.0.1:8080 . <markup lang=\"bash\" >docker run -d --name webserver --network coherence-net \\ -e COHERENCE_EXTEND_ADDRESS=tenants \\ -e COHERENCE_EXTEND_PORT=20000 \\ -p 8080:8080 \\ ghcr.io/coherence-community/multi-cluster-client:latest Using docker ps the status of the webserver container should eventually be (healthy) too. Create the Tenant Meta-Data Once the webserver container is healthy the /tenants endpoint can be used to create the metadata for the two tenants. The curl command below will add the meta-data for the Marvel tenant. This will connect to the Marvel cluster using Coherence Extend on port 20000. The default extend proxy port in the server container is 20000. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"marvel\",\"type\":\"extend\",\"hostName\":\"marvel\",\"port\":20000,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:15:26 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"marvel\", \"port\":20000, \"serializer\":\"java\", \"tenant\":\"marvel\", \"type\":\"extend\" } The curl command below will add the meta-data for the Star Wars tenant. This will connect to the Star Wars cluster using Coherence gRPC API on port 1408. The default gRPC port in the server container is 1408. <markup lang=\"bash\" >curl -i -w '' -X POST http://127.0.0.1:8080/tenants \\ -d '{\"tenant\":\"star-wars\",\"type\":\"grpc\",\"hostName\":\"star-wars\",\"port\":1408,\"serializer\":\"java\"}' This should return a 200 response as show below: <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:17:49 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.TenantMetaData\", \"hostName\":\"star-wars\", \"port\":1408, \"serializer\":\"java\", \"tenant\":\"star-wars\", \"type\":\"grpc\" } Access the Multi-Tenant Endpoints First, try a simple GET request without a tenant header value. <markup lang=\"bash\" >curl -i -w '' -X GET http://127.0.0.1:8080/users/foo This should return a 400 bad request response as shown below <markup lang=\"bash\" >HTTP/1.1 400 Bad Request Date: Thu, 07 Jul 2022 15:33:23 GMT Transfer-encoding: chunked {\"Error\":\"Missing tenant identifier\"} Now try the same get, with a valid tenant identifier in the header. <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/foo This should return a 404, as no users have been created yet. <markup lang=\"bash\" >HTTP/1.1 404 Not Found Date: Thu, 07 Jul 2022 15:35:26 GMT Transfer-encoding: chunked {\"Error\":\"Unknown user foo\"} Create a User in the Marvel cluster with the command below, using the marvel tenant identifier in the header: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X POST http://127.0.0.1:8080/users \\ -d '{\"firstName\":\"Iron\",\"lastName\":\"Man\",\"email\":\"iron.man@marvel.com\"}' The response should be a 200 response, with the json of the user created. This will include the ID of the new user, in this case the ID is Iron.Man . <markup lang=\"bash\" >HTTP/1.1 200 OK Date: Thu, 07 Jul 2022 15:37:04 GMT Transfer-encoding: chunked { \"@class\":\"com.oracle.coherence.guides.client.model.User\", \"email\":\"iron.man@marvel.com\", \"firstName\":\"Iron\", \"id\":\"Iron.Man\", \"lastName\":\"Man\" } Now get the Iron.Man user from the Marvel cluster: <markup lang=\"bash\" >curl -i -w '' -H 'tenant: marvel' -X GET http://127.0.0.1:8080/users/Iron.Man This should respond with a 200 response code and the same json as above. Next, try to get the Iron.Man user from the Star Wars cluster by using the star-wars tenant ID in the header <markup lang=\"bash\" >curl -i -w '' -H 'tenant: star-wars' -X GET http://127.0.0.1:8080/users/Iron.Man The response should be a 404, not-found, as the Iron.Man user is not in the Star Wars tenant&#8217;s cluster. Clean-Up The demo is complete so everything can be cleaned up. <markup lang=\"bash\" >docker rm -f webserver tenants marvel star-wars docker network rm coherence-net ",
            "title": "Running the Example"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The data model used in this example is a simple User entity, with an id, a first name, a last name and an email address. A snippet of the source is shown below, the actual code includes serialization support for both Java and portable object format serialization. <markup lang=\"java\" title=\"User.java\" >/** * A simple user entity. */ public class User implements PortableObject, ExternalizableLite { /** * The user's identifier. */ private String id; /** * The user's first name. */ private String firstName; /** * The user's last name. */ private String lastName; /** * The user's email address. */ private String email; /** * A default constructor, required for Coherence serialization. */ public User() { } /** * Create a user. * * @param id the user's identifier * @param firstName the user's first name * @param lastName the user's last name * @param email the user's email address */ public User(String id, String firstName, String lastName, String email) { this.id = id; this.firstName = firstName; this.lastName = lastName; this.email = email; } /** * Returns the user's identifier. * * @return the user's identifier */ public String getId() { return id; } /** * Set the user's identifier. * * @param id the user's identifier */ public void setId(String id) { this.id = id; } /** * Returns the user's first name. * * @return the user's first name */ public String getFirstName() { return firstName; } /** * Set the user's first name. * * @param firstName the user's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the user's last name. * * @return the user's last name */ public String getLastName() { return lastName; } /** * Set the user's last name. * * @param lastName the user's last name */ public void setLastName(String lastName) { this.lastName = lastName; } /** * Returns the user's email address. * * @return the user's email address */ public String getEmail() { return email; } /** * Set the user's email address. * * @param email the user's email address */ public void setEmail(String email) { this.email = email; } } ",
            "title": "The Data Model"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The application does not have a main class with a main method. The Application class in the example code implements Coherence.LifecycleListener and will be discovered by Coherence using the Java ServiceLoader . The Application class then receives events when the Coherence bootstrap API starts and stops Coherence. Using these events, the Application class configures, starts and stops the web-server. The actual code is not discussed in detail here as it is not particularly relevant for the example. The application is started by running the com.tangosol.net.Coherence class; in this case Coherence is started as a client. ",
            "title": "The Main Class"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The TenantController class is one of two classes that expose REST endpoints in the web-server. The purpose of the TenantController is to perform CRUD operations on tenants, to allow runtime configuration of tenants, and which cluster a given tenant should connect to. Tenant meta-data is contained in a TenantMetaData class. This holds the tenant name, the host name and port of the Coherence cluster holding the tenants data and whether the client session should use Coherence Extend or Coherence gRPC to connect to the cluster. ",
            "title": "The TenantController"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The UserController class is the main class in the example. This class exposes some REST endpoints to perform CRUD operations on User entities. There are four methods supported by the controller, POST to create a user, 'PUT' to update a user, GET to get a user and DELETE to delete a user. Every request much contain a tenant header with the name of the tenant as the header value. Any request without a tenant header is rejected. ",
            "title": "The UserController"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The application is a very basic CRUD application to manage simple User entities in a Coherence cache. The application exposes a REST API with get, create (POST), update (PUT) and delete methods. The application is multi-tenanted, so each request has the relevant tenant identifier in a request header. Requests without a tenant identifier, or with an unknown tenant, are rejected. The web-server is a Coherence client application, and is hence storage disabled. The data for each tenant is held in a separate storage enabled Coherence cluster for each tenant. There is an admin cluster that holds the meta-data about tenants. This allows tenants and their cluster details to be added and maintained at runtime. The main reason for doing this in the example, is because it makes testing much simpler. A stateless web server that accesses data to service requests from different Coherence clusters has various pros and cons with its design, but the purpose of this example is to show connecting Coherence clients to different clusters, its purpose is not to produce the best, most efficient, web application. There is certainly no security built in to this example. The actual web-server implementation used in this example is unimportant and not really relevant to the example code. The code shown here could easily be ported to other web-application frameworks, such as Coherence CDI with Helidon , Coherence and Spring , or Coherence and Micronaut , etc. The Data Model The data model used in this example is a simple User entity, with an id, a first name, a last name and an email address. A snippet of the source is shown below, the actual code includes serialization support for both Java and portable object format serialization. <markup lang=\"java\" title=\"User.java\" >/** * A simple user entity. */ public class User implements PortableObject, ExternalizableLite { /** * The user's identifier. */ private String id; /** * The user's first name. */ private String firstName; /** * The user's last name. */ private String lastName; /** * The user's email address. */ private String email; /** * A default constructor, required for Coherence serialization. */ public User() { } /** * Create a user. * * @param id the user's identifier * @param firstName the user's first name * @param lastName the user's last name * @param email the user's email address */ public User(String id, String firstName, String lastName, String email) { this.id = id; this.firstName = firstName; this.lastName = lastName; this.email = email; } /** * Returns the user's identifier. * * @return the user's identifier */ public String getId() { return id; } /** * Set the user's identifier. * * @param id the user's identifier */ public void setId(String id) { this.id = id; } /** * Returns the user's first name. * * @return the user's first name */ public String getFirstName() { return firstName; } /** * Set the user's first name. * * @param firstName the user's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the user's last name. * * @return the user's last name */ public String getLastName() { return lastName; } /** * Set the user's last name. * * @param lastName the user's last name */ public void setLastName(String lastName) { this.lastName = lastName; } /** * Returns the user's email address. * * @return the user's email address */ public String getEmail() { return email; } /** * Set the user's email address. * * @param email the user's email address */ public void setEmail(String email) { this.email = email; } } The Main Class The application does not have a main class with a main method. The Application class in the example code implements Coherence.LifecycleListener and will be discovered by Coherence using the Java ServiceLoader . The Application class then receives events when the Coherence bootstrap API starts and stops Coherence. Using these events, the Application class configures, starts and stops the web-server. The actual code is not discussed in detail here as it is not particularly relevant for the example. The application is started by running the com.tangosol.net.Coherence class; in this case Coherence is started as a client. The TenantController The TenantController class is one of two classes that expose REST endpoints in the web-server. The purpose of the TenantController is to perform CRUD operations on tenants, to allow runtime configuration of tenants, and which cluster a given tenant should connect to. Tenant meta-data is contained in a TenantMetaData class. This holds the tenant name, the host name and port of the Coherence cluster holding the tenants data and whether the client session should use Coherence Extend or Coherence gRPC to connect to the cluster. The UserController The UserController class is the main class in the example. This class exposes some REST endpoints to perform CRUD operations on User entities. There are four methods supported by the controller, POST to create a user, 'PUT' to update a user, GET to get a user and DELETE to delete a user. Every request much contain a tenant header with the name of the tenant as the header value. Any request without a tenant header is rejected. ",
            "title": "The Example Application"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The work of obtaining or creating a Session for a tenant is in the UserController.ensureSession method. <markup lang=\"java\" title=\"UserController.java\" > private Session ensureSession(String tenant) { TenantMetaData metaData = tenants.get(tenant); if (metaData == null) { return null; } Coherence coherence = Coherence.getInstance(); return coherence.getSessionIfPresent(tenant) .orElseGet(() -&gt; createSession(coherence, metaData)); } The meta-data for the tenant is obtained from the tenants cache. If there is no meta-data in the cache, the method returns null . The default Coherence instance is obtained, as this will be the owner of all the client Session instances. The Coherence.getSessionIfPresent() method is called, which will return an existing Session for a given tenant name if one exists. The Coherence.getSessionIfPresent() returns an Optional&lt;Session&gt; and if this is empty, the supplier in the orElseGet() method is called, which calles the UserController.createSession() method to actually create a Session . ",
            "title": "The ensureSession Method"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " If a Session does not yet exist for a tenant, one must be created from the TenantMetaData for the tenant. The UserController.createSession() method is responsible for creating a Session for a tenant. <markup lang=\"java\" title=\"UserController.java\" > private Session createSession(Coherence coherence, TenantMetaData metaData) { String tenant = metaData.getTenant(); if (metaData.isExtend()) { coherence.addSessionIfAbsent(tenant, () -&gt; createExtendConfiguration(metaData)); } else { coherence.addSessionIfAbsent(tenant, () -&gt; createGrpcConfiguration(metaData)); } return coherence.getSession(tenant); } The createSession method is very simple, it just delegates to another method, depending on whether the required Session is for an Extend client or a gRPC client. A SessionConfiguration is created, either for an Extend client, or gRPC client, and is passed to the Coherence.addSessionIfAbsent() method. The add if absent method is used in case multiple threads attempt to create the same tenant&#8217;s session, it will only be added once. ",
            "title": "The createSession Method"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " A Session is simple to add to a running Coherence instance. It just requires creating a SessionConfiguration instance and adding it to the Coherence instance. An Extend client configuration can be created using the SessionConfiguration builder. <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createExtendConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.extend.address\", metaData.getHostName()) .withParameter(\"coherence.extend.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to remote-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address Extend client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.extend.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.extend.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. ",
            "title": "Creating an Extend Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " Creating a gRPC Session is as simple as creating an Extend Session . <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createGrpcConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.grpc.address\", metaData.getHostName()) .withParameter(\"coherence.grpc.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to grpc-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address gRPC client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.grpc.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.grpc.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. ",
            "title": "Creating a gRPC Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " The example code could be simplified if the application only ever used Extend or only ever used gRPC. There are also many alternative approaches to holding tenant metata data used to create the sessions. The important parts of the example are the methods in UserController to obtain a session from the Coherence instance, and create a new Session is one does not already exist. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " When a request comes to the UserController a Coherence Session must be obtained for the tenant. All the requests perform the same logic, the code below is from the get request handler. <markup lang=\"java\" title=\"UserController.java\" > public Response get(HttpRequest request) { String tenant = request.getHeaderString(TENANT_HEADER); if (tenant == null || tenant.isBlank()) { return Response.status(400).entity(Map.of(\"Error\", \"Missing tenant identifier\")).build(); } Session session = ensureSession(tenant); if (session == null) { return Response.status(400).entity(Map.of(\"Error\", \"Unknown tenant \" + tenant)).build(); } The \"tenant\" header value is obtained from the request If there is no tenant header a 400 response is returned A Session is obtained for the tenant (this is covered in detail below) If the Session is null , a 400 response is returned Once a valid Session has been obtained for the tenant, the rest of the request processing can continue. The ensureSession Method The work of obtaining or creating a Session for a tenant is in the UserController.ensureSession method. <markup lang=\"java\" title=\"UserController.java\" > private Session ensureSession(String tenant) { TenantMetaData metaData = tenants.get(tenant); if (metaData == null) { return null; } Coherence coherence = Coherence.getInstance(); return coherence.getSessionIfPresent(tenant) .orElseGet(() -&gt; createSession(coherence, metaData)); } The meta-data for the tenant is obtained from the tenants cache. If there is no meta-data in the cache, the method returns null . The default Coherence instance is obtained, as this will be the owner of all the client Session instances. The Coherence.getSessionIfPresent() method is called, which will return an existing Session for a given tenant name if one exists. The Coherence.getSessionIfPresent() returns an Optional&lt;Session&gt; and if this is empty, the supplier in the orElseGet() method is called, which calles the UserController.createSession() method to actually create a Session . The createSession Method If a Session does not yet exist for a tenant, one must be created from the TenantMetaData for the tenant. The UserController.createSession() method is responsible for creating a Session for a tenant. <markup lang=\"java\" title=\"UserController.java\" > private Session createSession(Coherence coherence, TenantMetaData metaData) { String tenant = metaData.getTenant(); if (metaData.isExtend()) { coherence.addSessionIfAbsent(tenant, () -&gt; createExtendConfiguration(metaData)); } else { coherence.addSessionIfAbsent(tenant, () -&gt; createGrpcConfiguration(metaData)); } return coherence.getSession(tenant); } The createSession method is very simple, it just delegates to another method, depending on whether the required Session is for an Extend client or a gRPC client. A SessionConfiguration is created, either for an Extend client, or gRPC client, and is passed to the Coherence.addSessionIfAbsent() method. The add if absent method is used in case multiple threads attempt to create the same tenant&#8217;s session, it will only be added once. Creating an Extend Session A Session is simple to add to a running Coherence instance. It just requires creating a SessionConfiguration instance and adding it to the Coherence instance. An Extend client configuration can be created using the SessionConfiguration builder. <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createExtendConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.extend.address\", metaData.getHostName()) .withParameter(\"coherence.extend.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to remote-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address Extend client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.extend.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.extend.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Creating a gRPC Session Creating a gRPC Session is as simple as creating an Extend Session . <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createGrpcConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.grpc.address\", metaData.getHostName()) .withParameter(\"coherence.grpc.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to grpc-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address gRPC client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.grpc.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.grpc.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Summary The example code could be simplified if the application only ever used Extend or only ever used gRPC. There are also many alternative approaches to holding tenant metata data used to create the sessions. The important parts of the example are the methods in UserController to obtain a session from the Coherence instance, and create a new Session is one does not already exist. ",
            "title": "Creating a Tenant&#8217;s Session"
        },
        {
            "location": "/examples/guides/910-multi-cluster-client/README",
            "text": " In this example, the tenants are dynamic and Coherence client sessions are created on-demand using meta-data held in a tenants cache. An alternative would have been to create all the tenant client sessions when the application started up using the Coherence Bootstrap API to configure them. This would have been a much simpler example, but then testing and demonstrating it would have been harder. A more dynamic multi-tenant system is probably closer to a real-world scenario. When the web-server starts it connects using Coherence Extend to the tenant meta-data cluster and obtains a reference to a Coherence NamedMap to hold tenant meta-data. Creating a Tenant&#8217;s Session When a request comes to the UserController a Coherence Session must be obtained for the tenant. All the requests perform the same logic, the code below is from the get request handler. <markup lang=\"java\" title=\"UserController.java\" > public Response get(HttpRequest request) { String tenant = request.getHeaderString(TENANT_HEADER); if (tenant == null || tenant.isBlank()) { return Response.status(400).entity(Map.of(\"Error\", \"Missing tenant identifier\")).build(); } Session session = ensureSession(tenant); if (session == null) { return Response.status(400).entity(Map.of(\"Error\", \"Unknown tenant \" + tenant)).build(); } The \"tenant\" header value is obtained from the request If there is no tenant header a 400 response is returned A Session is obtained for the tenant (this is covered in detail below) If the Session is null , a 400 response is returned Once a valid Session has been obtained for the tenant, the rest of the request processing can continue. The ensureSession Method The work of obtaining or creating a Session for a tenant is in the UserController.ensureSession method. <markup lang=\"java\" title=\"UserController.java\" > private Session ensureSession(String tenant) { TenantMetaData metaData = tenants.get(tenant); if (metaData == null) { return null; } Coherence coherence = Coherence.getInstance(); return coherence.getSessionIfPresent(tenant) .orElseGet(() -&gt; createSession(coherence, metaData)); } The meta-data for the tenant is obtained from the tenants cache. If there is no meta-data in the cache, the method returns null . The default Coherence instance is obtained, as this will be the owner of all the client Session instances. The Coherence.getSessionIfPresent() method is called, which will return an existing Session for a given tenant name if one exists. The Coherence.getSessionIfPresent() returns an Optional&lt;Session&gt; and if this is empty, the supplier in the orElseGet() method is called, which calles the UserController.createSession() method to actually create a Session . The createSession Method If a Session does not yet exist for a tenant, one must be created from the TenantMetaData for the tenant. The UserController.createSession() method is responsible for creating a Session for a tenant. <markup lang=\"java\" title=\"UserController.java\" > private Session createSession(Coherence coherence, TenantMetaData metaData) { String tenant = metaData.getTenant(); if (metaData.isExtend()) { coherence.addSessionIfAbsent(tenant, () -&gt; createExtendConfiguration(metaData)); } else { coherence.addSessionIfAbsent(tenant, () -&gt; createGrpcConfiguration(metaData)); } return coherence.getSession(tenant); } The createSession method is very simple, it just delegates to another method, depending on whether the required Session is for an Extend client or a gRPC client. A SessionConfiguration is created, either for an Extend client, or gRPC client, and is passed to the Coherence.addSessionIfAbsent() method. The add if absent method is used in case multiple threads attempt to create the same tenant&#8217;s session, it will only be added once. Creating an Extend Session A Session is simple to add to a running Coherence instance. It just requires creating a SessionConfiguration instance and adding it to the Coherence instance. An Extend client configuration can be created using the SessionConfiguration builder. <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createExtendConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.ClientFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.extend.address\", metaData.getHostName()) .withParameter(\"coherence.extend.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to remote-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address Extend client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.extend.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.extend.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Creating a gRPC Session Creating a gRPC Session is as simple as creating an Extend Session . <markup lang=\"java\" title=\"UserController.java\" > private SessionConfiguration createGrpcConfiguration(TenantMetaData metaData) { String tenant = metaData.getTenant(); return SessionConfiguration.builder() .named(tenant) .withScopeName(tenant) .withMode(Coherence.Mode.GrpcFixed) .withParameter(\"coherence.serializer\", metaData.getSerializer()) .withParameter(\"coherence.grpc.address\", metaData.getHostName()) .withParameter(\"coherence.grpc.port\", metaData.getPort()) .build(); } The session configuration has a unique name, in this case the tenant name A session configuration typically has a unique scope, in this case also the tenant name The coherence.client parameter is set to grpc-fixed . This is used by the default Coherence cache configuration file to make it use a fixed address gRPC client configuration. The name of the serializer is configured (in this example Java serialization is used, but \"pof\" would also be supported) The coherence.grpc.address property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. The coherence.grpc.port property is passed through to the cache configuration file, in this case the address comes from the tenant meta-data. finally the configuration is built and returned. Summary The example code could be simplified if the application only ever used Extend or only ever used gRPC. There are also many alternative approaches to holding tenant metata data used to create the sessions. The important parts of the example are the methods in UserController to obtain a session from the Coherence instance, and create a new Session is one does not already exist. ",
            "title": "Implementing Multi-Tenancy"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Each of the features above is backed by one or more Coherence caches, possibly with preconfigured interceptors, but for the most part you shouldn&#8217;t care about that: all interaction with lower level Coherence primitives is hidden behind various factory classes that allow you to get the instances of the classes you need. For example, you will use factory methods within Atomics class to get instances of various atomic types, Locks to get lock instances, Latches and Semaphores to get, well, latches and semaphores. ",
            "title": "Factory Classes"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " In many cases the factory classes will allow you to get both the local and the remote instances of various constructs. For example, Locks.localLock will give you an instance of a standard java.util.concurrent.locks.ReentrantLock , while Locks.remoteLock will return an instance of a RemoteLock . In cases where JDK doesn&#8217;t provide a standard interface, which is the case with atomics, latches and semaphores, we&#8217;ve extracted the interface from the existing JDK class, and created a thin wrapper around the corresponding JDK implementation. For example, Coherence Concurrent provides a Semaphore interface, and LocalSemaphore class that wraps java.util.concurrent.Semaphore . The same is true for the CountDownLatch , and all atomic types. The main advantage of using factory classes to construct both the local and the remote instances is that it allows you to name local locks the same way you have to name remote locks: calling Locks.localLock(\"foo\") will always return the same Lock instance, as the Locks class internally caches both the local and the remote instances it created. Of course, in the case of remote locks, every locally cached remote lock instance is ultimately backed by a shared lock instance somewhere in the cluster, which is used to synchronize lock state across the processes. ",
            "title": "Local vs Remote"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent supports both Java serialization and POF out-of-the-box, with Java serialization being the default. If you want to use POF instead, you will need to specify that by setting coherence.concurrent.serializer system property to pof . You will also need to include coherence-concurrent-pof-config.xml into your own POF configuration file, in order to register built-in Coherence Concurrent types. ",
            "title": "Serialization"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent supports both active and on-demand persistence, but just like in the rest of Coherence it is set to on-demand by default. In order to use active persistence you should set coherence.concurrent.persistence.environment system property to default-active , or another persistence environment that has active persistence enabled. ",
            "title": "Persistence"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent module provides distributed implementations of the concurrency primitives from the java.util.concurrent package that you are already familiar with, such as executors, atomics, locks, semaphores and latches. This allows you to implement concurrent applications using the constructs you are already familiar with, but to expand the \"scope\" of concurrency from a single process to potentially hundreds of processes within a Coherence cluster. You can use executors to submit tasks to be executed somewhere in the cluster; you can use locks, latches and semaphores to synchronize execution across many cluster members; you can use atomics to implement global counters across many processes, etc. Please keep in mind that while these features are extremely powerful and allow you to reuse the knowledge you already have, they may have detrimental effect on scalability and/or performance. Whenever you synchronize execution via locks, latches or semaphores, you are introducing a potential bottleneck into the architecture. Whenever you use a distributed atomic to implement a global counter, you are turning very simple operations that take mere nanoseconds locally, such as increment and decrement, into fairly expensive network calls that could take milliseconds (and potentially block even longer under heavy load). So, use these features sparingly. In many cases there is a better, faster and more scalable way to accomplish the same goal using Coherence primitives such as entry processors, aggregators and events, which were designed to perform and scale well in a distributed environment from the get go. Factory Classes Each of the features above is backed by one or more Coherence caches, possibly with preconfigured interceptors, but for the most part you shouldn&#8217;t care about that: all interaction with lower level Coherence primitives is hidden behind various factory classes that allow you to get the instances of the classes you need. For example, you will use factory methods within Atomics class to get instances of various atomic types, Locks to get lock instances, Latches and Semaphores to get, well, latches and semaphores. Local vs Remote In many cases the factory classes will allow you to get both the local and the remote instances of various constructs. For example, Locks.localLock will give you an instance of a standard java.util.concurrent.locks.ReentrantLock , while Locks.remoteLock will return an instance of a RemoteLock . In cases where JDK doesn&#8217;t provide a standard interface, which is the case with atomics, latches and semaphores, we&#8217;ve extracted the interface from the existing JDK class, and created a thin wrapper around the corresponding JDK implementation. For example, Coherence Concurrent provides a Semaphore interface, and LocalSemaphore class that wraps java.util.concurrent.Semaphore . The same is true for the CountDownLatch , and all atomic types. The main advantage of using factory classes to construct both the local and the remote instances is that it allows you to name local locks the same way you have to name remote locks: calling Locks.localLock(\"foo\") will always return the same Lock instance, as the Locks class internally caches both the local and the remote instances it created. Of course, in the case of remote locks, every locally cached remote lock instance is ultimately backed by a shared lock instance somewhere in the cluster, which is used to synchronize lock state across the processes. Serialization Coherence Concurrent supports both Java serialization and POF out-of-the-box, with Java serialization being the default. If you want to use POF instead, you will need to specify that by setting coherence.concurrent.serializer system property to pof . You will also need to include coherence-concurrent-pof-config.xml into your own POF configuration file, in order to register built-in Coherence Concurrent types. Persistence Coherence Concurrent supports both active and on-demand persistence, but just like in the rest of Coherence it is set to on-demand by default. In order to use active persistence you should set coherence.concurrent.persistence.environment system property to default-active , or another persistence environment that has active persistence enabled. ",
            "title": "Distributed Concurrency"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides a facility to dispatch tasks, either a Runnable or Callable to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. ",
            "title": "Overview"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. ",
            "title": "Usage Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. ",
            "title": "Configuration Elements"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; ",
            "title": "Configuration Examples"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread Fixed thread Creates an ExecutorService with a fixed number of threads Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; ",
            "title": "Configuration"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. ",
            "title": "ExecutorMBean Attributes"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. ",
            "title": "Operations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. ",
            "title": "Management"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON ",
            "title": "Management over REST"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Overview Coherence Concurrent provides a facility to dispatch tasks, either a Runnable or Callable to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. Usage Examples By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. Configuration Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread Fixed thread Creates an ExecutorService with a fixed number of threads Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; Management The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. Management over REST Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON CDI Support RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . ",
            "title": "Executors"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. ",
            "title": "Asynchronous Implementations"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides distributed implementations of atomic types, such as AtomicInteger , AtomicLong and AtomicReference . It also provides local implementations of the same types. The local implementations are just thin wrappers around existing java.util.concurrent.atomic types, which implement the same interface as their distributed variants, in order to be interchangeable. To create instances of atomic types you need to call the appropriate factory method on the Atomics class: <markup lang=\"java\" >AtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); AtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); AtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); creates a local, in-process instance of named AtomicInteger with an implicit initial value of 0 creates a remote, distributed instance of named AtomicInteger , distinct from the local instance foo , with an implicit initial value of 0 creates a remote, distributed instance of named AtomicLong , with an initial value of 5 Note that the AtomicInteger and AtomicLong types used above are not types from the java.util.concurrent.atomic package they you are familiar with&#8201;&#8212;&#8201;they are actually interfaces defined within com.oracle.coherence.concurrent.atomic package, that both LocalAtomicXyz and RemoteAtomicXyz classes implement, which are the instances that are actually returned by the methods above. That means that the above code could be rewritten as: <markup lang=\"java\" >LocalAtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); RemoteAtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); RemoteAtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); However, we strongly suggest that you use interfaces instead of concrete types, as they make it easy to switch between local and distributed implementations when necessary. Once created, these instances can be used the same way you would use any of the corresponding java.util.concurrent.atomic types: <markup lang=\"java\" >int counter1 = remoteFoo.incrementAndGet(); long counter5 = remoteBar.addAndGet(5L); Asynchronous Implementations The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. CDI Support Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. ",
            "title": "Atomics"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } ",
            "title": "Exclusive Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock() try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock() try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } ",
            "title": "Read/Write Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent provides distributed implementations of Lock and ReadWriteLock interfaces from the java.util.concurrent.locks package, allowing you to implement lock-based concurrency control across cluster members when necessary. Unlike local JDK implementations, the classes in this package use cluster member/process ID and thread ID to identify lock owner, and store shared lock state within a Coherence NamedMap . However, that also implies that the calls to acquire and release locks are remote, network calls, as they need to update shared state that is likely stored on a different cluster member, which will have an impact on performance of lock and unlock operations. Exclusive Locks A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } Read/Write Locks A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock() try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock() try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } CDI Support You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. ",
            "title": "Locks"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. ",
            "title": "Count Down Latch"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. ",
            "title": "Semaphore"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "CDI Support"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " Coherence Concurrent also provides distributed implementations of a CountDownLatch and Semaphore classes from java.util.concurrent package, allowing you to implement synchronization of execution across multiple Coherence cluster members as easily as you can implement it within a single process using those two JDK classes. It also provides interfaces for those two concurrency primitives, that both remote and local implementations conform to. Just like with atomics, the local implementations are nothing more than thin wrappers around corresponding JDK classes. Count Down Latch The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. Semaphore The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. CDI Support You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "Latches and Semaphores"
        },
        {
            "location": "/coherence-concurrent/README",
            "text": " In order to use Coherence Concurrent features, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-concurrent&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using the features it provides, as the following sections describe. Executors Executors Overview Executors Usage Executors Configuration Executors Configuration Examples Executors Management Executors Management over REST CDI Support for Executors Atomics Non-blocking Atomics CDI Support for Atomics Locks Exclusive Locks Read/Write Locks CDI Support for Locks Latches and Semaphores Count Down Latch Semaphore CDI Support for Latches and Semaphores Executors Overview Coherence Concurrent provides a facility to dispatch tasks, either a Runnable or Callable to a Coherence cluster for execution. Executors that will actually execute the submitted tasks are configured on each cluster member by defining one or more named executors within a cache configuration resource. Usage Examples By default, each Coherence cluster with the coherence-concurrent module on the classpath, will include a single-threaded executor that may be used to execute dispatched tasks. Given this, the simplest example would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.getDefault(); Future&lt;Void&gt; result = remoteExecutor.submit(() -&gt; System.out.println(\"Executed\")); result.get(); // block until completion If for example, an executor was configured named Fixed5 , the code would be: <markup lang=\"java\" >RemoteExecutor remoteExecutor = RemoteExecutor.get(\"Fixed5\"); If no executor has been configured with the given name, the RemoteExecutor will throw RejectedExecutionException . Each RemoteExecutor instance may hold local resources that should be released when the RemoteExecutor is no longer needed. Like an ExecutorService , a RemoteExecutor has similar methods to shut the executor down. When calling these methods, it will have no impact on the executors registered within the cluster. Configuration Several executor types are available for configuration. ExecutorService Type Description Single thread Creates an ExecutorService with a single thread Fixed thread Creates an ExecutorService with a fixed number of threads Cached Create an ExecutorService that will create new threads as needed and reuse existing threads when possible Work stealing Creates a work-stealing thread pool using the number of available processors as its target parallelism level. Configuration Elements Element Name Required Expected Type Description single no N/A Defines a single-thread executor fixed no N/A Defines a fixed-thread-pool executor cached no N/A Defines a cached-thread-pool executor work-stealing no N/A Defines a work-stealing-pool executor name yes java.lang.String Defines the logical name of the executor thread-count yes java.lang.Integer Defines the thread count for a fixed thread pool executor. parallelism no java.lang.Integer Defines the parallelism of a work-stealing thread pool executor. If not defined, it will default to the number of processors available on the system. thread-factory no N/A Defines a java.util.concurrent.ThreadFactory. Used by single , fixed , and cached executors. instance yes java.util.concurrent.ThreadFactory Defines how the ThreadFactory will be instantiated. See the docs for details on the instance element. This element must be a child of the thread-factory element. See the schema for full details. Configuration Examples To define executors, the cache-config root element needs to include the coherence-concurrent NamespaceHandler in order to recognize the configuration elements. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:c=\"class://com.oracle.coherence.concurrent.config.NamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd class://com.oracle.coherence.concurrent.config.NamespaceHandler concurrent.xsd\"&gt; . . . &lt;/cache-config&gt; Executors defined by configuration must precede any other elements in the document. Failing to do so, will prevent the document from validating. The following examples assume the xml namespace defined for the NamespaceHandler is c : <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; --&gt; &lt;c:single&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a single-threaded executor named &lt;em&gt;Single&lt;/em&gt; with a thread factor--&gt; &lt;c:single&gt; &lt;c:name&gt;SingleTF&lt;/c:name&gt; &lt;c:thread-factory&gt; &lt;c:instance&gt; &lt;c:class-name&gt;my.custom.ThreadFactory&lt;/c:class-name&gt; &lt;/c:instance&gt; &lt;/c:thread-factory&gt; &lt;/c:single&gt; <markup lang=\"xml\" >&lt;!-- creates a fixed-thread executor named &lt;em&gt;Fixed5&lt;/em&gt; --&gt; &lt;c:fixed&gt; &lt;c:name&gt;Single&lt;/c:name&gt; &lt;c:thread-count&gt;5&lt;/c:thread-count&gt; &lt;/c:fixed&gt; Management The ExecutorMBean represents the operational state of a registered executor. The object name of the MBean is: <markup >type=Executor,name=&lt;executor name&gt;,nodeId=&lt;cluster node&gt; ExecutorMBean Attributes Attribute Type Access Description MemberId java.lang.String read-only The member ID where the executor is running. Name java.lang.String read-only The logical name of the executor. Id java.lang.String read-only The ID of the registered executor. Description java.lang.String read-only The generated description of the executor. Location java.lang.String read-only The complete location details of the executor. State java.lang.String read-only The current state of the executor. May be one of JOINING , RUNNING , CLOSING_GRACEFULLY , CLOSING , CLOSED or REJECTING . TaskCompletedCount java.lang.Long read-only The number of tasks completed by this executor. TaskRejectedCount java.lang.Long read-only The number of tasks rejected by this executor. TaskInProgressCount java.lang.Long read-only The number of tasks currently running or pending to be run by this executor. TraceLogging java.lang.Boolean read-write Enables executor trace logging (WARNING! VERBOSE). Disabled by default. Operations The ExecutorMBean MBean includes a resetStatistics operation that resets the statistics for this executor. Management over REST Coherence Management over REST exposes endpoints to query and invoke actions against ExecutorMBean instances. Description Method Path Produces View all Executors GET /management/coherence/cluster/executors JSON View all Executors with matching name GET /management/coherence/cluster/executors/{name} JSON Reset Executor statistics by name POST /management/coherence/cluster/executors/{name}/resetStatistics JSON CDI Support RemoteExecutors may be injected via CDI. For example: <markup lang=\"java\" >@Inject private RemoteExecutor single; @Inject @Name(\"Fixed5\") private RemoteExecutor fixedPoolRemoteExecutor; injects a RemoteExecutor named single . injects a RemoteExecutor named Fixed5 . Atomics Coherence Concurrent provides distributed implementations of atomic types, such as AtomicInteger , AtomicLong and AtomicReference . It also provides local implementations of the same types. The local implementations are just thin wrappers around existing java.util.concurrent.atomic types, which implement the same interface as their distributed variants, in order to be interchangeable. To create instances of atomic types you need to call the appropriate factory method on the Atomics class: <markup lang=\"java\" >AtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); AtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); AtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); creates a local, in-process instance of named AtomicInteger with an implicit initial value of 0 creates a remote, distributed instance of named AtomicInteger , distinct from the local instance foo , with an implicit initial value of 0 creates a remote, distributed instance of named AtomicLong , with an initial value of 5 Note that the AtomicInteger and AtomicLong types used above are not types from the java.util.concurrent.atomic package they you are familiar with&#8201;&#8212;&#8201;they are actually interfaces defined within com.oracle.coherence.concurrent.atomic package, that both LocalAtomicXyz and RemoteAtomicXyz classes implement, which are the instances that are actually returned by the methods above. That means that the above code could be rewritten as: <markup lang=\"java\" >LocalAtomicInteger localFoo = Atomics.localAtomicInteger(\"foo\"); RemoteAtomicInteger remoteFoo = Atomics.remoteAtomicInteger(\"foo\"); RemoteAtomicLong remoteBar = Atomics.remoteAtomicLong(\"bar\", 5L); However, we strongly suggest that you use interfaces instead of concrete types, as they make it easy to switch between local and distributed implementations when necessary. Once created, these instances can be used the same way you would use any of the corresponding java.util.concurrent.atomic types: <markup lang=\"java\" >int counter1 = remoteFoo.incrementAndGet(); long counter5 = remoteBar.addAndGet(5L); Asynchronous Implementations The instances of numeric atomic types, such as AtomicInteger and AtomicLong , are frequently used to represent various counters in the application, where a client may need to increment the value, but doesn&#8217;t necessarily need to know what the new value is. When working with the local atomics, the same API shown above can be used, and the return value simply ignored. However, when using distributed atomics that would introduce unnecessary blocking on the client while waiting for the response from the server, which would then simply be discarded. Obviously, this would have negative impact on both performance and throughput of the atomics. To reduce the impact of remote calls in those situations, Coherence Concurrent also provides non-blocking, asynchronous implementations of all atomic types it supports. To obtain a non-blocking instance of any supported atomic type, simply call async method on the blocking instance of that type: <markup lang=\"java\" >AsyncAtomicInteger asyncFoo = Atomics.remoteAtomicInteger(\"foo\").async(); AsyncAtomicLong asyncBar = Atomics.remoteAtomicLong(\"bar\", 5L).async(); creates a remote, distributed instance of named, non-blocking AsyncAtomicInteger , with an implicit initial value of 0 creates a remote, distributed instance of named, non-blocking AsyncAtomicLong , with an initial value of 5 Once created, these instances can be used the same way you would use any of the corresponding blocking types. The only difference is that they will simply return a CompletableFuture for the result, and will not block: <markup lang=\"java\" >CompletableFuture&lt;Integer&gt; futureCounter1 = asyncFoo.incrementAndGet(); CompletableFuture&lt;Long&gt; futureCounter5 = asyncBar.addAndGet(5L); Both the blocking and the non-blocking instance of any distributed atomic type, with the same name, are backed by the same cluster-side atomic instance state, so they can be used interchangeably. CDI Support Atomic types from Coherence Concurrent can also be injected using CDI, which eliminates the need for explicit factory method calls on the Atomics class. <markup lang=\"java\" >@Inject @Name(\"foo\") private AtomicInteger localFoo; @Inject @Remote @Name(\"foo\") private AtomicInteger remoteFoo; @Inject @Remote private AsyncAtomicLong asyncBar injects a local, in-process instance of an AtomicInteger named foo , with an implicit initial value of 0 injects a remote, distributed instance of an AtomicInteger named foo , distinct from the local instance foo , with an implicit initial value of 0 injects a remote, distributed instance of non-blocking AsyncAtomicLong , with an implicit name of asyncBar Once an instance of an atomic type is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Atomics factory class. Locks Coherence Concurrent provides distributed implementations of Lock and ReadWriteLock interfaces from the java.util.concurrent.locks package, allowing you to implement lock-based concurrency control across cluster members when necessary. Unlike local JDK implementations, the classes in this package use cluster member/process ID and thread ID to identify lock owner, and store shared lock state within a Coherence NamedMap . However, that also implies that the calls to acquire and release locks are remote, network calls, as they need to update shared state that is likely stored on a different cluster member, which will have an impact on performance of lock and unlock operations. Exclusive Locks A RemoteLock class provides an implementation of a Lock interface and allows you to ensure that only one thread on one member is running critical section guarded by the lock at any given time. To obtain an instance of a RemoteLock , call Locks.remoteLock factory method: <markup lang=\"java\" >Lock foo = Locks.remoteLock(\"foo\"); Just like with Atomics , you can also obtain a local Lock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantLock , by calling localLock factory method: <markup lang=\"java\" >Lock foo = Locks.localLock(\"foo\"); Once you have a Lock instance, you can use it as you normally would: <markup lang=\"java\" >foo.lock(); try { // critical section guarded by the exclusive lock `foo` } finally { foo.unlock(); } Read/Write Locks A RemoteReadWriteLock class provides an implementation of a ReadWriteLock interface and allows you to ensure that only one thread on one member is running critical section guarded by the write lock at any given time, while allowing multiple concurrent readers. To obtain an instance of a RemoteReadWriteLock , call Locks.remoteReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.remoteReadWriteLock(\"bar\"); Just like with Atomics , you can also obtain a local ReadWriteLock instance from the Locks class, with will simply return an instance of a standard java.util.concurrent.locks.ReentrantReadWriteLock , by calling localReadWriteLock factory method: <markup lang=\"java\" >ReadWriteLock bar = Locks.localReadWriteLock(\"bar\"); Once you have a ReadWriteLock instance, you can use it as you normally would: <markup lang=\"java\" >bar.writeLock().lock() try { // critical section guarded by the exclusive write lock `bar` } finally { bar.writeLock().unlock(); } Or: <markup lang=\"java\" >bar.readLock().lock() try { // critical section guarded by the shared read lock `bar` } finally { bar.readLock().unlock(); } CDI Support You can also use CDI to inject both the exclusive and read/write lock instances into objects that need them: <markup lang=\"java\" >@Inject @Remote @Name(\"foo\") private Lock lock; @Inject @Remote private ReadWriteLock bar; injects distributed exclusive lock named foo into lock field injects distributed read/write lock named bar into bar field Once an instance of lock is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Locks factory class. Latches and Semaphores Coherence Concurrent also provides distributed implementations of a CountDownLatch and Semaphore classes from java.util.concurrent package, allowing you to implement synchronization of execution across multiple Coherence cluster members as easily as you can implement it within a single process using those two JDK classes. It also provides interfaces for those two concurrency primitives, that both remote and local implementations conform to. Just like with atomics, the local implementations are nothing more than thin wrappers around corresponding JDK classes. Count Down Latch The RemoteCoundDownLatch class provides a distributed implementation of a CountDownLatch , and allows you to ensure that the execution of the code on any cluster member that is waiting for the latch proceeds only when the latch reaches zero. Any cluster member can both wait for a latch, and count down. To obtain an instance of a RemoteCountDownLatch , call Latches.remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.remoteCountDownLatch(\"foo\", 5); create an instance of a RemoteCountDownLatch with the initial count of 5 Just like with Atomics and Locks , you can also obtain a local CountDownLatch instance from the Latches class by calling remoteCountDownLatch factory method: <markup lang=\"java\" >CoundDownLatch foo = Latches.localCountDownLatch(\"foo\", 10); create an instance of a LocalCountDownLatch with the initial count of 10 Once you have a RemoteCountDownLatch instance, you can use it as you normally would, by calling countDown and await methods on it. Semaphore The RemoteSemaphore class provides a distributed implementation of a Semaphore , and allows any cluster member to acquire and release permits from the same semaphore instance. To obtain an instance of a RemoteSemaphore , call Semaphores.remoteSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.remoteSemaphore(\"foo\", 5); create an instance of a RemoteSemaphore with 5 permits Just like with Atomics and Locks , you can also obtain a local Semaphore instance from the Semaphores class by calling localSemaphore factory method: <markup lang=\"java\" >Semaphore foo = Semaphores.localSemaphore(\"foo\"); create an instance of a LocalSemaphore with 0 permits Once you have a Semaphore instance, you can use it as you normally would, by calling release and acquire methods on it. CDI Support You can also use CDI to inject both the CountDownLatch and Semaphore instances into objects that need them: <markup lang=\"java\" >@Inject @Name(\"foo\") @Count(5) private CountDownLatch localLatchFoo; @Inject @Name(\"foo\") @Remote @Count(10) private CountDownLatch remoteLatchFoo; @Inject @Name(\"bar\") @Remote private Semaphore localSemaphoreBar; @Inject @Name(\"bar\") @Remote @Permits(1) private Semaphore remoteSemaphoreBar; inject an instance of a LocalCountDownLatch with the initial count of five inject an instance of a RemoteCountDownLatch with the initial count of ten inject an instance of a LocalSemaphore with zero permits available inject an instance of a RemoteSemaphore with one permit available Once a latch or a semaphore instance is obtained via CDI injection, it can be used the same way as an instance obtained directly from the Latches or Semaphores factory classes. The @Name annotation is optional in both cases, as long as the member name (in the examples above, the field name) can be obtained from the injection point, but is required otherwise (such as when using constructor injection). The @Count annotation specifies the initial latch count, and if omitted will be defaulted to one. The @Permits annotation specifies the number of available permits for a semaphore, and if omitted will be defaulted to zero, which means that the first acquire call will block until another thread releases one or more permits. ",
            "title": "Usage"
        },
        {
            "location": "/docs/about/04_important",
            "text": " If using Oracle Coherence Portable Object Format (POF) and/or remote Lambdas, it is recommended to start the JVM with: --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED ",
            "title": "Additional JDK Command Line Arguments"
        },
        {
            "location": "/docs/about/04_important",
            "text": " Coherence CE 23.03 requires a minimum of version 17 of the Java Development Kit (JDK). Additional JDK Command Line Arguments If using Oracle Coherence Portable Object Format (POF) and/or remote Lambdas, it is recommended to start the JVM with: --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang=ALL-UNNAMED ",
            "title": "Java Development Kit Requirements"
        },
        {
            "location": "/docs/about/04_important",
            "text": " Coherence CE 23.03 has migrated to Jakarta EE 9.1 from Java EE 8, importing types in jakarta packages instead of javax packages. The following table describes the mapping of javax packages to jakarta packages and Maven artifacts in Coherence CE 23.03. 'javax' Package 'jakarta' Package Maven Group ID Maven Artifact ID Version javax.activation jakarta.activation jakarta.activation jakarta.activation-api 2.0.1 javax.annotation jakarta.annotation jakarta.annotation jakarta.annotation-api 2.0.0 javax.enterprise jakarta.enterprise jakarta.enterprise jakarta.enterprise.cdi-api 3.0.0 javax.inject jakarta.inject jakarta.inject jakarta.inject-api 2.0.1 javax.interceptor jakarta.interceptor jakarta.interceptor jakarta.interceptor-api 2.0.0 javax.jms jakarta.jms jakarta.jms jakarta.jms-api 3.0.0 javax.json jakarta.json jakarta.json jakarta.json-api 2.0.2 javax.json.bind jakarta.json.bind jakarta.json.bind jakarta.json.bind-api 2.0.0 javax.resource jakarta.resource jakarta.resource jakarta.resource-api 2.0.0 javax.ws.rs jakarta.ws.rs jakarta.ws.rs jakarta.ws.rs-api 3.0.0 javax.xml.bind jakarta.xml.bind jakarta.xml.bind jakarta.xml.bind-api 3.0.1 We&#8217;ve updated our Coherence CE examples to use the jakarta packages where relevant. These examples still hold for older versions of Coherence CE; in these cases developers will need to change from jakarta to javax . In addition to these standard APIs being migrated, we&#8217;ve also updated some of our major dependent libraries that have undertaken this migration as well. Most notably: Library Version Helidon 3.0.0 Jersey 3.0.5 Jackson 2.13.3 Jackson DataBind 2.13.3 Weld 4.0.3.Final JAXB Core 3.0.2 JAXB Implementation 3.0.2 Eclipse MP Config 3.0.1 Eclipse MP Metrics 4.0 Note If using the older jackson-rs-base and jackson-jaxrs-json-provider libraries, it will be necessary to migrate to the 'jakarta' versions. The Maven groupId for the 'jakarta' versions is com.fasterxml.jackson.jakarta.rs with the artifactIds being jackson-jakarta-rs-base and jackson-jakarta-rs-json-provider , respectively. Note If using the older jackson-module-jaxb-annotations library, it will be necessary to migrate to the jakarta versions. The maven groupId for the 'jakarta' version remains the same ( com.fasterxml.jackson.module ), however the artifactId should now be jackson-module-jakarta-xmlbind-annotations ",
            "title": "Jakarta EE 9.1 Compatibility"
        },
        {
            "location": "/docs/about/04_important",
            "text": " The following deprecated packages have been removed from this release: com.oracle.datagrid.persistence com.tangosol.persistence com.oracle.common.base (NOTE: these classes are now in com.oracle.coherence.common.base) ",
            "title": "Deprecated Code Removal"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example H2 R2DBC Non Blocking Entry Store Example Pluggable Cache Stores Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. ",
            "title": "CacheLoader and CacheStore Interfaces"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). ",
            "title": "Simple CacheLoader"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll ",
            "title": "Simple CacheStore"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Enable Write Behind"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. ",
            "title": "Simple Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One ",
            "title": "File Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSqlDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"Customer\"); reloadCustomersDB(); } @Test public void testHSqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >long start = System.nanoTime(); // issue a get and it will load the existing customer Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, namedMap.size()); assertNotNull(customer); assertEquals(1, customer.getId()); assertEquals(\"Customer 1\", customer.getName()); You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >// issue a get again and it should be quicker start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"no read-through\")); You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >// remove a customer number 1 namedMap.remove(1); // we should have one less customer in the database assertEquals(MAX_CUSTOMERS - 1, getCustomerDBCount()); assertNull(namedMap.get(1)); // customer should not exist in DB assertNull(getCustomerFromDB(1)); Issue a get for another customer and then update the customer details. <markup lang=\"java\" >// Load customer 2 Customer customer2 = namedMap.get(2); assertNotNull(customer2); // update customer 2 with \"New Address\" namedMap.compute(2, (k, v)-&gt;{ v.setAddress(\"New Address\"); return v; }); // customer should have new address in cache and DB assertEquals(\"New Address\", namedMap.get(2).getAddress()); assertEquals(\"New Address\", getCustomerFromDB(2).getAddress()); Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >// add a new customer 1010 namedMap.put(101, new Customer(101, \"Customer Name 101\", \"Customer address 101\", 20000)); assertTrue(namedMap.containsKey(101)); assertEquals(\"Customer address 101\", getCustomerFromDB(101).getAddress()); namedMap.remove(101); assertFalse(namedMap.containsKey(101)); assertNull(getCustomerFromDB(101)); Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >// clean the cache and reset the database namedMap.clear(); reloadCustomersDB(); assertEquals(0, namedMap.size()); // demonstrate loading the cache from the current contents of the DB // this can be done many ways but for this exercise you could fetch all the // customer id' from the DB but as we know there are 1..100 we can pretend we have. Set&lt;Integer&gt; keySet = IntStream.rangeClosed(1, 100).boxed().collect(Collectors.toSet()); namedMap.invokeAll(keySet, new PreloadRequest&lt;&gt;()); // cache should be fully primed assertEquals(MAX_CUSTOMERS, namedMap.size()); ",
            "title": "HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerExpiring\"); reloadCustomersDB(); } @Test public void testHSQLDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue a get for customer 1 and log the time to load <markup lang=\"java\" >// expiry delay is setup to 20s for the cache and refresh ahead is 0.5 which // means that after 10s if the entry is read the old value is returned but after which a // refresh is done which means that subsequents reads will be fast as the new value is already present long start = System.nanoTime(); Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, customer.getId()); Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >// update the database updateCustomerCreditLimitInDB(1, 10000); // sleep for 11 seconds get the cache entry, we should still get the original value Base.sleep(11000L); assertEquals(5000, namedMap.get(1).getCreditLimit()); The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >// wait for another 10 seconds and the refresh-ahead should have completed Base.sleep(10000L); start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"after refresh-ahead\")); Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms ",
            "title": "Refresh Ahead HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreWriteBehindTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerWriteBehind\"); } @Test public void testHsqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain no customers assertEquals(0, getCustomerDBCount()); Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >// add 10 customers Map&lt;Integer, Customer&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100; i++) { map.put(i, new Customer(i, \"Name \" + i, \"Address \" + i, i * 1000)); } namedMap.putAll(map); // initial check of the database should return 0 as we have write-delay set assertEquals(0, getCustomerDBCount()); Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >// sleep for 15 seconds and the database should be populated as write-delay has elapsed Base.sleep(15000L); // Issuing Eventually assertThat in case of heavily loaded machine Eventually.assertThat(invoking(this).getCustomerDBCount(), is(100)); You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries ",
            "title": "Write Behind HSQLDb Cache Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " In this H2 R2DBC cache store example, we use the H2Person cache which implements the NonBlockingEntryStore for non-blocking APIs and access to entries in their serialized ( BinaryEntry ) form. Review the Cache Configuration The h2r2dbc-entry-store-cache-config.xml below shows the H2Person cache specifying the class name of the NonBlockingEntryStore implementation. <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;H2Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.H2R2DBCEntryStore&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Unit Test Next we will run the H2R2DBCEntryStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { createTable(); startupCoherence(\"h2r2dbc-entry-store-cache-config.xml\"); } /** * Performs some cache manipulations. */ @Test public void testNonBlockingEntryStore() { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"H2Person\", TypeAssertion.withTypes(Long.class, Person.class)); Person person1 = namedMap.get(Long.valueOf(101)); assertEquals(\"Robert\", person1.getFirstname()); Insert 1 person using a put operation and confirm the data is in the cache. <markup lang=\"java\" >Person person2 = new Person(Long.valueOf(102), 40, \"Tony\", \"Soprano\"); namedMap.put(Long.valueOf(102), person2); Person person3 = namedMap.get(Long.valueOf(102)); assertEquals(\"Tony\", person3.getFirstname()); Delete a couple records and verify the state of the cache. <markup lang=\"java\" >namedMap.remove(Long.valueOf(101)); namedMap.remove(Long.valueOf(102)); assertEquals(null, namedMap.get(Long.valueOf(101))); assertEquals(null, namedMap.get(Long.valueOf(102))); Insert 10 persons using a putAll operation and confirm the data is in the cache. The actual database operations take place in parallel.s <markup lang=\"java\" >Map&lt;Long, Person&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 10; i++) { map.put(Long.valueOf(i), new Person(Long.valueOf(i), 20 + i, \"firstname\" + i, \"lastname\" + i)); } namedMap.putAll(map); Person person5 = namedMap.get(Long.valueOf(5)); assertEquals(\"firstname5\", person5.getFirstname()); assertEquals(10, namedMap.size()); You should see messages indicating activity on the store side: 2021-06-29 15:01:36.365/5.583 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.495/5.713 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore store 2021-06-29 15:01:36.501/5.720 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.504/5.722 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.507/5.726 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.508/5.727 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.509/5.728 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.512/5.730 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Could not find row for key: 101 2021-06-29 15:01:36.515/5.734 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore storeAll ",
            "title": "H2 R2DBC Non Blocking Entry Store Example"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); ",
            "title": "Pluggable Cache Stores"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " You have seen how to use and configure Cache Stores within Coherence. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/190-cache-stores/README",
            "text": " This guide walks you through how to use and configure Cache Stores within Coherence. Coherence supports transparent read/write caching of any data source, including databases, web services, packaged applications and file systems; however, databases are the most common use case. As shorthand, \"database\" is used to describe any back-end data source. Effective caches must support both intensive read-only and read/write operations, and for read/write operations, the cache and database must be kept fully synchronized. To accomplish caching of data sources, Coherence supports Read-Through, Write-Through, Refresh-Ahead and Write-Behind caching. Coherence also supports BinaryEntryStore which provides access to the serialized form of entries for data sources capable of manipulating those. A variant of BinaryEntryStore is the NonBlockingEntryStore which, besides providing access to entries in their BinaryEntry form, integrates with data sources with non-blocking APIs such as R2DBC or Kafka. See the Coherence Documentation for detailed information on Cache Stores. Table of Contents What You Will Build What You Need CacheLoader and CacheStore Interface Simple Cache Store Example Simple CacheLoader Simple CacheStore Enable Write Behind File Cache Store Example HSQLDb Cache Store Example Refresh Ahead Expiring HSQLDb Cache Store Example Write Behind HSQLDb Cache Store Example H2 R2DBC Non Blocking Entry Store Example Pluggable Cache Stores Summary See Also What You Will Build This code is written as a number of separate classes representing the different types of cache stores and can be run as a series of Junit tests to show the functionality. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build As this example consists of Junit tests, please add -DskipTests for Maven or -x test for Gradle. CacheLoader and CacheStore Interfaces Before we go into some examples, we should review two interfaces that are key. CacheLoader - CacheLoader - defines an interface for loading individual entries via a key or a collection keys from a backend database . CacheStore - CacheStore - defines and interface for storing ior erasing individual entries via a key or collection of keys into a backend database . This interface also extends CacheLoader . In the rest of this document we will refer to CacheLoaders and CacheStores as just \"Cache Stores\" for simplicity. Coherence caches have an in-memory backing map on each storage-enabled member to store cache data. When cache stores are defined against a cache, operations are carried out on the cache stores in addition to the backing map. We will explain this in more detail below. Simple Cache Store Example Before we jump straight into using a \"Database\", we will demonstrate how CacheLoaders and CacheStores work by implementing a mock cache loader that outputs messages to help us understand how this works behind the scenes. Simple CacheLoader The CacheLoader interface defines the following methods: public V load(K key) - Return the value associated with the specified key public default Map&lt;K, V&gt; loadAll(Collection&lt;? extends K&gt; colKeys) - Return the values associated with each the specified keys in the passed collection We just need to implement the load method. See below for the SimpleCacheLoader implementation. The implementation of a CacheLoader is also known as Read-Through Caching as if the data is not present in the cache it is read from the cache loader. Review the SimpleCacheLoader <markup lang=\"java\" >public class SimpleCacheLoader implements CacheLoader&lt;Integer, String&gt; { private String cacheName; /** * Constructs a {@link SimpleCacheLoader}. * * @param cacheName cache name */ public SimpleCacheLoader(String cacheName) { this.cacheName = cacheName; Logger.info(\"SimpleCacheLoader constructed for cache \" + this.cacheName); } /** * An implementation of a load which returns the String \"Number \" + the key. * * @param key key whose associated value is to be returned * @return the value for the given key */ @Override public String load(Integer key) { Logger.info(\"load called for key \" + key); return \"Number \" + key; } } Implement a CacheLoader with key Integer and value of String Construct the cache loader passing in the cache name (not used in this case) Implement the load method by returning a String \"Number \" plus the key and log the message We are just logging messages for the sake of this example, and we would recommend that logging only used in rare cases where you might need to signify an error. Review the Cache Configuration simple-cache-loader-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-loader&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-loader scheme Specifies this schema has a cache store Specify the class that implements the CacheLoader interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheLoaderTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-loader-cache-config.xml\"); } @Test public void testSimpleCacheLoader() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // subsequent get will not cause read-through as value is already in cache assertEquals(\"Number 1\", namedMap.get(1)); // Remove the cache entry will cause a read-through again namedMap.remove(1); assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // load multiple keys will load all values namedMap.getAll(new HashSet&lt;&gt;(Arrays.asList(2, 3, 4))); assertEquals(4, namedMap.size()); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store and placed in the cache and returned to the user. See the message from the cache store. Issue a second get against the key 1 and the cache store is not called and returned from the cache Remove the cache entry for key 1 and re-issue the get. The value is read-through from the cache store. Load a Collection of keys, causing each one to be loaded from cache loader. Run the Test For this test and all others you can run the test in one of three ways: Using your IDE Using Maven via mvn clean verify -Dtest=SimpleCacheLoaderTest verify Using Gradle via ./gradlew test --tests SimpleCacheLoaderTest Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCache, member=1): SimpleCacheLoader constructed for cache simple-test ... ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:6, member=1): load called for key 3 Notice there are two loads of the key 1 which are the first get and subsequent get after the value was removed. The following loads are fom the getAll(). Simple CacheStore The CacheStore interface defines the following methods: public void store(K key, V value) - Store the specified value under the specified key in the underlying store public default void storeAll(Map&lt;? extends K, ? extends V&gt; mapEntries) - Store the specified values under the specified keys in the underlying store public void erase(K key) - Remove the specified key from the underlying store if present public default void eraseAll(Collection&lt;? extends K&gt; colKeys) - Remove the specified keys from the underlying store if present Our implementation will extend the SimpleCacheLoader and implement the store and erase methods. See below for the SimpleCacheStore implementation. The implementation of a CacheStore is also known as Write-Through Caching as when the data is written to the cache it is also written through to the back end cache store in the same synchronous operation as the primate and backup. E.g. the client will block until primary, backup and cache store operations are complete. See write-behind on changing this behaviour. We can change Review the SimpleCacheStore <markup lang=\"java\" >public class SimpleCacheStore extends SimpleCacheLoader implements CacheStore&lt;Integer, String&gt; { /** * Constructs a {@link SimpleCacheStore}. * * @param cacheName cache name */ public SimpleCacheStore(String cacheName) { super(cacheName); Logger.info(\"SimpleCacheStore instantiated for cache \" + cacheName); } @Override public void store(Integer integer, String s) { Logger.info(\"Store key \" + integer + \" with value \" + s); } @Override public void erase(Integer integer) { Logger.info(\"Erase key \" + integer); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the cache name (not used in this case) Implement the store method by logging a message Implement the erase method by logging a message Review the Cache Configuration simple-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;simple-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.SimpleCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the simple-cache-store scheme Specifies this schema has a cache store Specify the class that implements the CacheStore interface Pass the cache name using the in-built macro to the constructor Review the Test <markup lang=\"java\" >public class SimpleCacheStoreTest extends AbstractCacheStoreTest { @BeforeAll public static void startup() { startupCoherence(\"simple-cache-store-cache-config.xml\"); } @Test public void testSimpleCacheStore() { NamedMap&lt;Integer, String&gt; namedMap = getSession() .getMap(\"simple-test\", TypeAssertion.withTypes(Integer.class, String.class)); namedMap.clear(); // initial get will cause read-through and the object is placed in the cache and returned to the user assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // update the cache and the the store method is called namedMap.put(1, \"New Value\"); assertEquals(\"New Value\", namedMap.get(1)); // remove the entry from the cache and the erase method is called assertEquals(\"New Value\", namedMap.remove(1)); // Get the cache entry will cause a read-through again (cache loader) assertEquals(\"Number 1\", namedMap.get(1)); assertEquals(1, namedMap.size()); // Issue a puAll Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(2, \"value 2\"); map.put(3, \"value 3\"); map.put(4, \"value 4\"); namedMap.putAll(map); assertEquals(4, namedMap.size()); Base.sleep(20000L); } } Startup the test with the specified cache config Obtain the NamedMap Issue a get against the key 1 and as the cache entry is not present, the value will be loaded from the cache store. (This is the SimpleCacheLoader.load() method) Issue a put against the key 1 and the cache store store method is called and the message is logged Remove the cache entry for key 1 and the cache store erase method is called and a message is logged Issue a get against the key 1 and it will be loaded my the cache loader Issue a putAll on the cache and the cache store storeAll method is called We are not exercising the eraseAll method as this is used internally. Run the Test, using Maven in our case <markup lang=\"bash\" >mvn clean verify -Dtest=SimpleCacheStoreTest verify Running the test shows the following (abbreviated) output on the cache server, where the cache store is running. <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 1 with value New Value ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:7, member=1): Store key 3 with value value 3 Notice the store and erase for key 1 and the store for key 2 , 3 and 4 from the putAll Enable Write Behind Typically, the time taken to write the primary and backup copy of an object is much less that writing to a back-end data store such as a database. These operations may be many orders of magnitude slower e.g. 1-2 ms to write primary and backup and 100-200ms to write to a database. In these cases we can change a cache store to use write-behind. In the Write-Behind scenario, modified cache entries are asynchronously written to the data source after a configured delay, whether after 10 seconds or a day. This only applies to cache inserts and updates - cache entries are removed synchronously from the data source. See the Coherence Documentation for detailed information and explanations on write-behind. The advantages of write-behind are: 1. Improved application performance as the client does not have to wait for the value to be written to the back-end cache store. As long as the primary and backup are complete, the control is returned to the client. 2. The back-end cache store, usually a database, can more efficiently batch updates that one at a time 3. The application can be mostly immune from back-end database failures as the failure can be requeued. Open the Cache Configuration simple-cache-store-cache-config.xml and change the value of the write-delay from the default value of 0s to 5s . This simple change will make the cache store write-behind with a delay of 5 seconds before entries are written to the cache. <markup lang=\"xml\" >&lt;write-delay&gt;0s&lt;/write-delay&gt; Uncomment out the sleep in the SimpleCacheStoreTest class. This is to ensure that the unit test does not exit before the values are written asynchronously to the cache store. This is not required in production systems. <markup lang=\"java\" > Base.sleep(20000L); Run the SimpleCacheStoreTest test <markup lang=\"text\" >... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Erase key 1 ... &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): load called for key 1 DELAY of approx 5s ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 4 with value value 4 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 2 with value value 2 ... &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.SimpleCacheStore):DistributedCache:simple-test, member=1): Store key 3 with value value 3 You will see that there is a delay of at least 5 seconds before the stores for keys 2, 3 and 4. You can see that they are on the thread WriteBehindThread . The load and erase operations are on a DistributedCacheWorker thread and are executed as synchronous operations. File Cache Store Example In this next example, we will create a file-based cache store which stores values in files with the name of the key under a specific directory. This is to show how a back-end cache store, and the cache interact. This is an example only to see how cache stores work under the covers and will not work with multiple cache servers running and is not recommended for production use. Review the FileCacheStore <markup lang=\"java\" >public class FileCacheStore implements CacheStore&lt;Integer, String&gt; { /** * Base directory off which to store data. */ private final File directory; public FileCacheStore(String directoryName) { if (directoryName == null || directoryName.equals(\"\")) { throw new IllegalArgumentException(\"A directory must be specified\"); } directory = new File(directoryName); if (!directory.isDirectory() || !directory.canWrite()) { throw new IllegalArgumentException(\"Unable to open directory \" + directory); } Logger.info(\"FileCacheStore constructed with directory \" + directory); } @Override public void store(Integer key, String value) { try { BufferedWriter writer = new BufferedWriter(new FileWriter(getFile(directory, key), false)); writer.write(value); writer.close(); } catch (IOException e) { throw new RuntimeException(\"Unable to delete key \" + key, e); } } @Override public void erase(Integer key) { // we ignore result of delete as the key may not exist getFile(directory, key).delete(); } @Override public String load(Integer key) { File file = getFile(directory, key); try { // use Java 1.8 method return Files.readAllLines(file.toPath()).get(0); } catch (IOException e) { return null; // does not exist in cache store } } protected static File getFile(File directory, Integer key) { return new File(directory, key + \".txt\"); } } Implement a CacheStore with key Integer and value of String which extends SimpleCacheLoader Construct the cache store passing in the directory to use Implement the store method by writing the String value to a file in the base directory with the key + \".txt\" as the name Implement the erase method by removing the file with the key + \".txt\" as the name Implement the load method by loading the contents of the file with the key + \".txt\" as the name Review the Cache Configuration file-cache-store-cache-config.xml <markup lang=\"xml\" > &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;file-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme/&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.FileCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value system-property=\"test.base.dir\"&gt;/tmp/&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;0s&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Specify the class that implements the CacheStore interface Pass the directory to the constructor and optionally using a system property to override Uncomment the commented line below to a directory of your choice which must already exist. Comment out the line containg the FileHelper call. <markup lang=\"java\" >baseDirectory = FileHelper.createTempDir(); // baseDirectory = new File(\"/tmp/tim\"); Also comment out the deleteDirectory below so you can look at the contents of the directory. <markup lang=\"java\" >FileHelper.deleteDir(baseDirectory); Inspect the contents of your directory: <markup lang=\"bash\" >$ ls -l /tmp/tim total 64 -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 2.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 3.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 4.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 5.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 6.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 7.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 8.txt -rw-r--r-- 1 timmiddleton wheel 8 18 Feb 14:37 9.txt You will see there are 8 files for the 8 entries that were written to the cache store. entry 1.txt was removed so does not exist in the cache store. Create a file 1.txt in the directory and put the text One . Re-run the test. You will notice that the test fails as when the test issues the following assertion as the value was not in the cache, but it was in the cache store and loaded into memory: <markup lang=\"java\" >assertNull(namedMap.get(1)); <markup lang=\"bash\" >org.opentest4j.AssertionFailedError: Expected :null Actual :One HSQLDb Cache Store Example In this next example, we will manually create a database backed cache store using a HSQLDb database in embedded mode. This will show how a cache store could interact with a back-end database. In this example we are using an embedded HSQLDb database just as an example and normally the back-end database would be on a physically separate machine and not in-memory. In this example we are storing a simple Customer class in our cache and cache-store. Continue below to review the HSQLDbCacheStore class. Review the HSQLDbCacheStore Specify the class that implements the CacheStore interface <markup lang=\"java\" >public class HSQLDbCacheStore extends Base implements CacheStore&lt;Integer, Customer&gt; { Construct the CacheStore passing the cache name to the constructor <markup lang=\"java\" >/** * Construct a cache store. * * @param cacheName cache name * * @throws SQLException if any SQL errors */ public HSQLDbCacheStore(String cacheName) throws SQLException { this.tableName = cacheName; dbConn = DriverManager.getConnection(DB_URL); Logger.info(\"HSQLDbCacheStore constructed with cache Name \" + cacheName); } Implement the load method by selecting the customer from the database based upon the primary key of id <markup lang=\"java\" >@Override public Customer load(Integer key) { String query = \"SELECT id, name, address, creditLimit FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; ResultSet resultSet = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); resultSet = statement.executeQuery(); return resultSet.next() ? createFromResultSet(resultSet) : null; } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(resultSet); close(statement); } } Implement the store method by calling storeInternal and then issuing a commit. <markup lang=\"java\" >@Override public void store(Integer key, Customer customer) { try { storeInternal(key, customer); dbConn.commit(); } catch (Exception e) { throw ensureRuntimeException(e); } } Internal implementation of store to be re-used by store and storeAll to insert or update the record in the database <markup lang=\"java\" >/** * Store a {@link Customer} object using the id. This method does not issue a * commit so that either the store or storeAll method can reuse this. * * @param key customer id * @param customer {@link Customer} object */ private void storeInternal(Integer key, Customer customer) { // the following is very inefficient; it is recommended to use DB // specific functionality that is, REPLACE for MySQL or MERGE for Oracle String query = load(key) != null ? \"UPDATE \" + tableName + \" SET name = ?, address = ?, creditLimit = ? where id = ?\" : \"INSERT INTO \" + tableName + \" (name, address, creditLimit, id) VALUES(?, ?, ?, ?)\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setString(1, customer.getName()); statement.setString(2, customer.getAddress()); statement.setInt(3, customer.getCreditLimit()); statement.setInt(4, customer.getId()); statement.execute(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Implement the storeAll method <markup lang=\"java\" >@Override public void storeAll(Map&lt;? extends Integer, ? extends Customer&gt; mapEntries) { try { for (Customer customer : mapEntries.values()) { storeInternal(customer.getId(), customer); } dbConn.commit(); Logger.info(\"Ran storeAll on \" + mapEntries.size() + \" entries\"); } catch (Exception e) { try { dbConn.rollback(); } catch (SQLException ignore) { } throw ensureRuntimeException(e); } } The storeAll method will use a single transaction to insert/update all values. This method will be used internally for write-behind only. Implement the erase method by removing the entry from the database. <markup lang=\"java\" >@Override public void erase(Integer key) { String query = \"DELETE FROM \" + tableName + \" where id = ?\"; PreparedStatement statement = null; try { statement = dbConn.prepareStatement(query); statement.setInt(1, key); statement.execute(); dbConn.commit(); } catch (SQLException sqle) { throw ensureRuntimeException(sqle); } finally { close(statement); } } Review the Cache Configuration Review the Cache Configuration hsqldb-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Customer&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt; com.oracle.coherence.guides.cachestores.HSQLDbCacheStore &lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;!-- Normally the assumption is the cache name will be the same as the table name but in this example we are hard coding the table name --&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;Customer&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;write-delay&gt;{write-delay 0s}&lt;/write-delay&gt; &lt;write-batch-factor&gt;0&lt;/write-batch-factor&gt; &lt;write-requeue-threshold&gt;0&lt;/write-requeue-threshold&gt; &lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for Customer cache to the hsqldb-cache-loader scheme Cache mapping for CustomerExpiring cache to the hsqldb-cache-loader scheme (see next section) Set the expiry to 20 seconds for the expiring cache Override the refresh-ahead factor for the expiring cache Specify the class that implements the CacheStore interface Specify the cache name Run the Unit Test Next we will run the HSqlDbCacheStoreTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"Customer\"); reloadCustomersDB(); } @Test public void testHSqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue an initial get on the NamedMap and validate the object is read from the cache store. <markup lang=\"java\" >long start = System.nanoTime(); // issue a get and it will load the existing customer Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, namedMap.size()); assertNotNull(customer); assertEquals(1, customer.getId()); assertEquals(\"Customer 1\", customer.getName()); You will see a message similar to the following indicating the time to retrieve a NamedMap entry that is not in the cache. (thread=main, member=1): Time for read-through 17.023 ms Issue a second get, the entry will be retrieved directly from memory and not the cache store. <markup lang=\"java\" >// issue a get again and it should be quicker start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"no read-through\")); You will see a message similar to the following indicating the time to retrieve a NamedMap entry is significantly quicker. (thread=main, member=1): Time for no read-through 0.889 ms Remove and entry from the NamedMap and the value should be removed from the underlying store. <markup lang=\"java\" >// remove a customer number 1 namedMap.remove(1); // we should have one less customer in the database assertEquals(MAX_CUSTOMERS - 1, getCustomerDBCount()); assertNull(namedMap.get(1)); // customer should not exist in DB assertNull(getCustomerFromDB(1)); Issue a get for another customer and then update the customer details. <markup lang=\"java\" >// Load customer 2 Customer customer2 = namedMap.get(2); assertNotNull(customer2); // update customer 2 with \"New Address\" namedMap.compute(2, (k, v)-&gt;{ v.setAddress(\"New Address\"); return v; }); // customer should have new address in cache and DB assertEquals(\"New Address\", namedMap.get(2).getAddress()); assertEquals(\"New Address\", getCustomerFromDB(2).getAddress()); Add a new customer and ensure it is created in the database. Then remove the same customer. <markup lang=\"java\" >// add a new customer 1010 namedMap.put(101, new Customer(101, \"Customer Name 101\", \"Customer address 101\", 20000)); assertTrue(namedMap.containsKey(101)); assertEquals(\"Customer address 101\", getCustomerFromDB(101).getAddress()); namedMap.remove(101); assertFalse(namedMap.containsKey(101)); assertNull(getCustomerFromDB(101)); Clear the NamedMap and show how to preload the data from the cache store. <markup lang=\"java\" >// clean the cache and reset the database namedMap.clear(); reloadCustomersDB(); assertEquals(0, namedMap.size()); // demonstrate loading the cache from the current contents of the DB // this can be done many ways but for this exercise you could fetch all the // customer id' from the DB but as we know there are 1..100 we can pretend we have. Set&lt;Integer&gt; keySet = IntStream.rangeClosed(1, 100).boxed().collect(Collectors.toSet()); namedMap.invokeAll(keySet, new PreloadRequest&lt;&gt;()); // cache should be fully primed assertEquals(MAX_CUSTOMERS, namedMap.size()); Refresh Ahead HSQLDb Cache Store Example In this next example, we use the CustomerExpiring cache which will expire data after 20 seconds and also has a refresh-ahead-factor of 0.5 meaning that if the cache is accessed after 10 seconds then an asynchronous refresh-ahead will be performed to speed up the next access to the data. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerExpiring cache passing in parameters to the caching-scheme to override expiry and refresh ahead values. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerExpiring&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;back-expiry&lt;/param-name&gt; &lt;param-value&gt;20s&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;refresh-ahead-factor&lt;/param-name&gt; &lt;param-value&gt;0.5&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; The local-scheme uses the back-expiry parameter passed in: <markup lang=\"xml\" >&lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;expiry-delay&gt;{back-expiry 0}&lt;/expiry-delay&gt; &lt;/local-scheme&gt; The read-write-backing-map-scheme uses the refresh-ahead-factor parameter passed in: <markup lang=\"xml\" >&lt;refresh-ahead-factor&gt;{refresh-ahead-factor 0.0}&lt;/refresh-ahead-factor&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreExpiringTest.java unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerExpiring\"); reloadCustomersDB(); } @Test public void testHSQLDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain the correct number of customers assertEquals(MAX_CUSTOMERS, getCustomerDBCount()); Issue a get for customer 1 and log the time to load <markup lang=\"java\" >// expiry delay is setup to 20s for the cache and refresh ahead is 0.5 which // means that after 10s if the entry is read the old value is returned but after which a // refresh is done which means that subsequents reads will be fast as the new value is already present long start = System.nanoTime(); Customer customer = namedMap.get(1); long duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"read-through\")); assertEquals(1, customer.getId()); Notice the initial read through time similar to the following in the log: (thread=main, member=1): Time for read-through 19.129 ms Update the credit limit to 10000 in the database for customer 1 and ensure that after 11 seconds the value is still 5000 in the NamedMap. <markup lang=\"java\" >// update the database updateCustomerCreditLimitInDB(1, 10000); // sleep for 11 seconds get the cache entry, we should still get the original value Base.sleep(11000L); assertEquals(5000, namedMap.get(1).getCreditLimit()); The get within the 10 seconds (20s * 0.5), will cause an asynchronous refresh-ahead. Wait for 10 seconds and then retrieve the customer object which has been updated. <markup lang=\"java\" >// wait for another 10 seconds and the refresh-ahead should have completed Base.sleep(10000L); start = System.nanoTime(); customer = namedMap.get(1); duration = System.nanoTime() - start; Logger.info(getDurationMessage(duration, \"after refresh-ahead\")); Notice the time to retrieve the entry is significantly reduced: (thread=main, member=1): Time for after refresh-ahead 1.116 ms Write Behind HSQLDb Cache Store Example In this HSQLDb cache store example, we use the CustomerWriteBehind cache which has a write delay of 10 seconds. Review the Cache Configuration The hsqldb-cache-store-cache-config.xml below shows the CustomerWriteBehind cache passing in parameters to the caching-scheme to override write-delay value. <markup lang=\"xml\" >&lt;cache-mapping&gt; &lt;cache-name&gt;CustomerWriteBehind&lt;/cache-name&gt; &lt;scheme-name&gt;hsqlb-cache-store&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;write-delay&lt;/param-name&gt; &lt;param-value&gt;10s&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; Run the Unit Test Next we will run the HSqlDbCacheStoreWriteBehindTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { _startup(\"CustomerWriteBehind\"); } @Test public void testHsqlDbCacheStore() throws SQLException { try { NamedMap&lt;Integer, Customer&gt; namedMap = getSession() .getMap(getCacheName(), TypeAssertion.withTypes(Integer.class, Customer.class)); // cache should be empty assertEquals(0, namedMap.size()); // Customer table should contain no customers assertEquals(0, getCustomerDBCount()); Insert 10 customers using an efficient putAll operation and confirm the data is not yet in the cache. <markup lang=\"java\" >// add 10 customers Map&lt;Integer, Customer&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 100; i++) { map.put(i, new Customer(i, \"Name \" + i, \"Address \" + i, i * 1000)); } namedMap.putAll(map); // initial check of the database should return 0 as we have write-delay set assertEquals(0, getCustomerDBCount()); Wait till after the write-delay has passed and confirm that the customers are in the database. <markup lang=\"java\" >// sleep for 15 seconds and the database should be populated as write-delay has elapsed Base.sleep(15000L); // Issuing Eventually assertThat in case of heavily loaded machine Eventually.assertThat(invoking(this).getCustomerDBCount(), is(100)); You will notice that you should see messages indicating 100 entries have been written. You may also see multiple writes as the data will be added in different partitions. load. &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 3 entries &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 97 entries OR &lt;Info&gt; (thread=WriteBehindThread:CacheStoreWrapper(com.oracle.coherence.guides.cachestores.HSQLDbCacheStore):DistributedCache:CustomerWriteBehind, member=1): Ran storeAll on 10 entries H2 R2DBC Non Blocking Entry Store Example In this H2 R2DBC cache store example, we use the H2Person cache which implements the NonBlockingEntryStore for non-blocking APIs and access to entries in their serialized ( BinaryEntry ) form. Review the Cache Configuration The h2r2dbc-entry-store-cache-config.xml below shows the H2Person cache specifying the class name of the NonBlockingEntryStore implementation. <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;H2Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-h2r2dbc&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.guides.cachestores.H2R2DBCEntryStore&lt;/class-name&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Run the Unit Test Next we will run the H2R2DBCEntryStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { createTable(); startupCoherence(\"h2r2dbc-entry-store-cache-config.xml\"); } /** * Performs some cache manipulations. */ @Test public void testNonBlockingEntryStore() { NamedMap&lt;Long, Person&gt; namedMap = getSession() .getMap(\"H2Person\", TypeAssertion.withTypes(Long.class, Person.class)); Person person1 = namedMap.get(Long.valueOf(101)); assertEquals(\"Robert\", person1.getFirstname()); Insert 1 person using a put operation and confirm the data is in the cache. <markup lang=\"java\" >Person person2 = new Person(Long.valueOf(102), 40, \"Tony\", \"Soprano\"); namedMap.put(Long.valueOf(102), person2); Person person3 = namedMap.get(Long.valueOf(102)); assertEquals(\"Tony\", person3.getFirstname()); Delete a couple records and verify the state of the cache. <markup lang=\"java\" >namedMap.remove(Long.valueOf(101)); namedMap.remove(Long.valueOf(102)); assertEquals(null, namedMap.get(Long.valueOf(101))); assertEquals(null, namedMap.get(Long.valueOf(102))); Insert 10 persons using a putAll operation and confirm the data is in the cache. The actual database operations take place in parallel.s <markup lang=\"java\" >Map&lt;Long, Person&gt; map = new HashMap&lt;&gt;(); for (int i = 1; i &lt;= 10; i++) { map.put(Long.valueOf(i), new Person(Long.valueOf(i), 20 + i, \"firstname\" + i, \"lastname\" + i)); } namedMap.putAll(map); Person person5 = namedMap.get(Long.valueOf(5)); assertEquals(\"firstname5\", person5.getFirstname()); assertEquals(10, namedMap.size()); You should see messages indicating activity on the store side: 2021-06-29 15:01:36.365/5.583 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.495/5.713 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore store 2021-06-29 15:01:36.501/5.720 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.504/5.722 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.507/5.726 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore erase 2021-06-29 15:01:36.508/5.727 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Rows updated: 1 2021-06-29 15:01:36.509/5.728 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore load key: 101 2021-06-29 15:01:36.512/5.730 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): Could not find row for key: 101 2021-06-29 15:01:36.515/5.734 Oracle Coherence GE 14.1.2.0.0 &lt;Info&gt; (thread=DistributedCacheWorker:0x0000:5, member=1): H2R2DBCEntryStore storeAll Pluggable Cache Stores A cache store is an application-specific adapter used to connect a cache to an underlying data source. The cache store implementation accesses the data source by using a data access mechanism (for example, Hibernate, Toplink, JPA, application-specific JDBC calls, etc). The cache store understands how to build a Java object using data retrieved from the data source, map and write an object to the data source, and erase an object from the data source. In this example we are going to use a Hibernate cache store from the Coherence Hibernate OpenSource Project . Review the Configuration Review the Cache Configuration hibernate-cache-store-cache-config.xml <markup lang=\"xml\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;Person&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-hibernate&lt;/scheme-name&gt; &lt;backing-map-scheme&gt; &lt;read-write-backing-map-scheme&gt; &lt;internal-cache-scheme&gt; &lt;local-scheme&gt;&lt;/local-scheme&gt; &lt;/internal-cache-scheme&gt; &lt;cachestore-scheme&gt; &lt;class-scheme&gt; &lt;class-name&gt;com.oracle.coherence.hibernate.cachestore.HibernateCacheStore&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;java.lang.String&lt;/param-type&gt; &lt;param-value&gt;com.oracle.coherence.guides.cachestores.{cache-name}&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/class-scheme&gt; &lt;/cachestore-scheme&gt; &lt;/read-write-backing-map-scheme&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; Cache mapping for all caches to the distributed-hibernate scheme Specify the HibernateCacheStore scheme Pass the cache name using the in-built macro to the constructor In this case we do not have to write any code for our cache store as the Hibernate cache store understands the entity mapping and will deal with this. Review the Hibernate Configuration <markup lang=\"xml\" >&lt;hibernate-configuration&gt; &lt;session-factory&gt; &lt;!-- Database connection settings --&gt; &lt;property name=\"connection.driver_class\"&gt;org.hsqldb.jdbcDriver&lt;/property&gt; &lt;property name=\"connection.url\"&gt;jdbc:hsqldb:mem:test&lt;/property&gt; &lt;property name=\"connection.username\"&gt;sa&lt;/property&gt; &lt;property name=\"connection.password\"&gt;&lt;/property&gt; &lt;!-- JDBC connection pool (use the built-in) --&gt; &lt;property name=\"connection.pool_size\"&gt;1&lt;/property&gt; &lt;!-- SQL dialect --&gt; &lt;property name=\"dialect\"&gt;org.hibernate.dialect.HSQLDialect&lt;/property&gt; &lt;!-- Enable Hibernate's automatic session context management --&gt; &lt;property name=\"current_session_context_class\"&gt;thread&lt;/property&gt; &lt;!-- Echo all executed SQL to stdout --&gt; &lt;property name=\"show_sql\"&gt;true&lt;/property&gt; &lt;!-- Drop and re-create the database schema on startup --&gt; &lt;property name=\"hbm2ddl.auto\"&gt;update&lt;/property&gt; &lt;mapping resource=\"Person.hbm.xml\"/&gt; &lt;/session-factory&gt; &lt;/hibernate-configuration&gt; - Specifies the Person mapping Review the Hibernate Mapping <markup lang=\"xml\" >&lt;hibernate-mapping package=\"com.oracle.coherence.guides.cachestores\"&gt; &lt;class name=\"Person\" table=\"PERSON\"&gt; &lt;id name=\"id\" column=\"id\"&gt; &lt;generator class=\"native\"/&gt; &lt;/id&gt; &lt;property name=\"age\"/&gt; &lt;property name=\"firstname\"/&gt; &lt;property name=\"lastname\"/&gt; &lt;/class&gt; &lt;/hibernate-mapping&gt; Specifies the Person mapping Run the Unit Test Next we will run the HibernateCacheStoreTest unit test below and observe the behaviour. Start and confirm NamedMap and database contents. In this example we are not preloading the database. <markup lang=\"java\" >@BeforeAll public static void startup() throws SQLException { startupCoherence(\"hibernate-cache-store-cache-config.xml\"); connection = DriverManager.getConnection(\"jdbc:hsqldb:mem:test\"); } Create a new Person and put it into the NamedMap. <markup lang=\"java\" >Person person1 = new Person(1L, 50, \"Tom\", \"Jones\"); namedMap.put(person1.getId(), person1); assertEquals(1, namedMap.size()); Retrieve the Person from the database and validate that the person from the database and cache are equal. <markup lang=\"java\" >Person person2 = getPersonFromDB(1L); person1 = namedMap.get(1L); assertNotNull(person2); assertEquals(person2, person1); Update the persons age in the NamedMap and confirm it is saved in the database <markup lang=\"java\" >person2.setAge(100); namedMap.put(person2.getId(), person2); Person person3 = getPersonFromDB(1L); assertNotNull(person2); assertEquals(person3.getAge(), 100); Remove person 1 and ensure they are also removed from the database. <markup lang=\"java\" >namedMap.remove(1L); Person person4 = getPersonFromDB(1L); assertNull(person4); Summary You have seen how to use and configure Cache Stores within Coherence. See Also Caching Data Stores Coherence Hibernate OpenSource Project ",
            "title": "Cache Stores"
        },
        {
            "location": "/examples/guides/000-overview",
            "text": " These simple guides are designed to be a quick hands-on introduction to a specific feature of Coherence. In most cases they require nothing more than a Coherence jar and an IDE (or a text editor if you&#8217;re really old-school). Guides are typically built as a combination Maven and Gradle project including the corresponding wrappers for those tools making them simple to build as stand-alone projects without needing to build the whole Coherence source tree. Bootstrap Coherence This guide walks you through various methods to configure and bootstrap a Coherence instance. Coherence*Extend Provides a guide for clients to connect to a Coherence Cluster via Coherence*Extend. Put Get and Remove This guide walks you through basic CRUD put , get , and remove operations on a NamedMap . Querying Caches This guide walks you through the basic concepts of querying Coherence caches. Built-in Aggregators This guide walks you through how to use built-in aggregators within Coherence. Custom Aggregators This guide walks you through how to create custom aggregators within Coherence. Views Learn about the basic concepts of working with views using the ContinuousQueryCache . Streams This guide walks you through how to use the Streams API with Coherence. Entry Processors This guide walks you through how to use Entry Processors with Coherence. Federation This guide walks you through how to use Federation within Coherence. Topics This guide walks you through how to use Topics within Coherence. Near Caching This guide walks you through how to use near caching within Coherence. Client Events This guide walks you through how to use client events within Coherence. Server-Side Events This guide walks you through how to use server-side events within Coherence. Durable Events This guide walks you through how to use durable events within Coherence. Cache Stores This guide walks you through how to use and configure Cache Stores. Bulk Loading Caches This guide shows approaches to bluk load data into caches, typically this would be loading data into caches from a DB when applications start. Securing with SSL This guide walks you through how to secure Coherence using SSL/TLS. Key Association This guide walks you through a use case for key association in Coherence. Multi-Cluster Client An example of how to connect an Extend or gRPC client to multiple Coherence clusters. ",
            "title": "Guides"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " In this example you will run a number of tests and that show the following features of client events including: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " The example code comprises the ClientEventsTest class, which runs a test showing various aspects of client events. The testMapListeners runs the following test code for various scenarios testStandardMapListener - standard MapListener implementation listening to all events testMultiplexingMapListener - MultiplexingMapListener listening to all events through the onMapEvent() method testSimpleMapListener - SimpleMapListener allows the use of lambdas to add event handlers to listen to events testListenOnQueries - listening for only for events on New York customers testEventTypes - listening for new or updated GOLD customers Review the Customer class All the tests use the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private int id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review the test boostrap and cleanup to start the cluster before all the tests and shutdown after the tests <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); coherence.start().join(); customers = coherence.getSession().getMap(\"customers\"); } <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Review the testStandardMapListener code This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. <markup lang=\"java\" >/** * Simple {@link MapListener} implementation for Customers. */ public static class CustomerMapListener implements MapListener&lt;Integer, Customer&gt; { private final AtomicInteger insertCount = new AtomicInteger(); private final AtomicInteger updateCount = new AtomicInteger(); private final AtomicInteger removeCount = new AtomicInteger(); private final AtomicInteger liteEvents = new AtomicInteger(); @Override public void entryInserted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"New customer: new key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getNewValue()); insertCount.incrementAndGet(); if (mapEvent.getNewValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryUpdated(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Updated customer key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); updateCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryDeleted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Deleted customer: old key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getOldValue()); removeCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } public int getInsertCount() { return insertCount.get(); } public int getUpdateCount() { return updateCount.get(); } public int getRemoveCount() { return removeCount.get(); } public int getLiteEvents() { return liteEvents.get(); } } Implements MapListener interface AtomicIntegers for test validation Respond to insert events with new value Respond to update events with old and new values Respond to delete events with old value <markup lang=\"java\" >Logger.info(\"*** testStandardMapListener\"); customers.clear(); CustomerMapListener mapListener = new CustomerMapListener(); customers.addMapListener(mapListener); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.invoke(1, Processors.update(Customer::setCreditLimit, 2000L)); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(1)); Eventually.assertThat(invoking(mapListener).getRemoveCount(), is(1)); customers.removeMapListener(mapListener); Create the MapListener Add the MapListener to listen for all events Add the customers Update the credit limit for customer 1 Remove customer 1 Wait for all events Review the testMultiplexingMapListener code This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"java\" >/** * Simple {@link MultiplexingMapListener} implementation for Customers. */ public static class MultiplexingCustomerMapListener extends MultiplexingMapListener&lt;Integer, Customer&gt; { private final AtomicInteger counter = new AtomicInteger(); @Override protected void onMapEvent(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"isInsert=\" + mapEvent.isInsert() + \", isDelete=\" + mapEvent.isDelete() + \", isUpdate=\" + mapEvent.isUpdate()); Logger.info(\"key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); Logger.info(mapEvent.toString()); counter.incrementAndGet(); } public int getCount() { return counter.get(); } } Extends abstract class MultiplexingMapListener AtomicInteger for test validation Respond to all events and use MapEvent methods to determine type of event <markup lang=\"java\" >Logger.info(\"*** testMultiplexingMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; multiplexingMapListener = new MultiplexingCustomerMapListener(); // Multiplexing MapListener listening on all entries customers.addMapListener(multiplexingMapListener); customer1 = new Customer(1, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.invoke(1, Processors.update(Customer::setAddress, \"Updated address\")); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking((MultiplexingCustomerMapListener) multiplexingMapListener).getCount(), is(3)); customers.removeMapListener(multiplexingMapListener); Create the MapListener Add the MapListener to listen for all events Mutate the customers Wait for all events Review the testSimpleMapListener code This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"java\" >Logger.info(\"*** testSimpleMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; simpleMapListener = new SimpleMapListener&lt;Integer, Customer&gt;() .addInsertHandler((e) -&gt; Logger.info(\"New Customer added with id=\" + e.getNewValue().getId())) .addDeleteHandler((e) -&gt; Logger.info(\"Deleted customer id =\" + e.getOldValue().getId())) .addInsertHandler((e) -&gt; insertCount.incrementAndGet()) .addDeleteHandler((e) -&gt; deleteCount.incrementAndGet()); customers.addMapListener(simpleMapListener, 1, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.clear(); // should only be 1 insert and 1 delete as we are listening on the key Eventually.assertThat(invoking(this).getInsertCount(), is(1)); Eventually.assertThat(invoking(this).getDeleteCount(), is(1)); Create the SimpleMapListener instance Add an insert handler to display new customers Add delete a handler to display deleted customers Add an insert handler to increment an atomic Add delete a handler to increment an atomic Register the listener on the key 1 (customer id 1) wait for all events Review the testListenOnQueries code This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"java\" >Logger.info(\"*** testListenOnQueries\"); customers.clear(); mapListener = new CustomerMapListener(); // MapListener listening only to new customers from NY Filter&lt;Customer&gt; filter = Filters.like(Customer::getAddress, \"%NY%\"); MapEventFilter&lt;Integer, Customer&gt; eventFilter = new MapEventFilter&lt;&gt;(filter); customer1 = new Customer(1, \"Tim\", \"123 James Street, Perth, Australia\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street, New York, NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customer4 = new Customer(4, \"James Stewart\", \"123 5th Ave, New York, NY\", Customer.SILVER, 200); // Listen only for events where address is in New York customers.addMapListener(mapListener, eventFilter, true); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); customers.put(customer4.getId(), customer4); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); // ensure we only receive lite events Eventually.assertThat(invoking(mapListener).getLiteEvents(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the MapListener instance Create a like filter to select only customers whose address contains NY Add the map listener and specify a MapEventFilter which takes the filter created above as well as specifying the event is lite event where the new and old values may not necessarily be present wait for all events Review the testEventTypes code This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"java\" >Logger.info(\"*** testEventTypes\"); customers.clear(); mapListener = new CustomerMapListener(); filter = Filters.equal(Customer::getCustomerType, Customer.GOLD); // listen only for events where customers has been inserted as GOLD or updated to GOLD status or were changed from GOLD int mask = MapEventFilter.E_INSERTED | MapEventFilter.E_UPDATED_ENTERED| MapEventFilter.E_UPDATED_LEFT; eventFilter = new MapEventFilter&lt;&gt;(mask, filter); customers.addMapListener(mapListener, eventFilter, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); // update customer 1 from BRONZE to GOLD customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); customers.invoke(2, Processors.update(Customer::setCustomerType, Customer.SILVER)); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(1)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the CustomerMapListener instance Create an equals filter to select only GOLD customers Create a mask for inserted events for when the filter is matched or events that are updated and now the filter matches Add the map listener and specify a MapEventFilter which takes the filter created above/ wait for all events ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.clientevents.ClientEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code testStandardMapListener Output This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. Output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testStandardMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer: old key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} Insert event from new customer id 1 Insert event from new customer id 2 Update event from updating of customer 1&#8217;s credit limit Delete event containing old version of deleted customer 1 testMultiplexingMapListener Output This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=true, isDelete=false, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=null, new=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=false, isUpdate=true &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=true, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000}, new=null Insert event from new customer id 1 Update event from an update of customer 1 address Delete event from customer 1 testSimpleMapListener Output This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testSimpleMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New Customer added with id=1 &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer id =1 testListenOnQueries Output This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/null &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=4/null Both above queries only return the key because they are lite events and only customer 2 and 4 are returned as they are the only ones with NY in the address. testEventTypes Output This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testEventTypes &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='GOLD', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=2, old=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='SILVER', balance=10000} Insert event from new GOLD customer id 2 Update event changing customer type from BRONZE to GOLD for customer id 1 Update event changing customer type from GOLD to BRONZE for customer id 2 ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " In this example you have seen how to use the following features of client events: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/140-client-events/README",
            "text": " This guide walks you through how to use client events within Coherence to listen for insert, update or delete events on a Coherence NamedMap . An application object that implements the MapListener interface can sign up for events from any Coherence NamedMap simply by passing an instance of the application&#8217;s MapListener implementation to a addMapListener() method. The MapListener can be registered against all entries, a specific key, or a Filter. Registrations with filters can use MapEventFilter which provide more fine-grained control for event registrations or InKeySetFilter which can be used to register against a Set of keys. The MapListener interface provides a call back mechanism for NamedMap events where any changes that happen to the source (NamedMap) are delivered to relevant clients asynchronously. The MapEvent object that is passed to the MapListener carries all the necessary information about the event that has occurred Including the event type (insert, update, or delete), the key, old value, new value, and the source ( NameMap ) that emitted the event. Client events are the key building blocks for other Coherence functionality including Near Cache and Continuous Query Caches (CQC). See the Coherence Documentation links below for more information: Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches Table of Contents What You Will Build What You Need Building the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of client events including: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Tests The example code comprises the ClientEventsTest class, which runs a test showing various aspects of client events. The testMapListeners runs the following test code for various scenarios testStandardMapListener - standard MapListener implementation listening to all events testMultiplexingMapListener - MultiplexingMapListener listening to all events through the onMapEvent() method testSimpleMapListener - SimpleMapListener allows the use of lambdas to add event handlers to listen to events testListenOnQueries - listening for only for events on New York customers testEventTypes - listening for new or updated GOLD customers Review the Customer class All the tests use the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private int id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review the test boostrap and cleanup to start the cluster before all the tests and shutdown after the tests <markup lang=\"java\" >@BeforeAll static void boostrapCoherence() { Coherence coherence = Coherence.clusterMember(); coherence.start().join(); customers = coherence.getSession().getMap(\"customers\"); } <markup lang=\"java\" >@AfterAll static void shutdownCoherence() { Coherence coherence = Coherence.getInstance(); coherence.close(); } Review the testStandardMapListener code This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. <markup lang=\"java\" >/** * Simple {@link MapListener} implementation for Customers. */ public static class CustomerMapListener implements MapListener&lt;Integer, Customer&gt; { private final AtomicInteger insertCount = new AtomicInteger(); private final AtomicInteger updateCount = new AtomicInteger(); private final AtomicInteger removeCount = new AtomicInteger(); private final AtomicInteger liteEvents = new AtomicInteger(); @Override public void entryInserted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"New customer: new key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getNewValue()); insertCount.incrementAndGet(); if (mapEvent.getNewValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryUpdated(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Updated customer key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); updateCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } @Override public void entryDeleted(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"Deleted customer: old key/value=\" + mapEvent.getKey() + \"/\" + mapEvent.getOldValue()); removeCount.incrementAndGet(); if (mapEvent.getOldValue() == null) { liteEvents.incrementAndGet(); } } public int getInsertCount() { return insertCount.get(); } public int getUpdateCount() { return updateCount.get(); } public int getRemoveCount() { return removeCount.get(); } public int getLiteEvents() { return liteEvents.get(); } } Implements MapListener interface AtomicIntegers for test validation Respond to insert events with new value Respond to update events with old and new values Respond to delete events with old value <markup lang=\"java\" >Logger.info(\"*** testStandardMapListener\"); customers.clear(); CustomerMapListener mapListener = new CustomerMapListener(); customers.addMapListener(mapListener); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.invoke(1, Processors.update(Customer::setCreditLimit, 2000L)); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(1)); Eventually.assertThat(invoking(mapListener).getRemoveCount(), is(1)); customers.removeMapListener(mapListener); Create the MapListener Add the MapListener to listen for all events Add the customers Update the credit limit for customer 1 Remove customer 1 Wait for all events Review the testMultiplexingMapListener code This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"java\" >/** * Simple {@link MultiplexingMapListener} implementation for Customers. */ public static class MultiplexingCustomerMapListener extends MultiplexingMapListener&lt;Integer, Customer&gt; { private final AtomicInteger counter = new AtomicInteger(); @Override protected void onMapEvent(MapEvent&lt;Integer, Customer&gt; mapEvent) { Logger.info(\"isInsert=\" + mapEvent.isInsert() + \", isDelete=\" + mapEvent.isDelete() + \", isUpdate=\" + mapEvent.isUpdate()); Logger.info(\"key=\" + mapEvent.getKey() + \", old=\" + mapEvent.getOldValue() + \", new=\" + mapEvent.getNewValue()); Logger.info(mapEvent.toString()); counter.incrementAndGet(); } public int getCount() { return counter.get(); } } Extends abstract class MultiplexingMapListener AtomicInteger for test validation Respond to all events and use MapEvent methods to determine type of event <markup lang=\"java\" >Logger.info(\"*** testMultiplexingMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; multiplexingMapListener = new MultiplexingCustomerMapListener(); // Multiplexing MapListener listening on all entries customers.addMapListener(multiplexingMapListener); customer1 = new Customer(1, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.invoke(1, Processors.update(Customer::setAddress, \"Updated address\")); customers.remove(1); // ensure that we see all events Eventually.assertThat(invoking((MultiplexingCustomerMapListener) multiplexingMapListener).getCount(), is(3)); customers.removeMapListener(multiplexingMapListener); Create the MapListener Add the MapListener to listen for all events Mutate the customers Wait for all events Review the testSimpleMapListener code This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"java\" >Logger.info(\"*** testSimpleMapListener\"); customers.clear(); MapListener&lt;Integer, Customer&gt; simpleMapListener = new SimpleMapListener&lt;Integer, Customer&gt;() .addInsertHandler((e) -&gt; Logger.info(\"New Customer added with id=\" + e.getNewValue().getId())) .addDeleteHandler((e) -&gt; Logger.info(\"Deleted customer id =\" + e.getOldValue().getId())) .addInsertHandler((e) -&gt; insertCount.incrementAndGet()) .addDeleteHandler((e) -&gt; deleteCount.incrementAndGet()); customers.addMapListener(simpleMapListener, 1, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.clear(); // should only be 1 insert and 1 delete as we are listening on the key Eventually.assertThat(invoking(this).getInsertCount(), is(1)); Eventually.assertThat(invoking(this).getDeleteCount(), is(1)); Create the SimpleMapListener instance Add an insert handler to display new customers Add delete a handler to display deleted customers Add an insert handler to increment an atomic Add delete a handler to increment an atomic Register the listener on the key 1 (customer id 1) wait for all events Review the testListenOnQueries code This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"java\" >Logger.info(\"*** testListenOnQueries\"); customers.clear(); mapListener = new CustomerMapListener(); // MapListener listening only to new customers from NY Filter&lt;Customer&gt; filter = Filters.like(Customer::getAddress, \"%NY%\"); MapEventFilter&lt;Integer, Customer&gt; eventFilter = new MapEventFilter&lt;&gt;(filter); customer1 = new Customer(1, \"Tim\", \"123 James Street, Perth, Australia\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street, New York, NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customer4 = new Customer(4, \"James Stewart\", \"123 5th Ave, New York, NY\", Customer.SILVER, 200); // Listen only for events where address is in New York customers.addMapListener(mapListener, eventFilter, true); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); customers.put(customer4.getId(), customer4); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(2)); // ensure we only receive lite events Eventually.assertThat(invoking(mapListener).getLiteEvents(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the MapListener instance Create a like filter to select only customers whose address contains NY Add the map listener and specify a MapEventFilter which takes the filter created above as well as specifying the event is lite event where the new and old values may not necessarily be present wait for all events Review the testEventTypes code This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"java\" >Logger.info(\"*** testEventTypes\"); customers.clear(); mapListener = new CustomerMapListener(); filter = Filters.equal(Customer::getCustomerType, Customer.GOLD); // listen only for events where customers has been inserted as GOLD or updated to GOLD status or were changed from GOLD int mask = MapEventFilter.E_INSERTED | MapEventFilter.E_UPDATED_ENTERED| MapEventFilter.E_UPDATED_LEFT; eventFilter = new MapEventFilter&lt;&gt;(mask, filter); customers.addMapListener(mapListener, eventFilter, false); customer1 = new Customer(1, \"Tim\", \"123 James Street Perth\", Customer.BRONZE, 1000); customer2 = new Customer(2, \"James Brown\", \"1 Main Street New York NY\", Customer.GOLD, 10000); customer3 = new Customer(3, \"Tony Stark\", \"Malibu Point 10880, 90265 Malibu, CA\", Customer.SILVER, 333333); customers.put(customer1.getId(), customer1); customers.put(customer2.getId(), customer2); customers.put(customer3.getId(), customer3); // update customer 1 from BRONZE to GOLD customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); customers.invoke(2, Processors.update(Customer::setCustomerType, Customer.SILVER)); // ensure that we see all events Eventually.assertThat(invoking(mapListener).getInsertCount(), is(1)); Eventually.assertThat(invoking(mapListener).getUpdateCount(), is(2)); customers.removeMapListener(mapListener, eventFilter); Create the CustomerMapListener instance Create an equals filter to select only GOLD customers Create a mask for inserted events for when the filter is matched or events that are updated and now the filter matches Add the map listener and specify a MapEventFilter which takes the filter created above/ wait for all events Run the Examples Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.clientevents.ClientEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code testStandardMapListener Output This test uses the CustomerMapListener class which is an implementation of a standard MapListener listening for all events. Output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testStandardMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer: old key/value=1/Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=2000} Insert event from new customer id 1 Insert event from new customer id 2 Update event from updating of customer 1&#8217;s credit limit Delete event containing old version of deleted customer 1 testMultiplexingMapListener Output This test uses the MultiplexingCustomerMapListener class which extends MultiplexingMapListener to listen for all events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=true, isDelete=false, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=null, new=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=false, isUpdate=true &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): isInsert=false, isDelete=true, isUpdate=false &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): key=1, old=Customer{id=1, name='James Brown', address='Updated address', customerType='GOLD', balance=10000}, new=null Insert event from new customer id 1 Update event from an update of customer 1 address Delete event from customer 1 testSimpleMapListener Output This test uses the SimpleMapListener and lambdas to register event handlers for only the key 1 . <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testSimpleMapListener &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New Customer added with id=1 &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Deleted customer id =1 testListenOnQueries Output This test uses the CustomerMapListener to listen on a query for customers in NY and returns lite events. <markup lang=\"bash\" >&lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/null &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=4/null Both above queries only return the key because they are lite events and only customer 2 and 4 are returned as they are the only ones with NY in the address. testEventTypes Output This test uses the CustomerMapListener but also applies a filter to only receive insert or update events for GOLD customers. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): *** testEventTypes &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): New customer: new key/value=2/Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=1, old=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='BRONZE', balance=1000}, new=Customer{id=1, name='Tim', address='123 James Street Perth', customerType='GOLD', balance=1000} &lt;Info&gt; (thread=DistributedCache:PartitionedCache:EventDispatcher, member=1): Updated customer key=2, old=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='GOLD', balance=10000}, new=Customer{id=2, name='James Brown', address='1 Main Street New York NY', customerType='SILVER', balance=10000} Insert event from new GOLD customer id 2 Update event changing customer type from BRONZE to GOLD for customer id 1 Update event changing customer type from GOLD to BRONZE for customer id 2 Summary In this example you have seen how to use the following features of client events: Understanding the MapListener interface Listening for all events Using SimpleMapListener and MultiplexingMapListener Using lite events Listening for events for a particular key Listening for events based upon filters See Also Develop Applications using Map Events Understanding Near Caches Using Continuous Query Caches ",
            "title": "Client Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": "",
            "title": "Coherence Java CDI gRPC Client"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. ",
            "title": "Configure gRPC Channels"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence gRPC client will attempt to connect to a default server endpoint on localhost:1409 if no other channel has been configured. This is fine for development and testing but in most real-world applications the client will need to know the endpoint to connect to. Most applications would only require a channel to connect to a single Coherence cluster but there are use-cases where clients connect to multiple clusters, and the Coherence gRPC Java client supports these use-cases too. The Coherence gRPC client has been built on top of the Helidon Microprofile gRPC library and uses it to provide gRPC channels. Configure gRPC Channels Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. ",
            "title": "Remote Connections"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. ",
            "title": "Session Types"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " Coherence uses the concept of a Session to manage a set of related Coherence resources, such as maps, caches, topics, etc. When using the Coherence Java gRPC client a Session connects to a specific gRPC channel (described above) and uses a specific serialization format to marshal requests and responses. This means that different sessions using different serializers may connect to the same server endpoint. Typically, for efficiency the client and server would be configured to use matching serialization formats to avoid deserialization of data on the server but this does not have to be the case. If the server is using a different serializer for the server side caches it must be able to deserialize the client&#8217;s requests, so there must be a serializer configured on the server to match that used by the client. As with gRPC channels above, Coherence Sessions can be configured using Helidon configuration. Coherence sessions are configured in the coherence.sessions section of the configuration. Each session has its own entry in the configuration hierarchy, as shown below: <markup lang=\"yaml\" >coherence: sessions: default: serializer: pof channel: default The example above shows configuration for the default Coherence session, this is the session that will be used to provide Coherence beans when no session name has been specified for an injection point. In this example, the default session will use POF serialization and connect to the server using the default gRPC channel. The default session, if not configured, will use the default channel and will use Java serialization. As with channels, multiple sessions can be configured: <markup lang=\"yaml\" >coherence: sessions: test: serializer: pof channel: test prod: serializer: pof channel: prod # Helidon gRPC configuration grpc: channels: - name: test host: test.storage.acme.com port: 1408 - name: prod host: storage.acme.com port: 1408 In the example above, there are two Coherence sessions configured and two corresponding gRPC channels. Session Types By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. ",
            "title": "Coherence gRPC Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " A number of commonly used Coherence objects can be injected when using Java gRPC client. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In order to inject an instance of a NamedMap into your gRPC client CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >@Inject private NamedMap&lt;Long, Person&gt; people; <markup lang=\"java\" >@Inject @SesionName(\"products\") private NamedMap&lt;Long, Product&gt; products; In this example the Coherence CDI extensions will use the products session to provide the client side NamedMap backed on the server by a NamedMap called products . Other remote resources, such a NamedCache can be injected the same way: <markup lang=\"java\" >@Inject private NamedCache&lt;Long, Product&gt; products; The Coherence CDI documentation covers the different types of resources supported by CDI. When using them with the gRPC Java client. ",
            "title": "Injecting NamedMap NamedCache and Related Objects"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " If an application bean requires multiple maps or caches where the names will only be known at runtime then a Coherence com.tangosol.net.Session can be injected instead of other specific named resources. The required maps or caches can then be obtained from the Session by calling methods such as Session.getMap or Session.getCache , etc. <markup lang=\"java\" >@Inject @Name(\"products\") private Session session; The @Name qualifier has the value products , so the Session injected here will be the pre-configured Session named products . ",
            "title": "Injecting Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. ",
            "title": "Observe Events for Maps and Caches from Specific Sessions"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. ",
            "title": "Observe Events for Maps and Caches from Specific Server-side Scopes"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence NamedMap and NamedCache APIs allow implementations of MapListener to be added that will then receive events as map/cache entries get inserted, updated or deleted. When using CDI it is possible to subscribe to the same events using CDI observer methods. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } The Observes qualifier is what makes this method a standard CDI observer. The MapName qualifier determines which map/cache to observer. If this qualifier is not present events from all caches will be observed. Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. Observe Events for Maps and Caches from Specific Sessions In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. Observe Events for Maps and Caches from Specific Server-side Scopes In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Using Asynchronous Observers All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Using CDI Observers to Handle MapEvents"
        },
        {
            "location": "/coherence-helidon-client/README",
            "text": " The Coherence gRPC Helidon client is a CDI enabled library that allows Java clients to connect via gRPC to a Coherence proxy server. This library has a dependency on Helidon for some services. In order to use Coherence gRPC Helidon client, you need to declare it as a dependency in your pom.xml <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-helidon-client&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Using the Coherence gRPC Helidon client assumes that there is a corresponding server process running which is using the Coherence gRPC proxy service to expose the required gRPC endpoints. Once the necessary dependency is in place, the simplest way to start using it is to just inject Coherence resources into the application&#8217;s beans. A lot of the annotations and qualifiers are identical to those described in the Coherence CDI documentation. The following sections describe different injection points in more detail. Configuring gRPC Connections Configuring Coherence Remote Sessions Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache , and related objects Session Injection Using CDI Observers to Handle Coherence Map or Cache Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches owned by a specific Session Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Remote Connections The Coherence gRPC client will attempt to connect to a default server endpoint on localhost:1409 if no other channel has been configured. This is fine for development and testing but in most real-world applications the client will need to know the endpoint to connect to. Most applications would only require a channel to connect to a single Coherence cluster but there are use-cases where clients connect to multiple clusters, and the Coherence gRPC Java client supports these use-cases too. The Coherence gRPC client has been built on top of the Helidon Microprofile gRPC library and uses it to provide gRPC channels. Configure gRPC Channels Remote gRPC connections are configured using Helidon configuration, typically this would be a configuration file, but Helidon supports many ways to provide the configuration, or override the configuration with System properties or environment variables. The examples here will just use a configuration file. All gRPC channels are configured in the grpc.channels section of the application configuration. The example below is a simple configuration for a gRPC channel: <markup lang=\"yaml\" >grpc: channels: default: host: storage.acme.com port: 1408 The name of the channel is default . The host name of the gRPC server is storage.acme.com The port which the server is listening on is 1408 The default channel name is a special case that the Coherence gRPC client will use to locate a channel configuration if no channel name has been specified in CDI injection points. The example below shows a configuration with multiple channels, one named test and one named prod . <markup lang=\"yaml\" >grpc: channels: test: host: test.storage.acme.com port: 1408 prod: host: storage.acme.com port: 1408 The configuration may contain as many channels as required, the only stipulation being that each has a unique name. Coherence gRPC Sessions Coherence uses the concept of a Session to manage a set of related Coherence resources, such as maps, caches, topics, etc. When using the Coherence Java gRPC client a Session connects to a specific gRPC channel (described above) and uses a specific serialization format to marshal requests and responses. This means that different sessions using different serializers may connect to the same server endpoint. Typically, for efficiency the client and server would be configured to use matching serialization formats to avoid deserialization of data on the server but this does not have to be the case. If the server is using a different serializer for the server side caches it must be able to deserialize the client&#8217;s requests, so there must be a serializer configured on the server to match that used by the client. As with gRPC channels above, Coherence Sessions can be configured using Helidon configuration. Coherence sessions are configured in the coherence.sessions section of the configuration. Each session has its own entry in the configuration hierarchy, as shown below: <markup lang=\"yaml\" >coherence: sessions: default: serializer: pof channel: default The example above shows configuration for the default Coherence session, this is the session that will be used to provide Coherence beans when no session name has been specified for an injection point. In this example, the default session will use POF serialization and connect to the server using the default gRPC channel. The default session, if not configured, will use the default channel and will use Java serialization. As with channels, multiple sessions can be configured: <markup lang=\"yaml\" >coherence: sessions: test: serializer: pof channel: test prod: serializer: pof channel: prod # Helidon gRPC configuration grpc: channels: - name: test host: test.storage.acme.com port: 1408 - name: prod host: storage.acme.com port: 1408 In the example above, there are two Coherence sessions configured and two corresponding gRPC channels. Session Types By default, all sessions configured in Helidon configuration are gRPC client sessions. The type can be specifically set using the type property for the session configuration. There are two valid values for the grpc and coherence . A grpc session type specified that the session is a gRPC client session. A coherence type specifies that the session wraps a ConfigurableCacheFactory loaded from a Coherence configuration file. For example: <markup lang=\"yaml\" >coherence: sessions: products: type: grpc serializer: pof channel: prod extend: type: coherence configUri: coherence-config.xml The products session has a type: grpc property so it will be a gRPC client session. The `extend session will wrap a ConfigurableCacheFactory using the coherence-config.xml config file. Injecting Coherence Objects into CDI Beans A number of commonly used Coherence objects can be injected when using Java gRPC client. Injecting NamedMap NamedCache and Related Objects In order to inject an instance of a NamedMap into your gRPC client CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >@Inject private NamedMap&lt;Long, Person&gt; people; <markup lang=\"java\" >@Inject @SesionName(\"products\") private NamedMap&lt;Long, Product&gt; products; In this example the Coherence CDI extensions will use the products session to provide the client side NamedMap backed on the server by a NamedMap called products . Other remote resources, such a NamedCache can be injected the same way: <markup lang=\"java\" >@Inject private NamedCache&lt;Long, Product&gt; products; The Coherence CDI documentation covers the different types of resources supported by CDI. When using them with the gRPC Java client. Injecting Sessions If an application bean requires multiple maps or caches where the names will only be known at runtime then a Coherence com.tangosol.net.Session can be injected instead of other specific named resources. The required maps or caches can then be obtained from the Session by calling methods such as Session.getMap or Session.getCache , etc. <markup lang=\"java\" >@Inject @Name(\"products\") private Session session; The @Name qualifier has the value products , so the Session injected here will be the pre-configured Session named products . Using CDI Observers to Handle MapEvents The Coherence NamedMap and NamedCache APIs allow implementations of MapListener to be added that will then receive events as map/cache entries get inserted, updated or deleted. When using CDI it is possible to subscribe to the same events using CDI observer methods. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } The Observes qualifier is what makes this method a standard CDI observer. The MapName qualifier determines which map/cache to observer. If this qualifier is not present events from all caches will be observed. Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event type qualifiers. Qualifier Description @Inserted Observes insert events, raised when new entries are added to a map or cache. @Updated Observes update events, raised when entries in a map or cache are modified. @Deleted Observes deleted events, raised when entries are deleted from a map or cache. For example: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onAddOrRemove(@Observes @Inserted @Deleted @MapName(\"people\") MapEvent&lt;?, ?&gt; event) { // handle INSERTED and DELETED events raised by the 'people' map/cache } The first observer method above will observe only update events. Multiple event type qualifiers can be added, so the second observer method will observer insert or delete events. Note The client supports connecting to a server using different named Sessions and different named Scopes . The observer methods above are not qualified with either session name or scope name so will observe events for all maps or caches with the name people in all sessions and scopes. In most Coherence use-cases that only use a single client session and a single default server side scope this is not an issue but is something to be aware of if using multiple sessiosn or scopes. See the following sections on how to qualify the observer to restrict the maps and caches it observes. Observe Events for Maps and Caches from Specific Sessions In addition, to the @MapName qualifier, you can also specify a Session name as a way to limit the events received to maps or caches from a specific Session . This is achieved by specifying a value for the @SessionName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"test\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the test Session. } In the example above the @SesionName qualifier has a value test , so the events will only be observed from the people map on the server that corresponds to the map of the same name owned by the client side Session named test . Note Maps or caches in different client side Sessions may correspond to the same server side map or cache and hence events in one server side map or cache can be observed by multiple client side observers. For example: Suppose a Map named people has been created in the default scope on the server. On the client there are two Sessions configured, session-one and session-two but both of these connect to the same server and have the same default scope. The two observers below are on the client: <markup lang=\"java\" >private void onMapChange(@Observes @SesionName(\"session-one\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } private void onMapChange(@Observes @SesionName(\"session-two\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { //... } In this case both observer methods are actually observing the same server-side map and will receive the same events event though they have different qualifiers. Observe Events for Maps and Caches from Specific Server-side Scopes In addition, to the @MapName qualifier, you can also specify a scope name as a way to limit the events received to maps or caches from a specific server-side scope name. This is achieved by specifying a value for the @ScopeName qualifier. See the Sessions section for more details on multiple `Session`s. For example: <markup lang=\"java\" >private void onMapChange(@ObservesAsync @ScopeName(\"employees\") @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache owned by the employees scope. } In the example above the @ScopeName qualifier has a value employees , so the events will only be observed from the people map in by the scope named employees on the server. Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values, and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type MapEvent&lt;Long, Person&gt; this method will receive events of type MapEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") MapEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type MapEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Using Asynchronous Observers All the examples above used synchronous observers by specifying the @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onMapChange(@ObservesAsync @MapName(\"people\") MapEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Usage"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Key association in Coherence is a way of associating related data together in a single partition. This data could be entries in a single cache, or it could be entries in multiple caches managed by the same cache service. If related data is known to exist in a single partition, then this allows those related entries to be accessed as part of a single atomic partition level transaction. For example a single entry processor call could atomically update multiple related entries, possibly across multiple caches. Queries could also make use of this, for example a custom aggregator could aggregate results from multiple entries possibly from multiple caches, in a single partition. This can be a way to simulate certain types of join query for related data. Key association can be used to implement similar behaviour to a multi-map, where a single key maps to a list or set of related data. Using key association and related caches instead of a single multi-map offers a lot more flexibility for supporting various use-cases. ",
            "title": "Key Association"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " This example is going to demonstrate a simple use case of handling notifications sent to customers. A customer can have zero or more notifications. A customer may span regions, so notifications are region specific. A notification also has an expiry time, so it will be automatically evicted when the expiry time is reached. Using key association, notifications for a customer will be co-located in the same partition. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " In this example there will be two Java mode classes, Customer and Notification . The Customer has a String id field and String first name and last name fields. <markup lang=\"java\" title=\"Customer.java\" >@PortableType(id=1001, version=1) public class Customer { /** * The customer's identifier. */ private String id; /** * The customer's first name. */ private String firstName; /** * The customer's last name. */ private String lastName; /** * Create a customer. * * @param id the customer's identifier * @param firstName the customer's first name * @param lastName the customer's last name */ public Customer(String id, String firstName, String lastName) { this.id = id; this.firstName = firstName; this.lastName = lastName; } /** * Returns the customer's identifier. * * @return the customer's identifier */ public String getId() { return id; } /** * Returns the customer's first name. * * @return the customer's first name */ public String getFirstName() { return firstName; } /** * Set the customer's first name. * * @param firstName the customer's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the customer's last name. * * @return the customer's last name */ public String getLastName() { return lastName; } /** * Set the customer's last name. * * @param lastName the customer's last name */ public void setLastName(String lastName) { this.lastName = lastName; } } The Notification has a String body and a Java time LocalDateTime ttl field, to represent its expiry time. <markup lang=\"java\" title=\"Notification.java\" >@PortableType(id=1010, version=1) public class Notification { /** * The notification text. */ private String body; /** * The time the notification expires. */ private LocalDateTime ttl; /** * Create a {@link Notification}. * * @param body the notification text * @param ttl the time the notification expires */ public Notification(String body, LocalDateTime ttl) { this.body = body; this.ttl = ttl; } /** * Returns the notification text. * * @return the notification text */ public String getBody() { return body; } /** * Returns the time the notification expires. * * @return the time the notification expires */ public LocalDateTime getTTL() { return ttl; } } Both of the model classes are annotated with the @PortableType . This annotation is used by the Coherence POF Maven plugin to generate Portable Object code for the classes. Using the Coherence POF generator in this way avoids having to manually write serialization code and ensures that the serialization code generated is supports evolvability between versions. ",
            "title": "Model Classes"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The customers cache will be used to store customer data. The cache key will be the String customer id . The notifications cache will be used to store notification data. A NotificationId class will be used for the key of the cache. The NotificationId will hold the notification&#8217;s corresponding customer id, region and a unique UUID identifier for the notifications. The caches in this example do not require and special functionality, so the default cache configuration file will support everything required. ",
            "title": "Caches"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The data model used in this example is very simple and is made up of two entities, a Customer and a Notification . A customer can have zero or more notifications. A notification is specific to a region and has an expiry time. For example, in json the customer notification data may look like this: <markup lang=\"json\" title=\"customers.json\" >[ { \"id\": \"User01\", \"notificationsByRegion\": [ { \"region\": \"US\", \"notifications\": [ { \"body\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\", \"ttl\": \"24:00:00\" }, { \"body\": \"Eu turpis egestas pretium aenean.\", \"ttl\": \"12:00:00\" } ] }, { \"region\": \"EU\", \"notifications\": [ { \"body\": \"Tincidunt id aliquet risus feugiat.\", \"ttl\": \"06:00:00\" }, { \"body\": \"Quis risus sed vulputate odio ut enim blandit volutpat.\", \"ttl\": \"48:00:00\" }, { \"body\": \"Sem et tortor consequat id porta nibh.\", \"ttl\": \"01:00:00\" } ] } ] }, { \"id\": \"User02\", \"notificationsByRegion\": [ { \"region\": \"US\", \"notifications\": [ { \"body\": \"Et malesuada fames ac turpis egestas sed tempus urna.\", \"ttl\": \"01:23:45\" } ] } ] } ] This structure could be contained in Java, and hence in Coherence, in a Map&lt;String, Map&lt;String, List&lt;Notification&gt;&gt;&gt; or some other multi-map type of data structure. The disadvantages of this are that a customers' notifications are then treated as a single blob of data which could make certain operations less efficient. Any mutation or addition of notifications would require everything to be deserialized. There is also a requirement in this example to automatically expire notifications from the cache based on their TTL is reached. If all the notifications for a customer are in a single map structure, this would require some complex server side logic whereas holding each notification as a separate cache entry can leverage Coherence&#8217;s built in expiry functionality. The json data above is really just notification data and this example could use just a single cache, but using two entities and two caches, for Customer and Notification, will make the example a bit more interesting. Model Classes In this example there will be two Java mode classes, Customer and Notification . The Customer has a String id field and String first name and last name fields. <markup lang=\"java\" title=\"Customer.java\" >@PortableType(id=1001, version=1) public class Customer { /** * The customer's identifier. */ private String id; /** * The customer's first name. */ private String firstName; /** * The customer's last name. */ private String lastName; /** * Create a customer. * * @param id the customer's identifier * @param firstName the customer's first name * @param lastName the customer's last name */ public Customer(String id, String firstName, String lastName) { this.id = id; this.firstName = firstName; this.lastName = lastName; } /** * Returns the customer's identifier. * * @return the customer's identifier */ public String getId() { return id; } /** * Returns the customer's first name. * * @return the customer's first name */ public String getFirstName() { return firstName; } /** * Set the customer's first name. * * @param firstName the customer's first name */ public void setFirstName(String firstName) { this.firstName = firstName; } /** * Returns the customer's last name. * * @return the customer's last name */ public String getLastName() { return lastName; } /** * Set the customer's last name. * * @param lastName the customer's last name */ public void setLastName(String lastName) { this.lastName = lastName; } } The Notification has a String body and a Java time LocalDateTime ttl field, to represent its expiry time. <markup lang=\"java\" title=\"Notification.java\" >@PortableType(id=1010, version=1) public class Notification { /** * The notification text. */ private String body; /** * The time the notification expires. */ private LocalDateTime ttl; /** * Create a {@link Notification}. * * @param body the notification text * @param ttl the time the notification expires */ public Notification(String body, LocalDateTime ttl) { this.body = body; this.ttl = ttl; } /** * Returns the notification text. * * @return the notification text */ public String getBody() { return body; } /** * Returns the time the notification expires. * * @return the time the notification expires */ public LocalDateTime getTTL() { return ttl; } } Both of the model classes are annotated with the @PortableType . This annotation is used by the Coherence POF Maven plugin to generate Portable Object code for the classes. Using the Coherence POF generator in this way avoids having to manually write serialization code and ensures that the serialization code generated is supports evolvability between versions. Caches The customers cache will be used to store customer data. The cache key will be the String customer id . The notifications cache will be used to store notification data. A NotificationId class will be used for the key of the cache. The NotificationId will hold the notification&#8217;s corresponding customer id, region and a unique UUID identifier for the notifications. The caches in this example do not require and special functionality, so the default cache configuration file will support everything required. ",
            "title": "The Example Data Model"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " In this use case key association will be used to co-locate a Customer and all the Notification entries for that customer in the same Coherence partition. This will allow notifications to be added and queried for as specific customer as an atomic operation. To use key association, the key classes fo the caches to be associated must either be the same or implement the Coherence com.tangosol.net.cache.KeyAssociation interface. If notifications for a customer were going to be held in a map or list in a single cache entry, we could just use the same String customer identifier as the key and the customer and the notification map would automatically be assigned to the same partition, as they would have the same key value. In this case though, there will be many notification entries for a single customer so the notifications cache requires a custom key class that implements KeyAssociation . The NotificationId class is shown below: <markup lang=\"java\" title=\"NotificationId.java\" >@PortableType(id=1011, version=1) public class NotificationId implements KeyAssociation&lt;String&gt;, Comparable&lt;NotificationId&gt; { /** * The customer the notification is for. */ private String customerId; /** * The region the notification applies to. */ private String region; /** * The notification unique identifier. */ private UUID id; /** * Create a notification identifier. * * @param customerId the customer the notification is for * @param region the region the notification applies to * @param id the notification identifier */ public NotificationId(String customerId, String region, UUID id) { this.customerId = customerId; this.region = region; this.id = id; } /** * Returns the identifier of the customer the notification is for. * * @return the identifier of the customer the notification is for */ public String getCustomerId() { return customerId; } /** * Returns the region the notification applies to. * * @return the region the notification applies to */ public String getRegion() { return region; } /** * Returns the notification identifier. * * @return the notification identifier */ public UUID getId() { return id; } @Override public String getAssociatedKey() { return customerId; } @Override public int compareTo(NotificationId o) { int n = SafeComparator.compareSafe(Comparator.naturalOrder(), customerId, o.customerId); if (n == 0) { n = Long.compare(id.getTimestamp(), o.id.getTimestamp()); if (n == 0) { n = Long.compare(id.getCount(), o.id.getCount()); } } return n; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } NotificationId that = (NotificationId) o; return Objects.equals(customerId, that.customerId) &amp;&amp; Objects.equals(region, that.region) &amp;&amp; Objects.equals(id, that.id); } @Override public int hashCode() { return Objects.hash(customerId, region, id); } } Like the Customer and Notification classes, the NotificationId class is annotated with @PortableType to automatically generate the PortableObject serialization code. All classes that will be used as cache keys in Coherence must properly implement the equals and hashCode methods and include all fields in those methods. The important method for this example is the getAssociatedKey() method from the KeyAssociation interface. This method should return the value that this key is to be associated with. In this case notifications are associated to customers, so the customer identifier is returned. This will then guarantee that a customer and its notifications are all located in the same partition in Coherence. <markup lang=\"java\" title=\"NotificationId.java\" > @Override public String getAssociatedKey() { return customerId; } ",
            "title": "Coherence Key Association"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " This example is going to use the \"repository\" functionality in Coherence. A repository is a simple class that provides CRUD operations for an entity. In this case the repository will be for the Customer entity, because that is the root entry point for all operations, including those on notifications. Making all updates and queries access caches via the customer in this way, ensures that updates to notifications are treated as a single atomic operation. The example does not require the use of a repository class, but it is a nice way to group all the customer related operations together in a single class. The minimum amount of code to implement a repository is shown below. The CustomerRepository class extends the com.oracle.coherence.repository.AbstractRepository base class and implements the required abstract methods. <markup lang=\"java\" title=\"CustomerRepository.java\" >public class CustomerRepository extends AbstractRepository&lt;String, Customer&gt; { /** * The customer's cache. */ private final NamedMap&lt;String, Customer&gt; customers; public CustomerRepository(NamedCache&lt;String, Customer&gt; customers) { this.customers = customers; } @Override protected String getId(Customer entity) { return entity.getId(); } @Override protected Class&lt;? extends Customer&gt; getEntityType() { return Customer.class; } @Override protected NamedMap&lt;String, Customer&gt; getMap() { return customers; } } In the rest of the example the CustomerRepository will be enhanced to add additional functionality for notifications. ",
            "title": "The Customer Repository"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Although the CustomerRepository.addNotifications method could be called and would execute, the AddNotifications.process method is empty, so no notifications will actually be added. The next step is to implement the process method to add the notifications to the notifications cache. At this point it is worth going over what the process method must do for each entry in the notification map. Check the ttl of the entry, if it has already passed then ignore the notification as there i sno point adding it to be immediately expired Create a NotificationId for the key of the new notification cache entry. Use the key to obtain the cache entry to insert Set the notification as the value for the cache entry Set the expiry value for the new entry based on the ttl value of the notification. Iterate Over the Notifications The process method can simply iterate over the map of notifications like this: <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { // process notification... }); }); } Work out the Expiry Delay A Coherence cache entry expects the expiry for an entry to be the number of milliseconds after the entry is inserted or updated before it expires. The ttl value in the Notification class is a Java LocalDateTime so the expiry is the difference between now and the ttl in milliseconds. In Java that can be written as shown below: <markup lang=\"java\" >long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); If the ttlInMillis is greater than zero the notification can be added. If it is less than or equal to zero, then there is no point adding the notification as the ttl is already in the past. <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); if (ttlInMillis &gt; 0) { // add the notification... } }); }); } Create a NotificationId Creating the NotificationId is simple. The customer identifier can be taken from the key of the entry passed to the process method String customerId = entry.getKey(); , the region comes from the notifications map and the UUID is just a new UUID created at runtime. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); Obtain the Notification Cache Entry When using Coherence partition level transactions to atomically update other cache entries in an entry processor, those additional entries must be properly obtained from the relevant cache&#8217;s BackingMapContext . Coherence will then ensure that all mutations are properly handled, backup messages sent, events fired, etc. Each additional entry enlisted in this sort of lite partition transactions, will be locked until the entry processor completes processing. This can cause issues if two entry processors run that try to enlist the same set of entries but in different orders. Each processor may be holding locks on a sub-set of the entries, and then each is unable to obtain locks on the remaining entries it requires. The safest way around this is to sort the keys that will be enlisted so both processors always enlist entries in the same order. In this example, notifications are only ever inserted, so there is no chance of two processors enlisting the same entries. The entry processor is executing on an entry from the customers cache, so to obtain the BackingMapContext for the notifications cache can be obtained via the customer entry. <markup lang=\"java\" >BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); To obtain the entry to insert from the BackingMapContext the BackingMapContext.getBackingMapEntry() method is used. This method takes the key of the entry to obtain, but this key must be in serialized Binary format, not a plain NotificationId . The BackingMapManagerContext conveniently has a converter that can do the serialization. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); The notification is then set as the entry value using the setValue() method and the expiry set using the expire() method. <markup lang=\"java\" >binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); This can all be put together in the final process method: <markup lang=\"java\" title=\"AddNotifications.java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { BackingMapManagerContext context = entry.asBinaryEntry().getContext(); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); BackingMapContext ctxNotifications = context.getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); String customerId = entry.getKey(); LocalDateTime now = LocalDateTime.now(); notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(now, notification.getTTL()); if (ttlInMillis &gt; 0) { NotificationId id = new NotificationId(customerId, region, new UUID()); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); } }); }); return null; } ",
            "title": "Implement the Process Method"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " To perform the add operation, a custom Coherence entry processor can be written. This entry processor will take the map of notifications and apply it to the customer. As key association is being used, the entry processor will be executed against the customer identifier in the customer cache and apply all the notifications in a single atomic partition level transaction. For the duration of the operation on the server the customer will effectively be locked, guaranteeing that only a single concurrent mutation operation can happen to a customer. The boilerplate code for the AddNotifications entry processor is shown below. As with other classes, the entry processor is annotated with @PortableType to generate PortableObject code. The result returned from this entry processor&#8217;s process method is Void as there is no information that the caller requires as a result. <markup lang=\"java\" title=\"AddNotifications.java\" >@PortableType(id=1100, version=1) public class AddNotifications implements InvocableMap.EntryProcessor&lt;String, Customer, Void&gt; { /** * The notifications to add to the customer. */ private Map&lt;String, List&lt;Notification&gt;&gt; notifications; /** * Create a {@link AddNotifications} processor. * * @param notifications the notifications to add to the customer */ public AddNotifications(Map&lt;String, List&lt;Notification&gt;&gt; notifications) { this.notifications = notifications; } @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { return null; } A new addNotifications method can be added to the repository, which will invoke the AddNotifications entry processor against a specific customer identifier. The addNotifications first ensures the repository is initialized and then invokes the entry processor. using the map of notifications. The method will throw a NullPointerException if the customer identifier is null . <markup lang=\"java\" title=\"CustomerRepository.java\" > public void addNotifications(String customerId, Map&lt;String, List&lt;Notification&gt;&gt; notifications) { ensureInitialized(); customers.invoke(Objects.requireNonNull(customerId), new AddNotifications(Objects.requireNonNull(notifications))); } Implement the Process Method Although the CustomerRepository.addNotifications method could be called and would execute, the AddNotifications.process method is empty, so no notifications will actually be added. The next step is to implement the process method to add the notifications to the notifications cache. At this point it is worth going over what the process method must do for each entry in the notification map. Check the ttl of the entry, if it has already passed then ignore the notification as there i sno point adding it to be immediately expired Create a NotificationId for the key of the new notification cache entry. Use the key to obtain the cache entry to insert Set the notification as the value for the cache entry Set the expiry value for the new entry based on the ttl value of the notification. Iterate Over the Notifications The process method can simply iterate over the map of notifications like this: <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { // process notification... }); }); } Work out the Expiry Delay A Coherence cache entry expects the expiry for an entry to be the number of milliseconds after the entry is inserted or updated before it expires. The ttl value in the Notification class is a Java LocalDateTime so the expiry is the difference between now and the ttl in milliseconds. In Java that can be written as shown below: <markup lang=\"java\" >long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); If the ttlInMillis is greater than zero the notification can be added. If it is less than or equal to zero, then there is no point adding the notification as the ttl is already in the past. <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); if (ttlInMillis &gt; 0) { // add the notification... } }); }); } Create a NotificationId Creating the NotificationId is simple. The customer identifier can be taken from the key of the entry passed to the process method String customerId = entry.getKey(); , the region comes from the notifications map and the UUID is just a new UUID created at runtime. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); Obtain the Notification Cache Entry When using Coherence partition level transactions to atomically update other cache entries in an entry processor, those additional entries must be properly obtained from the relevant cache&#8217;s BackingMapContext . Coherence will then ensure that all mutations are properly handled, backup messages sent, events fired, etc. Each additional entry enlisted in this sort of lite partition transactions, will be locked until the entry processor completes processing. This can cause issues if two entry processors run that try to enlist the same set of entries but in different orders. Each processor may be holding locks on a sub-set of the entries, and then each is unable to obtain locks on the remaining entries it requires. The safest way around this is to sort the keys that will be enlisted so both processors always enlist entries in the same order. In this example, notifications are only ever inserted, so there is no chance of two processors enlisting the same entries. The entry processor is executing on an entry from the customers cache, so to obtain the BackingMapContext for the notifications cache can be obtained via the customer entry. <markup lang=\"java\" >BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); To obtain the entry to insert from the BackingMapContext the BackingMapContext.getBackingMapEntry() method is used. This method takes the key of the entry to obtain, but this key must be in serialized Binary format, not a plain NotificationId . The BackingMapManagerContext conveniently has a converter that can do the serialization. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); The notification is then set as the entry value using the setValue() method and the expiry set using the expire() method. <markup lang=\"java\" >binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); This can all be put together in the final process method: <markup lang=\"java\" title=\"AddNotifications.java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { BackingMapManagerContext context = entry.asBinaryEntry().getContext(); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); BackingMapContext ctxNotifications = context.getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); String customerId = entry.getKey(); LocalDateTime now = LocalDateTime.now(); notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(now, notification.getTTL()); if (ttlInMillis &gt; 0) { NotificationId id = new NotificationId(customerId, region, new UUID()); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); } }); }); return null; } ",
            "title": "The AddNotifications Entry Processor"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The CustomerRepository can then be used to add customers and notifications, this can be seen in the functional tests that are part of this example. <markup lang=\"java\" > CustomerRepository repository = new CustomerRepository(); Customer customer = new Customer(\"QA22\", \"Julian\", \"Alaphilippe\"); repository.save(customer); Notification notification = new Notification(\"Ride TdF\", LocalDateTime.now().plusDays(1)); repository.addNotifications(customer, \"FRA\", notification); ",
            "title": "Adding Notifications via the CustomerRepository"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The obvious starting point would be to enhance the repository to be able to add notifications for a customer. Read operations will come later, as they&#8217;d be a bit pointless without first having add operations. The use case here is to allow multiple notifications to be added to a customer is a single atomic operations. Notifications are specific to a region, so the obvious structure to hold the notifications to be added would be a map of the form Map&lt;String, List&lt;Notification&gt;&gt; where the key is the region and the value is a list of notifications for that region. The AddNotifications Entry Processor To perform the add operation, a custom Coherence entry processor can be written. This entry processor will take the map of notifications and apply it to the customer. As key association is being used, the entry processor will be executed against the customer identifier in the customer cache and apply all the notifications in a single atomic partition level transaction. For the duration of the operation on the server the customer will effectively be locked, guaranteeing that only a single concurrent mutation operation can happen to a customer. The boilerplate code for the AddNotifications entry processor is shown below. As with other classes, the entry processor is annotated with @PortableType to generate PortableObject code. The result returned from this entry processor&#8217;s process method is Void as there is no information that the caller requires as a result. <markup lang=\"java\" title=\"AddNotifications.java\" >@PortableType(id=1100, version=1) public class AddNotifications implements InvocableMap.EntryProcessor&lt;String, Customer, Void&gt; { /** * The notifications to add to the customer. */ private Map&lt;String, List&lt;Notification&gt;&gt; notifications; /** * Create a {@link AddNotifications} processor. * * @param notifications the notifications to add to the customer */ public AddNotifications(Map&lt;String, List&lt;Notification&gt;&gt; notifications) { this.notifications = notifications; } @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { return null; } A new addNotifications method can be added to the repository, which will invoke the AddNotifications entry processor against a specific customer identifier. The addNotifications first ensures the repository is initialized and then invokes the entry processor. using the map of notifications. The method will throw a NullPointerException if the customer identifier is null . <markup lang=\"java\" title=\"CustomerRepository.java\" > public void addNotifications(String customerId, Map&lt;String, List&lt;Notification&gt;&gt; notifications) { ensureInitialized(); customers.invoke(Objects.requireNonNull(customerId), new AddNotifications(Objects.requireNonNull(notifications))); } Implement the Process Method Although the CustomerRepository.addNotifications method could be called and would execute, the AddNotifications.process method is empty, so no notifications will actually be added. The next step is to implement the process method to add the notifications to the notifications cache. At this point it is worth going over what the process method must do for each entry in the notification map. Check the ttl of the entry, if it has already passed then ignore the notification as there i sno point adding it to be immediately expired Create a NotificationId for the key of the new notification cache entry. Use the key to obtain the cache entry to insert Set the notification as the value for the cache entry Set the expiry value for the new entry based on the ttl value of the notification. Iterate Over the Notifications The process method can simply iterate over the map of notifications like this: <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { // process notification... }); }); } Work out the Expiry Delay A Coherence cache entry expects the expiry for an entry to be the number of milliseconds after the entry is inserted or updated before it expires. The ttl value in the Notification class is a Java LocalDateTime so the expiry is the difference between now and the ttl in milliseconds. In Java that can be written as shown below: <markup lang=\"java\" >long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); If the ttlInMillis is greater than zero the notification can be added. If it is less than or equal to zero, then there is no point adding the notification as the ttl is already in the past. <markup lang=\"java\" >public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(LocalDateTime.now(), notification.getTTL()); if (ttlInMillis &gt; 0) { // add the notification... } }); }); } Create a NotificationId Creating the NotificationId is simple. The customer identifier can be taken from the key of the entry passed to the process method String customerId = entry.getKey(); , the region comes from the notifications map and the UUID is just a new UUID created at runtime. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); Obtain the Notification Cache Entry When using Coherence partition level transactions to atomically update other cache entries in an entry processor, those additional entries must be properly obtained from the relevant cache&#8217;s BackingMapContext . Coherence will then ensure that all mutations are properly handled, backup messages sent, events fired, etc. Each additional entry enlisted in this sort of lite partition transactions, will be locked until the entry processor completes processing. This can cause issues if two entry processors run that try to enlist the same set of entries but in different orders. Each processor may be holding locks on a sub-set of the entries, and then each is unable to obtain locks on the remaining entries it requires. The safest way around this is to sort the keys that will be enlisted so both processors always enlist entries in the same order. In this example, notifications are only ever inserted, so there is no chance of two processors enlisting the same entries. The entry processor is executing on an entry from the customers cache, so to obtain the BackingMapContext for the notifications cache can be obtained via the customer entry. <markup lang=\"java\" >BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); To obtain the entry to insert from the BackingMapContext the BackingMapContext.getBackingMapEntry() method is used. This method takes the key of the entry to obtain, but this key must be in serialized Binary format, not a plain NotificationId . The BackingMapManagerContext conveniently has a converter that can do the serialization. <markup lang=\"java\" >String customerId = entry.getKey(); NotificationId id = new NotificationId(customerId, region, new UUID()); BackingMapManagerContext context = entry.asBinaryEntry().getContext(); BackingMapContext ctxNotifications = context.getBackingMapContext(\"notifications\"); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); The notification is then set as the entry value using the setValue() method and the expiry set using the expire() method. <markup lang=\"java\" >binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); This can all be put together in the final process method: <markup lang=\"java\" title=\"AddNotifications.java\" > @Override @SuppressWarnings(\"unchecked\") public Void process(InvocableMap.Entry&lt;String, Customer&gt; entry) { BackingMapManagerContext context = entry.asBinaryEntry().getContext(); Converter&lt;NotificationId, Binary&gt; converter = context.getKeyToInternalConverter(); BackingMapContext ctxNotifications = context.getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); String customerId = entry.getKey(); LocalDateTime now = LocalDateTime.now(); notifications.forEach((region, notificationsForRegion) -&gt; { notificationsForRegion.forEach(notification -&gt; { long ttlInMillis = ChronoUnit.MILLIS.between(now, notification.getTTL()); if (ttlInMillis &gt; 0) { NotificationId id = new NotificationId(customerId, region, new UUID()); Binary binaryKey = converter.convert(id); BinaryEntry&lt;NotificationId, Notification&gt; binaryEntry = (BinaryEntry&lt;NotificationId, Notification&gt;) ctxNotifications.getBackingMapEntry(binaryKey); binaryEntry.setValue(notification); binaryEntry.expire(ttlInMillis); } }); }); return null; } Adding Notifications via the CustomerRepository The CustomerRepository can then be used to add customers and notifications, this can be seen in the functional tests that are part of this example. <markup lang=\"java\" > CustomerRepository repository = new CustomerRepository(); Customer customer = new Customer(\"QA22\", \"Julian\", \"Alaphilippe\"); repository.save(customer); Notification notification = new Notification(\"Ride TdF\", LocalDateTime.now().plusDays(1)); repository.addNotifications(customer, \"FRA\", notification); ",
            "title": "Adding Notifications"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " When the extractor&#8217;s extractFromEntry method executes, in this case the entry passed in by the aggregator will be an instance of BinaryEntry , so just like in the entry processor above, the BackingMapContext for the notifications cache can be obtained and from there access to the notification entries. Coherence does not currently have an API on a BackingMapContext that allows the data to be queried. For example, in this case some sort of filter query over all the entries in the partition with a specific customer id would get the notification required. This can be worked around by using cache indexes. The indexes on a cache are accessible via the BackingMapContext and from the index contents the required cache entries can be obtained. Take the first requirement, all notifications for a customer. By creating an index of customer id on the notifications cache, the keys of the entries for a given customer can be obtained from the index and the corresponding notifications returned from the extractor. Customer Id Index Indexes are created on a cache using a ValueExtractor to extract the values to be indexed. In the case of the customer id for a notification, this is a field in the NotificationId , which is the key to the notifications cache. An extractor to extract customer id can be created as shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); This extractor can be used as an index by calling the addIndex method on NamedCache or NamedMap . <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); notifications.addIndex(extractor); The Region Index The second index required is to be able to find notifications for a customer and region. In theory this index is not required, the index to find all notifications for a customer could be used, then those notifications filtered to only return those for the required region. If there will only be a small number of notifications per customer, that may be a suitable approach. This is one of the typical pros and cons that needs to be weighed up when using indexes. Does the cost in memory usage of the index and time to maintain the index on every mutation outweigh the benefits in speed gained by queries. This example is going to add an index on region, because it is an example there are no concerns over performance, and it will show how to perform an indexed query. The extractor to extract region from the NotificationId cache entry key is shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); This can be used to create an index: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); notifications.addIndex(extractor); Creating the Indexes The repository class already has a method that is called to create any required indexes when it is initialized. This method can be overridden and used to ensure the notifications indexes are added. <markup lang=\"java\" title=\"CustomerRepository.java\" > @Override @SuppressWarnings({\"unchecked\", \"resource\"}) protected void createIndices() { super.createIndices(); CacheService service = customers.getService(); NamedCache&lt;NotificationId, Notification&gt; notifications = service.ensureCache(NOTIFICATIONS_MAP_NAME, service.getContextClassLoader()); notifications.addIndex(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); notifications.addIndex(ValueExtractor.of(NotificationId::getRegion).fromKey()); } Note, that the super class createIndicies() method must be called to ensure any other indicies required by the customer repository are created. ",
            "title": "Find the Customer&#8217;s Notifications"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Now that the required indexes will be present the NotificationExtractor.extractFromEntry() method can be written. The techniques used below rely on the indexes being present and would not work if there were no indexes. Without indexes other less efficient methods would be required. The steps the extract method must perform are shown below: Obtain the map of indexes for the notifications cache From the index map, obtain the customer id index From the customer id index obtain the set of notification keys matching the customer id If the region is specified, reduce the set of keys to only those matching the required region For each remaining key, obtain the read-only cache entry containing the notification and add it to the results list return the list of notifications found Each step is covered in detail below: Obtain the map of indexes for the notifications cache The entry passed to the NotificationExtractor.extractFromEntry method when used in an aggregator will be an instance of a BinaryEntry so the entry can safely be cast to BinaryEntry . From a BinaryEntry it is possible to obtain the BackingMapManagerContext and from there the BackingMapContext of other caches. Remember, in this example the aggregator is executed on an entry in the customers cache, so the extractor needs to obtain the BackingMapContext of the notifications cache. From the notifications cache BackingMapContext the map of indexes can be obtained. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); From the index map, obtain the customer id index The index map is a map of MapIndex instances keyed by the ValueExtractor used to create the index. To obtain the customer id index just call the get() method using the same customer id extractor used to create the index above. This is one of the main reasons that all ValueExtractor implementations must properly implement equals() and hashCode() so that they can be used in indexes. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); From the customer id index obtain the set of notification keys matching the customer id A Coherence MapIndex typically holds two internal indexes. The keys in the index are in serialized binary format, that is, they can be used directly to obtain corresponding entries. A map of cache key to the extracted index value for that key A map of extracted index value to the set of keys that match that value In the case of the customer id index that means the index holds a map of binary key to corresponding customer id and a map of customer id to keys of entries for that customer id. The second map is the one required for this use case, which can be obtained from the MapIndex.getIndexContents() method. The set of keys for the customer can then be obtained with a simple get(customerId) on the index contents map (the customer id is just the key of the entry passed to the extractFromEntry method. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); At this point the keys set is the key of all the notification entries for the customer. Further Filter by Region If the region has been specified, the set of keys needs to be further filtered to just those for the required region. This could be achieved a number of ways, but this example is going to show how Coherence filters and indexes can be used to reduce a set of keys. Almost all filters in Coherence implement IndexAwareFilter which means they have an applyIndex method: <markup lang=\"java\" >public &lt;RK&gt; Filter&lt;V&gt; applyIndex( Map&lt;? extends ValueExtractor&lt;? extends V, Object&gt;, ? extends MapIndex&lt;? extends RK, ? extends V, Object&gt;&gt; mapIndexes, Set&lt;? extends RK&gt; setKeys); When the applyIndex method is called, the Set of keys passed in will be reduced to only those keys matching the filter. This means that an EqualsFilter using the region extractor can be used to reduce the set of all keys for the customer down to just those keys matching the region too. Again, the extractor used in the EqualsFilter must be the same extractor used to create the region index. <markup lang=\"java\" >if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Now the keys set has been reduced to only key matching both customer id and region. Obtain the Notifications The set of keys can be used to obtain notification from the notifications cache. The safest way to do this is to use the BackingMapContext.getReadOnlyEntry() method. The final list of notifications will be ordered by creation data. This is possible because the NotificationId class used in this example implements Comparable and makes use of the fact that the Coherence UUID used as a unique id in the notification contains a timestamp. The example used Java streams to process the keys into a list of notifications, the code is shown below: <markup lang=\"java\" > Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2) -&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); The key is mapped to a read-only InvocableMap.Entry Only process entries that are present for the key (in case it has just been removed) Sort the entries using the comparator to sort by key (i.e. NotificationId ) Map the entry to just the value (i.e. the Notification ) Cast the value to a Notification (this is because Java does not know the InvocableMap.Entry generic types) Collect the final Notification instances into a list The Final Method All the code above can be combined into the final extractFromEntry() method. <markup lang=\"java\" > @Override @SuppressWarnings({\"rawtypes\", \"unchecked\"}) public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext() .getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); if (keys == null || keys.isEmpty()) { return Collections.emptyList(); } if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2) -&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); } Note Looking at the source code, or JavaDoc, for BackingMapContext will show the getBackingMap() method, which returns the actual map of Binary keys and values in the cache; it should also be obvious that this method is deprecated. It may seem like this is a good way to access the data in the cache for the use case above, but directly accessing the data this way can break the guarantees and locks provided by Coherence. Ideally this method would have been removed, but backwards compatibility constraints mean it is still there, but it should not be used. ",
            "title": "Write the NotificationExtractor extractFromEntry method"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The purpose of the custom ValueExtractor will be to obtain the notifications for a customer. The notifications are all co-located in a single partition, so when the extractor is run against an entry in the customer cache, all the notifications are also stored locally. This particular ValueExtract is going to need access to the entry the aggregator is executing on, so it needs to extend the Coherence com.tangosol.util.extractor.AbstractExtractor class. The AbstractExtractor is treated as a special case by Coherence when it is extracting data from a cache entry, where Coherence will call its extractFromEntry method. The boilerplate code for a custom extractor is shown below. All ValueExtractor implementations should have a correct equals() and hashCode() methods. The extractFromEntry method returns null , and will be completed in the next section. <markup lang=\"java\" title=\"NotificationExtractor.java\" >@PortableType(id=1200, version=1) public class NotificationExtractor extends AbstractExtractor&lt;Customer, List&lt;Notification&gt;&gt; { /** * An optional region identifier to use to retrieve * only notifications for a specific region. */ private String region; /** * Create a {@link NotificationExtractor} that will specifically * target the key when used to extract from a cache entry. * * @param region an optional region identifier */ public NotificationExtractor(String region) { this.region = region; } @Override public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { return null; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } if (!super.equals(o)) { return false; } NotificationExtractor that = (NotificationExtractor) o; return Objects.equals(region, that.region); } @Override public int hashCode() { return Objects.hash(super.hashCode(), region); } Find the Customer&#8217;s Notifications When the extractor&#8217;s extractFromEntry method executes, in this case the entry passed in by the aggregator will be an instance of BinaryEntry , so just like in the entry processor above, the BackingMapContext for the notifications cache can be obtained and from there access to the notification entries. Coherence does not currently have an API on a BackingMapContext that allows the data to be queried. For example, in this case some sort of filter query over all the entries in the partition with a specific customer id would get the notification required. This can be worked around by using cache indexes. The indexes on a cache are accessible via the BackingMapContext and from the index contents the required cache entries can be obtained. Take the first requirement, all notifications for a customer. By creating an index of customer id on the notifications cache, the keys of the entries for a given customer can be obtained from the index and the corresponding notifications returned from the extractor. Customer Id Index Indexes are created on a cache using a ValueExtractor to extract the values to be indexed. In the case of the customer id for a notification, this is a field in the NotificationId , which is the key to the notifications cache. An extractor to extract customer id can be created as shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); This extractor can be used as an index by calling the addIndex method on NamedCache or NamedMap . <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); notifications.addIndex(extractor); The Region Index The second index required is to be able to find notifications for a customer and region. In theory this index is not required, the index to find all notifications for a customer could be used, then those notifications filtered to only return those for the required region. If there will only be a small number of notifications per customer, that may be a suitable approach. This is one of the typical pros and cons that needs to be weighed up when using indexes. Does the cost in memory usage of the index and time to maintain the index on every mutation outweigh the benefits in speed gained by queries. This example is going to add an index on region, because it is an example there are no concerns over performance, and it will show how to perform an indexed query. The extractor to extract region from the NotificationId cache entry key is shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); This can be used to create an index: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); notifications.addIndex(extractor); Creating the Indexes The repository class already has a method that is called to create any required indexes when it is initialized. This method can be overridden and used to ensure the notifications indexes are added. <markup lang=\"java\" title=\"CustomerRepository.java\" > @Override @SuppressWarnings({\"unchecked\", \"resource\"}) protected void createIndices() { super.createIndices(); CacheService service = customers.getService(); NamedCache&lt;NotificationId, Notification&gt; notifications = service.ensureCache(NOTIFICATIONS_MAP_NAME, service.getContextClassLoader()); notifications.addIndex(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); notifications.addIndex(ValueExtractor.of(NotificationId::getRegion).fromKey()); } Note, that the super class createIndicies() method must be called to ensure any other indicies required by the customer repository are created. Write the NotificationExtractor extractFromEntry method Now that the required indexes will be present the NotificationExtractor.extractFromEntry() method can be written. The techniques used below rely on the indexes being present and would not work if there were no indexes. Without indexes other less efficient methods would be required. The steps the extract method must perform are shown below: Obtain the map of indexes for the notifications cache From the index map, obtain the customer id index From the customer id index obtain the set of notification keys matching the customer id If the region is specified, reduce the set of keys to only those matching the required region For each remaining key, obtain the read-only cache entry containing the notification and add it to the results list return the list of notifications found Each step is covered in detail below: Obtain the map of indexes for the notifications cache The entry passed to the NotificationExtractor.extractFromEntry method when used in an aggregator will be an instance of a BinaryEntry so the entry can safely be cast to BinaryEntry . From a BinaryEntry it is possible to obtain the BackingMapManagerContext and from there the BackingMapContext of other caches. Remember, in this example the aggregator is executed on an entry in the customers cache, so the extractor needs to obtain the BackingMapContext of the notifications cache. From the notifications cache BackingMapContext the map of indexes can be obtained. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); From the index map, obtain the customer id index The index map is a map of MapIndex instances keyed by the ValueExtractor used to create the index. To obtain the customer id index just call the get() method using the same customer id extractor used to create the index above. This is one of the main reasons that all ValueExtractor implementations must properly implement equals() and hashCode() so that they can be used in indexes. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); From the customer id index obtain the set of notification keys matching the customer id A Coherence MapIndex typically holds two internal indexes. The keys in the index are in serialized binary format, that is, they can be used directly to obtain corresponding entries. A map of cache key to the extracted index value for that key A map of extracted index value to the set of keys that match that value In the case of the customer id index that means the index holds a map of binary key to corresponding customer id and a map of customer id to keys of entries for that customer id. The second map is the one required for this use case, which can be obtained from the MapIndex.getIndexContents() method. The set of keys for the customer can then be obtained with a simple get(customerId) on the index contents map (the customer id is just the key of the entry passed to the extractFromEntry method. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); At this point the keys set is the key of all the notification entries for the customer. Further Filter by Region If the region has been specified, the set of keys needs to be further filtered to just those for the required region. This could be achieved a number of ways, but this example is going to show how Coherence filters and indexes can be used to reduce a set of keys. Almost all filters in Coherence implement IndexAwareFilter which means they have an applyIndex method: <markup lang=\"java\" >public &lt;RK&gt; Filter&lt;V&gt; applyIndex( Map&lt;? extends ValueExtractor&lt;? extends V, Object&gt;, ? extends MapIndex&lt;? extends RK, ? extends V, Object&gt;&gt; mapIndexes, Set&lt;? extends RK&gt; setKeys); When the applyIndex method is called, the Set of keys passed in will be reduced to only those keys matching the filter. This means that an EqualsFilter using the region extractor can be used to reduce the set of all keys for the customer down to just those keys matching the region too. Again, the extractor used in the EqualsFilter must be the same extractor used to create the region index. <markup lang=\"java\" >if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Now the keys set has been reduced to only key matching both customer id and region. Obtain the Notifications The set of keys can be used to obtain notification from the notifications cache. The safest way to do this is to use the BackingMapContext.getReadOnlyEntry() method. The final list of notifications will be ordered by creation data. This is possible because the NotificationId class used in this example implements Comparable and makes use of the fact that the Coherence UUID used as a unique id in the notification contains a timestamp. The example used Java streams to process the keys into a list of notifications, the code is shown below: <markup lang=\"java\" > Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2) -&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); The key is mapped to a read-only InvocableMap.Entry Only process entries that are present for the key (in case it has just been removed) Sort the entries using the comparator to sort by key (i.e. NotificationId ) Map the entry to just the value (i.e. the Notification ) Cast the value to a Notification (this is because Java does not know the InvocableMap.Entry generic types) Collect the final Notification instances into a list The Final Method All the code above can be combined into the final extractFromEntry() method. <markup lang=\"java\" > @Override @SuppressWarnings({\"rawtypes\", \"unchecked\"}) public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext() .getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); if (keys == null || keys.isEmpty()) { return Collections.emptyList(); } if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2) -&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); } Note Looking at the source code, or JavaDoc, for BackingMapContext will show the getBackingMap() method, which returns the actual map of Binary keys and values in the cache; it should also be obvious that this method is deprecated. It may seem like this is a good way to access the data in the cache for the use case above, but directly accessing the data this way can break the guarantees and locks provided by Coherence. Ideally this method would have been removed, but backwards compatibility constraints mean it is still there, but it should not be used. ",
            "title": "The NotificationExtractor"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Now the NotificationExtractor is complete, methods can now be added to the CustomerRepository to get notifications for a customer and optionally a region. <markup lang=\"java\" title=\"CustomerRepository.java\" > /** * Returns the notifications for a customer. * * @param customerId the identifier of the customer to obtain the notifications for * * @return the notifications for the customer */ public List&lt;Notification&gt; getNotifications(String customerId) { return getNotifications(customerId, null); } /** * Returns the notifications for a customer, and optionally a region. * * @param customerId the identifier of the customer to obtain the notifications for * @param region an optional region to get notifications for * * @return the notifications for the customer, optionally restricted to a region */ public List&lt;Notification&gt; getNotifications(String customerId, String region) { Map&lt;String, List&lt;Notification&gt;&gt; map = getAll(List.of(customerId), new NotificationExtractor(region)); return map.getOrDefault(customerId, Collections.emptyList()); } The getNotifications() method calls the getAll() method on the AbstractRepository super class, which takes a collection of keys and a ValueExtractor . Under the covers, the AbstractRepository.getAll() method just runs a ReducerAggregator with the provided ValueExtractor after ensuring the repository is properly initialized. The map of results returned by getAll() will only ever contain a single entry, as it is only ever called here with a singleton list of keys. The result map will be a map of customer id to a list of notifications. ",
            "title": "Add Get Notification Methods to the CustomerRepository"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " Now that notifications can be added for a customer, the read functions can be added to get notifications for a customer. There are two use cases to implement, first get all notifications for a customer, second get notification for a customer and specific region. As notifications are in their own cache, the notifications for a customer and customer/region could be obtained by simply running a filter query on the notifications cache. This example is all about uses of key association though, so the method used here will be slightly more complex, but it will show how key association can be used for reading entries as well as updating entries. Reading notifications could be implemented using an entry processor, which is invoked against the customer cache, that then returns the required notifications, either all for the customer or for a specific region. An entry processor is typically used for mutations and will cause an entry (or entries) to be locked for the duration of its execution. For read operations an aggregator is more efficient as it will not involve locking entries. To recap the use case, the aggregator needs to return either all the notifications for a customer, or just the notifications for a region. At this point a custom aggregator could be written, but sometimes writing aggregators can be complex and Coherence already has an aggregator that does most of what is required. The ReducerAggregator Coherence contains a built-in aggregator named com.tangosol.util.aggregator.ReducerAggregator . This aggregator takes a ValueExtractor and executes it against each entry and returns the results. The results returned will be a map of with the keys of the entries the aggregator ran over and the extracted values. By using the ReducerAggregator aggregator in this use case all that is required is a custom ValueExtractor . In this example the aggregator will only be run against a single entry (the customer) and the custom ValueExtractor will \"extract\" the required notifications. The NotificationExtractor The purpose of the custom ValueExtractor will be to obtain the notifications for a customer. The notifications are all co-located in a single partition, so when the extractor is run against an entry in the customer cache, all the notifications are also stored locally. This particular ValueExtract is going to need access to the entry the aggregator is executing on, so it needs to extend the Coherence com.tangosol.util.extractor.AbstractExtractor class. The AbstractExtractor is treated as a special case by Coherence when it is extracting data from a cache entry, where Coherence will call its extractFromEntry method. The boilerplate code for a custom extractor is shown below. All ValueExtractor implementations should have a correct equals() and hashCode() methods. The extractFromEntry method returns null , and will be completed in the next section. <markup lang=\"java\" title=\"NotificationExtractor.java\" >@PortableType(id=1200, version=1) public class NotificationExtractor extends AbstractExtractor&lt;Customer, List&lt;Notification&gt;&gt; { /** * An optional region identifier to use to retrieve * only notifications for a specific region. */ private String region; /** * Create a {@link NotificationExtractor} that will specifically * target the key when used to extract from a cache entry. * * @param region an optional region identifier */ public NotificationExtractor(String region) { this.region = region; } @Override public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { return null; } @Override public boolean equals(Object o) { if (this == o) { return true; } if (o == null || getClass() != o.getClass()) { return false; } if (!super.equals(o)) { return false; } NotificationExtractor that = (NotificationExtractor) o; return Objects.equals(region, that.region); } @Override public int hashCode() { return Objects.hash(super.hashCode(), region); } Find the Customer&#8217;s Notifications When the extractor&#8217;s extractFromEntry method executes, in this case the entry passed in by the aggregator will be an instance of BinaryEntry , so just like in the entry processor above, the BackingMapContext for the notifications cache can be obtained and from there access to the notification entries. Coherence does not currently have an API on a BackingMapContext that allows the data to be queried. For example, in this case some sort of filter query over all the entries in the partition with a specific customer id would get the notification required. This can be worked around by using cache indexes. The indexes on a cache are accessible via the BackingMapContext and from the index contents the required cache entries can be obtained. Take the first requirement, all notifications for a customer. By creating an index of customer id on the notifications cache, the keys of the entries for a given customer can be obtained from the index and the corresponding notifications returned from the extractor. Customer Id Index Indexes are created on a cache using a ValueExtractor to extract the values to be indexed. In the case of the customer id for a notification, this is a field in the NotificationId , which is the key to the notifications cache. An extractor to extract customer id can be created as shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); This extractor can be used as an index by calling the addIndex method on NamedCache or NamedMap . <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getCustomerId).fromKey(); notifications.addIndex(extractor); The Region Index The second index required is to be able to find notifications for a customer and region. In theory this index is not required, the index to find all notifications for a customer could be used, then those notifications filtered to only return those for the required region. If there will only be a small number of notifications per customer, that may be a suitable approach. This is one of the typical pros and cons that needs to be weighed up when using indexes. Does the cost in memory usage of the index and time to maintain the index on every mutation outweigh the benefits in speed gained by queries. This example is going to add an index on region, because it is an example there are no concerns over performance, and it will show how to perform an indexed query. The extractor to extract region from the NotificationId cache entry key is shown below: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); This can be used to create an index: <markup lang=\"java\" >ValueExtractor&lt;?, ?&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); notifications.addIndex(extractor); Creating the Indexes The repository class already has a method that is called to create any required indexes when it is initialized. This method can be overridden and used to ensure the notifications indexes are added. <markup lang=\"java\" title=\"CustomerRepository.java\" > @Override @SuppressWarnings({\"unchecked\", \"resource\"}) protected void createIndices() { super.createIndices(); CacheService service = customers.getService(); NamedCache&lt;NotificationId, Notification&gt; notifications = service.ensureCache(NOTIFICATIONS_MAP_NAME, service.getContextClassLoader()); notifications.addIndex(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); notifications.addIndex(ValueExtractor.of(NotificationId::getRegion).fromKey()); } Note, that the super class createIndicies() method must be called to ensure any other indicies required by the customer repository are created. Write the NotificationExtractor extractFromEntry method Now that the required indexes will be present the NotificationExtractor.extractFromEntry() method can be written. The techniques used below rely on the indexes being present and would not work if there were no indexes. Without indexes other less efficient methods would be required. The steps the extract method must perform are shown below: Obtain the map of indexes for the notifications cache From the index map, obtain the customer id index From the customer id index obtain the set of notification keys matching the customer id If the region is specified, reduce the set of keys to only those matching the required region For each remaining key, obtain the read-only cache entry containing the notification and add it to the results list return the list of notifications found Each step is covered in detail below: Obtain the map of indexes for the notifications cache The entry passed to the NotificationExtractor.extractFromEntry method when used in an aggregator will be an instance of a BinaryEntry so the entry can safely be cast to BinaryEntry . From a BinaryEntry it is possible to obtain the BackingMapManagerContext and from there the BackingMapContext of other caches. Remember, in this example the aggregator is executed on an entry in the customers cache, so the extractor needs to obtain the BackingMapContext of the notifications cache. From the notifications cache BackingMapContext the map of indexes can be obtained. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); From the index map, obtain the customer id index The index map is a map of MapIndex instances keyed by the ValueExtractor used to create the index. To obtain the customer id index just call the get() method using the same customer id extractor used to create the index above. This is one of the main reasons that all ValueExtractor implementations must properly implement equals() and hashCode() so that they can be used in indexes. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); From the customer id index obtain the set of notification keys matching the customer id A Coherence MapIndex typically holds two internal indexes. The keys in the index are in serialized binary format, that is, they can be used directly to obtain corresponding entries. A map of cache key to the extracted index value for that key A map of extracted index value to the set of keys that match that value In the case of the customer id index that means the index holds a map of binary key to corresponding customer id and a map of customer id to keys of entries for that customer id. The second map is the one required for this use case, which can be obtained from the MapIndex.getIndexContents() method. The set of keys for the customer can then be obtained with a simple get(customerId) on the index contents map (the customer id is just the key of the entry passed to the extractFromEntry method. <markup lang=\"java\" >BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext().getBackingMapContext(\"notifications\"); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); At this point the keys set is the key of all the notification entries for the customer. Further Filter by Region If the region has been specified, the set of keys needs to be further filtered to just those for the required region. This could be achieved a number of ways, but this example is going to show how Coherence filters and indexes can be used to reduce a set of keys. Almost all filters in Coherence implement IndexAwareFilter which means they have an applyIndex method: <markup lang=\"java\" >public &lt;RK&gt; Filter&lt;V&gt; applyIndex( Map&lt;? extends ValueExtractor&lt;? extends V, Object&gt;, ? extends MapIndex&lt;? extends RK, ? extends V, Object&gt;&gt; mapIndexes, Set&lt;? extends RK&gt; setKeys); When the applyIndex method is called, the Set of keys passed in will be reduced to only those keys matching the filter. This means that an EqualsFilter using the region extractor can be used to reduce the set of all keys for the customer down to just those keys matching the region too. Again, the extractor used in the EqualsFilter must be the same extractor used to create the region index. <markup lang=\"java\" >if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Now the keys set has been reduced to only key matching both customer id and region. Obtain the Notifications The set of keys can be used to obtain notification from the notifications cache. The safest way to do this is to use the BackingMapContext.getReadOnlyEntry() method. The final list of notifications will be ordered by creation data. This is possible because the NotificationId class used in this example implements Comparable and makes use of the fact that the Coherence UUID used as a unique id in the notification contains a timestamp. The example used Java streams to process the keys into a list of notifications, the code is shown below: <markup lang=\"java\" > Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2) -&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); The key is mapped to a read-only InvocableMap.Entry Only process entries that are present for the key (in case it has just been removed) Sort the entries using the comparator to sort by key (i.e. NotificationId ) Map the entry to just the value (i.e. the Notification ) Cast the value to a Notification (this is because Java does not know the InvocableMap.Entry generic types) Collect the final Notification instances into a list The Final Method All the code above can be combined into the final extractFromEntry() method. <markup lang=\"java\" > @Override @SuppressWarnings({\"rawtypes\", \"unchecked\"}) public List&lt;Notification&gt; extractFromEntry(Map.Entry entry) { BinaryEntry binaryEntry = (BinaryEntry) entry; BackingMapContext ctx = binaryEntry.getContext() .getBackingMapContext(CustomerRepository.NOTIFICATIONS_MAP_NAME); Map&lt;ValueExtractor, MapIndex&gt; indexMap = ctx.getIndexMap(); MapIndex&lt;Binary, Notification, String&gt; index = indexMap .get(ValueExtractor.of(NotificationId::getCustomerId).fromKey()); String customerId = (String) entry.getKey(); Set&lt;Binary&gt; keys = index.getIndexContents().get(customerId); if (keys == null || keys.isEmpty()) { return Collections.emptyList(); } if (region != null &amp;&amp; !region.isBlank()) { ValueExtractor&lt;NotificationId, String&gt; extractor = ValueExtractor.of(NotificationId::getRegion).fromKey(); EqualsFilter&lt;NotificationId, String&gt; filter = new EqualsFilter&lt;&gt;(extractor, region); filter.applyIndex(indexMap, keys); } Comparator&lt;InvocableMap.Entry&gt; comparator = (e1, e2) -&gt; SafeComparator.compareSafe(Comparator.naturalOrder(), e1.getKey(), e2.getKey()); return keys.stream() .map(ctx::getReadOnlyEntry) .filter(InvocableMap.Entry::isPresent) .sorted(comparator) .map(InvocableMap.Entry::getValue) .map(Notification.class::cast) .collect(Collectors.toList()); } Note Looking at the source code, or JavaDoc, for BackingMapContext will show the getBackingMap() method, which returns the actual map of Binary keys and values in the cache; it should also be obvious that this method is deprecated. It may seem like this is a good way to access the data in the cache for the use case above, but directly accessing the data this way can break the guarantees and locks provided by Coherence. Ideally this method would have been removed, but backwards compatibility constraints mean it is still there, but it should not be used. Add Get Notification Methods to the CustomerRepository Now the NotificationExtractor is complete, methods can now be added to the CustomerRepository to get notifications for a customer and optionally a region. <markup lang=\"java\" title=\"CustomerRepository.java\" > /** * Returns the notifications for a customer. * * @param customerId the identifier of the customer to obtain the notifications for * * @return the notifications for the customer */ public List&lt;Notification&gt; getNotifications(String customerId) { return getNotifications(customerId, null); } /** * Returns the notifications for a customer, and optionally a region. * * @param customerId the identifier of the customer to obtain the notifications for * @param region an optional region to get notifications for * * @return the notifications for the customer, optionally restricted to a region */ public List&lt;Notification&gt; getNotifications(String customerId, String region) { Map&lt;String, List&lt;Notification&gt;&gt; map = getAll(List.of(customerId), new NotificationExtractor(region)); return map.getOrDefault(customerId, Collections.emptyList()); } The getNotifications() method calls the getAll() method on the AbstractRepository super class, which takes a collection of keys and a ValueExtractor . Under the covers, the AbstractRepository.getAll() method just runs a ReducerAggregator with the provided ValueExtractor after ensuring the repository is properly initialized. The map of results returned by getAll() will only ever contain a single entry, as it is only ever called here with a singleton list of keys. The result map will be a map of customer id to a list of notifications. ",
            "title": "Getting Notifications"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " A question often asked about Coherence is whether it can support joins in queries like a database, the answer is that it does not. Efficiently performing distributed joins in queries is extremely difficult to do, and typically data ends up being pulled back to a single member where it is joined. Using key association to guarantee associated data is in a single partition can be used to implement join type aggregations across those related entities. These techniques have been used by customers to implement quite complex join and data enrichment queries in large Coherence applications. ",
            "title": "A Poor Man&#8217;s Join"
        },
        {
            "location": "/examples/guides/905-key-association/README",
            "text": " The examples above show just some uses of key association in Coherence. It can be quite a powerful concept if used wisely. There are some downsides, mainly in cases where the amount of associated data is not very even. For example, in the use case above, if some customers has a very large number of notifications, all those would be stored in single partition. This can lead to some partitions and hence some cluster members using a larger amount of memory than others. Generally in a Coherence cache, keys are reasonably evenly distributed over partitions and cache entry sizes are relatively consistent so uneven memory usage is not an issue, but when using key association it is something to be aware of. ",
            "title": "Summary"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. ",
            "title": "Image Contents"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 23.03-SNAPSHOT the image produced will be oraclecoherence/coherence-ce:23.03-SNAPSHOT To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:23.03-SNAPSHOT ",
            "title": "Building the Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the Image in Kubernetes"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. ",
            "title": "Run the image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. ",
            "title": "Specifying Coherence System Properties"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. ",
            "title": "Specifying JVM Options"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Adding to the Classpath"
        },
        {
            "location": "/coherence-docker/README",
            "text": " This module builds an example Coherence OCI compatible image. The image built in this module is a demo and example of how to build a Coherence image using the JIB Maven Plugin . The image is not intended to be used in production deployments or as a base image, it is specifically for demos, experimentation and learning purposes. Image Contents The Coherence image uses a distroless base image containing OpenJDK. There are many advantages of a distroless image, security being the main one. Of course, you are free to use whatever base image or build mechanism you want for your own images. The image built by the coherence-docker module contains the following Coherence components: Component Description Coherence The core Coherence server Coherence Extend A Coherence*Extend proxy, exposed on port 20000 Coherence gRPC Proxy A Coherence gRPC proxy, exposed on port 1408 Coherence Management Coherence Management over REST, exposed on port 30000 Coherence Metrics Standard Coherence metrics is installed and exposed on port 9612 , but is disabled by default. Coherence metrics can be enabled with the System property coherence.metrics.http.enabled=true Coherence Tracing Coherence tracing is configured to use a Jaeger tracing server. See the Tracing section below. Building the Image Assuming you have first cloned the Coherence CE project the to build the Coherence image run the following command from the top-level Maven prj/ folder: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker The name of the image produced comes from properties in the coherence-docker module pom.xml file. ${docker.registry}/coherence-ce:&lt;version&gt; Where &lt;version&gt; , is the version of the product from the pom.xml file. The ${docker.registry} property is the name of the registry that the image will be published to, by default this is oraclecoherence . So, if the version in the pom.xml is 23.03-SNAPSHOT the image produced will be oraclecoherence/coherence-ce:23.03-SNAPSHOT To change the registry name the image can be built by specifying the docker.registry property, for example: <markup lang=\"bash\" >mvn clean install -P docker -pl coherence-docker -Ddocker.registry=foo The example above would build an image named foo/coherence:23.03-SNAPSHOT Run the image Run the image just like any other image. In Docker this command would be: <markup lang=\"bash\" >docker run -d -P oraclecoherence/coherence-ce:{version-coherence-maven} The -P parameter will ensure that the Extend, gRPC, management and metrics ports will all be exposed. By default, when started the image will run com.tangosol.net.DefaultCacheServer . This may be changed by setting the COH_MAIN_CLASS environment variable to the name of another main class. <markup lang=\"bash\" >docker run -d -P \\ -e COH_MAIN_CLASS=com.tangosol.net.DefaultCacheServer \\ oraclecoherence/coherence-ce:{version-coherence-maven} Run the Image in Kubernetes This image can be run in Kubernetes using the Coherence Operator . The sections below on additional configurations do not apply when using the Coherence Operator to run the image in Kubernetes. The operator provides functionality to configure the container correctly. Specifying Coherence System Properties Many options in Coherence can be set from System properties prefixed with coherence. . The issue here is that System properties are not very easy to pass into the JVM in the container, whereas environment variables are. To help with this the main class which runs in the container will convert any environment variable prefixed with coherence. into a System property before it starts Coherence. <markup lang=\"bash\" >docker run -d -P \\ -e coherence.cluster=testing \\ -e coherence.role=storage \\ oraclecoherence/coherence-ce:{version-coherence-maven} The example above sets two environment variables, coherence.cluster=testing and coherence.role=storage . These will be converted to System properties so Coherence will start the same as it would if the variables had been passed to the JVM command line as -Dcoherence.cluster=testing -Dcoherence.role=storage This only applies to environment variables prefixed with coherence. that have not already set as System properties some other way. Specifying JVM Options Images built with JIB have a fixed entrypoint configured to run the application. This is not very flexible if additional options need to be passed to the JVM. The Coherence image makes use of the JVM&#8217;s ability to load options at start-up from a file by using a JVM option @&lt;file-name&gt; . The Coherence image entrypoint contains @/args/jvm-args.txt , so the JVM will load additional options on start-up from a file named /args/jvm-args.txt . This means that additional options can be provided by adding a volume mapping that adds this file to the container. For example, to set the heap to 5g, the Coherence cluster name to test-cluster and role name to storage then additional JVM arguments will be required. Create a file named jvm-args.txt containing these properties: <markup title=\"jvm-args.txt\" >-Xms5g -Xmx5g -Dcoherence.cluster=test-cluster -Dcoherence.role=storage If the file has been created in a local directory named /home/oracle/test-args then the image can be run with the following command: <markup lang=\"bash\" >docker run -d -P -v /home/oracle/test-args:/args oraclecoherence/coherence-ce:{version-coherence-maven} This will cause Docker to mount the local /home/oracle/test-args directory to the /args directory in the container where the JVM will find the jvm-args.txt file. Adding to the Classpath Images built with JIB have a fixed classpath configured, which is not very flexible if additional resources need to be added to the classpath. The Coherence image maps two additional directories to the classpath that are empty in the image and may be used to add items to the classpath by mapping external volumes to these directories. The additional classpath entries are: /coherence/ext/lib/* - this will add all .jar files under the /coherence/ext/lib/ directory to the classpath /coherence/ext/conf - this adds /coherence/ext/conf to the classpath so that any classes, packages or other resource files in this directory will be added to the classpath. For example: On the local Docker host there is a folder called /dev/my-app/lib that contains .jar files to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/lib:/coherence/ext/lib oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/lib to the /coherence/ext/lib in the container so that any .jar files in the /dev/my-app/lib directory will now be on the Coherence JVM&#8217;s classpath. On the local Docker host there is a folder called /dev/my-app/classes that contains .class files and other application resources to be added to the container classpath. <markup lang=\"bash\" >docker run -d -P -v /dev/my-app/classes:/coherence/ext/conf oraclecoherence/coherence-ce:{version-coherence-maven} The command above maps the local directory /dev/my-app/classes to the /coherence/ext/conf in the container so that any classes and resource files in the /dev/my-app/classes directory will now be on the Coherence JVM&#8217;s classpath. ",
            "title": "Coherence OCI Image"
        },
        {
            "location": "/coherence-docker/README",
            "text": " Multiple containers can be started to form a cluster. By default, Coherence uses multi-cast for cluster discovery but in containers this either will not work, or is not reliable, so well-known-addressing can be used. This example is going to use basic Docker commands and links between containers. There are other ways to achieve the same sort of functionality depending on the network configurations you want to use in Docker. First, determine the name to be used for the first container, in this example it will be storage-1 . Next, create a ` Start the first container in the cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-1 \\ --hostname storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} The first container has been started with a container name of storage-1 , and the host name also set to storage-1 . The container sets the WKA host name to storage-1 using -e coherence.wka=storage-1 (this will be converted to the System property coherence.wka=storage-1 see Specifying Coherence System Properties above). The container sets the Coherence cluster name to testing using -e coherence.cluster=testing (this will be converted to the System property coherence.cluster=testing see Specifying Coherence System Properties above). The important part here is that the container has a name, and the --hostname option has also been set. This will allow the subsequent cluster members to find this container. Now, subsequent containers can be started using the same cluster name and WKA host name, but with different container names and a link to the first container, all the containers will form a single Coherence cluster: <markup lang=\"bash\" >docker run -d -P \\ --name storage-2 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} docker run -d -P \\ --name storage-3 \\ --link storage-1 \\ -e coherence.wka=storage-1 \\ -e coherence.cluster=testing \\ oraclecoherence/coherence-ce:{version-coherence-maven} Two more containers, storage-2 and storage-3 will now be part of the cluster. All the members must have a --link option to the first container and have the same WKA and cluster name properties. ",
            "title": "Clustering"
        },
        {
            "location": "/coherence-docker/README",
            "text": " The Coherence image comes with tracing already configured, it just requires a suitable Jaeger server to send spans to. The simplest way to start is deploy the Jaeger all-in-one server, for example: <markup lang=\"bash\" >docker run -d --name jaeger \\ -e COLLECTOR_ZIPKIN_HTTP_PORT=9411 \\ -p 5775:5775/udp \\ -p 6831:6831/udp \\ -p 6832:6832/udp \\ -p 5778:5778 \\ -p 16686:16686 \\ -p 14268:14268 \\ -p 14250:14250 \\ -p 9411:9411 \\ jaegertracing/all-in-one:latest The Jaeger UI will be available to browse to at http://127.0.0.1:16686 Jaeger has been started with a container name of jaeger , so it will be discoverable using that host name by the Coherence containers. Start the Coherence container with a link to the Jaeger container and set the JAEGER_AGENT_HOST environment variable to jaeger : <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ oraclecoherence/coherence-ce:{version-coherence-maven} Once the Coherence container is running perform some interactions with it using one of the exposed services, i.e Extend or gRPC, and spans will be sent to the Jaeger collector and will be visible in the UI by querying for the coherence service name. The service name used can be changed by setting the JAEGER_SERVICE_NAME environment variable when starting the container, for example: <markup lang=\"bash\" >docker run -d -P --link jaeger \\ -e JAEGER_AGENT_HOST=jaeger \\ -e JAEGER_SERVICE_NAME=coherence-test oraclecoherence/coherence-ce:{version-coherence-maven} Spans will now be sent to Jaeger with the service name coherence-test . Tracing is very useful to show what happens under the covers for a given Coherence API call. Traces are more interesting when they come from a Coherence cluster with multiple members, where the traces span different cluster members. This can easily be done by running multiple containers with tracing enabled and configuring Clustering as described above. ",
            "title": "Tracing"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " This example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " In this example you will utilize streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Contacts have various attributes as described below including home and work addresses stored in the Address class. <markup lang=\"java\" >public class Contact implements Serializable { private int id; private String firstName; private String lastName; private LocalDate doB; private int age; private Address homeAddress; private Address workAddress; ",
            "title": "Contact"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Address contains address details for a Contact . <markup lang=\"java\" >public class Address implements Serializable { private String addressLine1; private String addressLine2; private String city; private String state; private String zip; ",
            "title": "Address"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " The data model consists of the following classes in two maps, customers and orders Contact - Represents a contact Address - Represents an address for a contact Contact Contacts have various attributes as described below including home and work addresses stored in the Address class. <markup lang=\"java\" >public class Contact implements Serializable { private int id; private String firstName; private String lastName; private LocalDate doB; private int age; private Address homeAddress; private Address workAddress; Address Address contains address details for a Contact . <markup lang=\"java\" >public class Address implements Serializable { private String addressLine1; private String addressLine2; private String city; private String state; private String zip; ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Example Details The runExample() method contains the code that exercises the streams API. Refer to the inline code comments for explanations of what each operation is carrying out. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Contact&gt; contacts = getContacts(); System.out.println(\"Cache size is \" + contacts.size()); // get the distinct years that the contacts were born in Set&lt;Integer&gt; setYears = contacts.stream(Contact::getDoB) .map(LocalDate::getYear) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct years the contacts were born in:\" + setYears); // get a set of contact names where the age is &gt; 40 Set&lt;String&gt; setNames = contacts.stream(greater(Contact::getAge, 60)) .map(entry-&gt;entry.extract(Contact::getLastName) + \" \" + entry.extract(Contact::getFirstName) + \" age=\" + entry.extract(Contact::getAge)) .collect(RemoteCollectors.toSet()); System.out.println(\"Set of contact names where age &gt; 60:\" + setNames); // get the distinct set of states for home addresses Set&lt;String&gt; setStates = contacts.stream(Contact::getHomeAddress) .map(Address::getState) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct set of states for home addresses:\" + setStates); // get the average ages of all contacts double avgAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .average() .orElse(0); // in-case of no values System.out.println(\"The average age of all contacts is: \" + avgAge); // get average age using collectors avgAge = contacts.stream() .collect(RemoteCollectors.averagingInt(Contact::getAge)); System.out.println(\"The average age of all contacts using collect() is: \" + avgAge); // get the maximum age of all contacts int maxAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .max() .orElse(0); // in-case of no values System.out.println(\"The maximum age of all contacts is: \" + maxAge); // get average age of contacts who live in MA // Note: The filter should be applied as early as possible, e.g as an argument // to the stream() call in order to take advantage of indexes avgAge = RemoteStream.toIntStream(contacts.stream(equal(homeState(), \"MA\"), Contact::getAge)) .average() .orElse(0); System.out.println(\"The average age of contacts who work in MA is: \" + avgAge); // get a map of birth months and the contact names for that month Map&lt;String, List&lt;Contact&gt;&gt; mapContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(birthMonth())); System.out.println(\"Contacts born in each month:\"); mapContacts.forEach( (key, value)-&gt;System.out.println(\"Month: \" + key + \", Contacts:\" + displayNames(value))); // get a map of states and the contacts living in each state Map&lt;String, List&lt;Contact&gt;&gt; mapStateContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(homeState())); System.out.println(\"Contacts with home addresses in each state:\"); mapStateContacts.forEach( (key, value)-&gt;System.out.println(\"State \" + key + \" has \" + value.size() + \" Contacts\" + displayNames(value))); } The following static extractors are referenced in the above example: <markup lang=\"java\" >/** * A {@link ValueExtractor} to extract the birth month from a {@link Contact}. * * @return the birth month */ protected static ValueExtractor&lt;Contact, String&gt; birthMonth() { return contact-&gt;contact.getDoB().getMonth().toString(); } /** * A {@link ValueExtractor} to extract the home state from a {@link Contact}. * * @return the home state */ protected static ValueExtractor&lt;Contact, String&gt; homeState() { return contact-&gt;contact.getHomeAddress().getState(); } ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Carry out the following to run this example: E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (output is truncated) <markup lang=\"bash\" >Creating 100 customers Cache size is 100 Distinct years the contacts were born in: [1984, 1985, 1986, 1987, 1989, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1979, 1980, 1981, 1983] Set of contact names where age &gt; 60: [Lastname12 Firstname12 age=64, Lastname100 Firstname100 age=70, Lastname77 Firstname77 age=63, Lastname82 Firstname82 age=66, Lastname45 Firstname45 age=71, Lastname84 Firstname84 age=63, Lastname40 Firstname40 age=62, Lastname20 Firstname20 age=68, Lastname63 Firstname63 age=68, Lastname85 Firstname85 age=69, ... truncated ... Lastname96 Firstname96 age=61, Lastname7 Firstname7 age=71, Lastname73 Firstname73 age=61, Lastname14 Firstname14 age=69, Lastname35 Firstname35 age=61 Distinct set of states for home addresses: [HI, TX, MA, TN, AK, WA, NY, AL, CA] The average age of all contacts is: 52.48 The average age of all contacts using collect() is: 52.48 The maximum age of all contacts is: 72 The average age of contacts who work in MA is: 46.666666666666664 Contacts born in each month: Month: JUNE, Contacts: Firstname77 Lastname77 Firstname38 Lastname38 Firstname32 Lastname32 Firstname91 Lastname91 Firstname48 Lastname48 Firstname92 Lastname92 Firstname80 Lastname80 Firstname34 Lastname34 Month: JANUARY, Contacts: Firstname47 Lastname47 Firstname94 Lastname94 Firstname16 Lastname16 Firstname46 Lastname46 Firstname57 Lastname57 Firstname10 Lastname10 Firstname100 Lastname100 Firstname4 Lastname4 Month: MAY, Contacts: Firstname65 Lastname65 Firstname55 Lastname55 Firstname1 Lastname1 Firstname93 Lastname93 Firstname96 Lastname96 Firstname42 Lastname42 Firstname14 Lastname14 Firstname25 Lastname25 Firstname54 Lastname54 ... truncated ... Month: APRIL, Contacts: Firstname59 Lastname59 Firstname15 Lastname15 Firstname90 Lastname90 Firstname50 Lastname50 Firstname45 Lastname45 Firstname33 Lastname33 Firstname76 Lastname76 Firstname23 Lastname23 Contacts with home addresses in each state: State HI has 6 Contacts Firstname32 Lastname32 Firstname68 Lastname68 Firstname17 Lastname17 Firstname42 Lastname42 Firstname18 Lastname18 Firstname39 Lastname39 State TX has 13 Contacts Firstname71 Lastname71 Firstname30 Lastname30 Firstname82 Lastname82 Firstname62 Lastname62 Firstname40 Lastname40 Firstname43 Lastname43 Firstname93 Lastname93 Firstname11 Lastname11 Firstname92 Lastname92 Firstname96 Lastname96 Firstname7 Lastname7 Firstname58 Lastname58 Firstname76 Lastname76 ... truncated ... State AL has 10 Contacts Firstname47 Lastname47 Firstname46 Lastname46 Firstname22 Lastname22 Firstname66 Lastname66 Firstname81 Lastname81 Firstname15 Lastname15 Firstname25 Lastname25 Firstname35 Lastname35 Firstname34 Lastname34 Firstname89 Lastname89 State CA has 14 Contacts Firstname77 Lastname77 Firstname61 Lastname61 Firstname28 Lastname28 Firstname5 Lastname5 Firstname1 Lastname1 Firstname91 Lastname91 Firstname87 Lastname87 Firstname79 Lastname79 Firstname80 Lastname80 Firstname12 Lastname12 Firstname33 Lastname33 Firstname95 Lastname95 Firstname98 Lastname98 Firstname100 Lastname100 ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " In this example you have seen how to utilized streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " Built in Aggregators Custom Aggregators ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/125-streams/README",
            "text": " This guide walks you through how to use the Streams API with Coherence. The Java streams implementation provides an efficient way to query and process data sequentially or in parallel to take advantage of multi-core architectures. The processing occurs in steps: Data is aggregated from a source (such as collections or arrays) into a read-only stream. The stream represents object references and does not actually store the data. Intermediate operations are then declared on the stream. Intermediate operations for filtering, sorting, mapping, and so on are supported. Lambda expressions are often used when declaring intermediate operations and provide a functional way to work on the data. Intermediate operations are aggregated and can be chained together: each subsequent operation is performed on a stream that contains the result of the previous operation. Intermediate operations are lazy and are not actually executed until a final terminal operation is performed. A final terminal operation is declared. Terminal operations for counting, adding, averaging, and so on are supported. The terminal operation automatically iterates over the objects in the stream returns an aggregated result. Java streams provide similar functionality as Coherence data grid aggregation. However, streams are not efficient when executed in a distributed environment. To leverage the stream programming model and also ensure that streams can be executed remotely across the cluster, Coherence has extended the streams API. For details see the com.tangosol.util.stream in the Java API Reference for Oracle Coherence. Table of Contents What You Will Build What You Need Example Data Model Review the Example Code Run the Example Summary See Also What You Will Build In this example you will utilize streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Example Data Model The data model consists of the following classes in two maps, customers and orders Contact - Represents a contact Address - Represents an address for a contact Contact Contacts have various attributes as described below including home and work addresses stored in the Address class. <markup lang=\"java\" >public class Contact implements Serializable { private int id; private String firstName; private String lastName; private LocalDate doB; private int age; private Address homeAddress; private Address workAddress; Address Address contains address details for a Contact . <markup lang=\"java\" >public class Address implements Serializable { private String addressLine1; private String addressLine2; private String city; private String state; private String zip; Review the Example Code Example Details The runExample() method contains the code that exercises the streams API. Refer to the inline code comments for explanations of what each operation is carrying out. <markup lang=\"java\" >/** * Run the example. */ public void runExample() { NamedMap&lt;Integer, Contact&gt; contacts = getContacts(); System.out.println(\"Cache size is \" + contacts.size()); // get the distinct years that the contacts were born in Set&lt;Integer&gt; setYears = contacts.stream(Contact::getDoB) .map(LocalDate::getYear) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct years the contacts were born in:\" + setYears); // get a set of contact names where the age is &gt; 40 Set&lt;String&gt; setNames = contacts.stream(greater(Contact::getAge, 60)) .map(entry-&gt;entry.extract(Contact::getLastName) + \" \" + entry.extract(Contact::getFirstName) + \" age=\" + entry.extract(Contact::getAge)) .collect(RemoteCollectors.toSet()); System.out.println(\"Set of contact names where age &gt; 60:\" + setNames); // get the distinct set of states for home addresses Set&lt;String&gt; setStates = contacts.stream(Contact::getHomeAddress) .map(Address::getState) .distinct() .collect(RemoteCollectors.toSet()); System.out.println(\"Distinct set of states for home addresses:\" + setStates); // get the average ages of all contacts double avgAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .average() .orElse(0); // in-case of no values System.out.println(\"The average age of all contacts is: \" + avgAge); // get average age using collectors avgAge = contacts.stream() .collect(RemoteCollectors.averagingInt(Contact::getAge)); System.out.println(\"The average age of all contacts using collect() is: \" + avgAge); // get the maximum age of all contacts int maxAge = contacts.stream(Contact::getAge) .mapToInt(Number::intValue) .max() .orElse(0); // in-case of no values System.out.println(\"The maximum age of all contacts is: \" + maxAge); // get average age of contacts who live in MA // Note: The filter should be applied as early as possible, e.g as an argument // to the stream() call in order to take advantage of indexes avgAge = RemoteStream.toIntStream(contacts.stream(equal(homeState(), \"MA\"), Contact::getAge)) .average() .orElse(0); System.out.println(\"The average age of contacts who work in MA is: \" + avgAge); // get a map of birth months and the contact names for that month Map&lt;String, List&lt;Contact&gt;&gt; mapContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(birthMonth())); System.out.println(\"Contacts born in each month:\"); mapContacts.forEach( (key, value)-&gt;System.out.println(\"Month: \" + key + \", Contacts:\" + displayNames(value))); // get a map of states and the contacts living in each state Map&lt;String, List&lt;Contact&gt;&gt; mapStateContacts = contacts.stream() .map(Map.Entry::getValue) .collect(RemoteCollectors.groupingBy(homeState())); System.out.println(\"Contacts with home addresses in each state:\"); mapStateContacts.forEach( (key, value)-&gt;System.out.println(\"State \" + key + \" has \" + value.size() + \" Contacts\" + displayNames(value))); } The following static extractors are referenced in the above example: <markup lang=\"java\" >/** * A {@link ValueExtractor} to extract the birth month from a {@link Contact}. * * @return the birth month */ protected static ValueExtractor&lt;Contact, String&gt; birthMonth() { return contact-&gt;contact.getDoB().getMonth().toString(); } /** * A {@link ValueExtractor} to extract the home state from a {@link Contact}. * * @return the home state */ protected static ValueExtractor&lt;Contact, String&gt; homeState() { return contact-&gt;contact.getHomeAddress().getState(); } Run the Example Carry out the following to run this example: E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test This will generate output similar to the following: (output is truncated) <markup lang=\"bash\" >Creating 100 customers Cache size is 100 Distinct years the contacts were born in: [1984, 1985, 1986, 1987, 1989, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1979, 1980, 1981, 1983] Set of contact names where age &gt; 60: [Lastname12 Firstname12 age=64, Lastname100 Firstname100 age=70, Lastname77 Firstname77 age=63, Lastname82 Firstname82 age=66, Lastname45 Firstname45 age=71, Lastname84 Firstname84 age=63, Lastname40 Firstname40 age=62, Lastname20 Firstname20 age=68, Lastname63 Firstname63 age=68, Lastname85 Firstname85 age=69, ... truncated ... Lastname96 Firstname96 age=61, Lastname7 Firstname7 age=71, Lastname73 Firstname73 age=61, Lastname14 Firstname14 age=69, Lastname35 Firstname35 age=61 Distinct set of states for home addresses: [HI, TX, MA, TN, AK, WA, NY, AL, CA] The average age of all contacts is: 52.48 The average age of all contacts using collect() is: 52.48 The maximum age of all contacts is: 72 The average age of contacts who work in MA is: 46.666666666666664 Contacts born in each month: Month: JUNE, Contacts: Firstname77 Lastname77 Firstname38 Lastname38 Firstname32 Lastname32 Firstname91 Lastname91 Firstname48 Lastname48 Firstname92 Lastname92 Firstname80 Lastname80 Firstname34 Lastname34 Month: JANUARY, Contacts: Firstname47 Lastname47 Firstname94 Lastname94 Firstname16 Lastname16 Firstname46 Lastname46 Firstname57 Lastname57 Firstname10 Lastname10 Firstname100 Lastname100 Firstname4 Lastname4 Month: MAY, Contacts: Firstname65 Lastname65 Firstname55 Lastname55 Firstname1 Lastname1 Firstname93 Lastname93 Firstname96 Lastname96 Firstname42 Lastname42 Firstname14 Lastname14 Firstname25 Lastname25 Firstname54 Lastname54 ... truncated ... Month: APRIL, Contacts: Firstname59 Lastname59 Firstname15 Lastname15 Firstname90 Lastname90 Firstname50 Lastname50 Firstname45 Lastname45 Firstname33 Lastname33 Firstname76 Lastname76 Firstname23 Lastname23 Contacts with home addresses in each state: State HI has 6 Contacts Firstname32 Lastname32 Firstname68 Lastname68 Firstname17 Lastname17 Firstname42 Lastname42 Firstname18 Lastname18 Firstname39 Lastname39 State TX has 13 Contacts Firstname71 Lastname71 Firstname30 Lastname30 Firstname82 Lastname82 Firstname62 Lastname62 Firstname40 Lastname40 Firstname43 Lastname43 Firstname93 Lastname93 Firstname11 Lastname11 Firstname92 Lastname92 Firstname96 Lastname96 Firstname7 Lastname7 Firstname58 Lastname58 Firstname76 Lastname76 ... truncated ... State AL has 10 Contacts Firstname47 Lastname47 Firstname46 Lastname46 Firstname22 Lastname22 Firstname66 Lastname66 Firstname81 Lastname81 Firstname15 Lastname15 Firstname25 Lastname25 Firstname35 Lastname35 Firstname34 Lastname34 Firstname89 Lastname89 State CA has 14 Contacts Firstname77 Lastname77 Firstname61 Lastname61 Firstname28 Lastname28 Firstname5 Lastname5 Firstname1 Lastname1 Firstname91 Lastname91 Firstname87 Lastname87 Firstname79 Lastname79 Firstname80 Lastname80 Firstname12 Lastname12 Firstname33 Lastname33 Firstname95 Lastname95 Firstname98 Lastname98 Firstname100 Lastname100 Summary In this example you have seen how to utilized streams methods on the NamedMap API to query and aggregate and group data from a contacts NamedMap . See Also Built in Aggregators Custom Aggregators ",
            "title": "Streams"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The simplest way to create a Subscribers is from the Coherence Session API, by calling the createSubscriber method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); The code snippet above creates an anonymous Subscriber that subscribes to String messages from the topic named test-topic . Alternatively, a Subscriber can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Subscriber&lt;String&gt; subscriber = topic.createSubscriber(); Both the Session.createSubscriber() and NamedTopic.createSubscriber() methods also take a var-args array of Subscriber.Option instances to further configure the behaviour of the subscriber. Some of these options are described below. ",
            "title": "Creating Subscribers"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " To create a subscriber that is part of a subscriber group the Subscriber.Name option can be used. Subscriber groups have a unique name and a subscriber joins a group by specifying the group name. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import static com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", inGroup(\"group-one\")); The code above creates a subscriber that subscribes from the test-topic .The subscriber is part of the group named group-one .This is specified by adding a Subscriber.Name option using the static factory method Subscriber.Name.inGroup . ",
            "title": "Creating Group Subscribers"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Subscribers should ideally be closed when application code finishes with them so that any server side and client side resources associated with them are also closed and cleaned up.Orphaned subscribers, where the client application has gone away, will eventually be cleaned up by server side code.Subscriber groups that are durable will remain until manually removed. Subscribers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); try (Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\")) { // ... receive messages ... } In the above example, the subscriber is used to receive messages inside the try/catch block.Once the try/catch block exits the subscriber is closed. When a subscriber is closed, it can no longer be used.Calls to subscriber methods after closing will throw an IllegalStateException . ",
            "title": "Closing Subscribers"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " By default, the CompletableFuture returned from a call to receive will not complete until a message is received. If the topic is empty (or in the case of a group subscriber all the channels owned by the subscriber are empty) the future will not complete until a new message is published to the topic or channel. This behaviour can be changed so that is a topic or owned channels are empty, the future will complete with a null element value. This is controlled by creating the subscriber with the CompleteOnEmpty option. For example, to create a subscriber where calls to receive return even if the topic is empty: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); Element&lt;String&gt; element = future.get(); The subscriber is created using the CompleteOnEmpty.enabled() option, so it will complete futures even if the topic is empty. The call to future.get() may return null if the topic or owned channels are empty. ",
            "title": "Future Completion"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Because the subscriber API is asynchronous, multiple consecutive calls can be made to the receive methods, without waiting for the first call to complete.To maintain message delivery order, the subscriber will complete the futures in the order that the calls were made. Important Any use of the CompletableFuture async API (for example future.thenApplyAsync() , future.handleAsync() etc) to hand of completion handling to another thread will then remove any ordering guarantees for message processing. The same applies to application code that manually hands the returned elements off to other worker threads for processing. It is up to the application code to then handle the futures in such a way that ordering is maintained if that is important to the application&#8217;s use-case. The use of the synchronous CompletableFuture API (for example future.thenApply() , future.handle() etc.) will cause completion of other futures by the subscriber to block until the handler code is complete.To maintain order of completion, the subscribe queues up the futures to be completed by a single daemon thread. For examples: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Void&gt; futureOne = subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); CompletableFuture&lt;Void&gt; futureTwo = subscriber.receive() .thenAccept(element -&gt; { // handle second element... }); In the example above, the code that handles the first element must fully complete before the second future will complete. In use cases where order of processing on the client is not important the full async API can be used. Important Another important aspect of using the async API with subscribers is correct error handling. This is bad code: <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); If the call to receive() fails and the future completes exceptionally, or the handler code in the thenAccept call fails and throws an exception, those exceptions will be lost and not even logged. A better way is to always finish with a handle call or use one of the other methods of the CompletableFuture API to check for exceptions. <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // process second element... }).handle((_void, error) -&gt; { if (error != null) { // something went wrong!!! } return null; }); ",
            "title": "Multiple calls to Receive and Message Ordering"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The sole purpose of a subscriber is to receive messages from a topic.This is done by calling the Subscriber.receive() method to receive a single message or Subscriber.receive(int) to receive multiple messages in a batch.Both forms of the receive method are asynchronous, and return a CompletableFuture that will be completed with the result of polling the topic for messages. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); CompletableFuture&lt;List&lt;Element&lt;String&gt;&gt;&gt; futureBatch subscriber.receive(10); The first call to receive will return a CompletableFuture that will complete with a Subscriber.Element that will contain the message from the topic and meta-data about the element. The second call to receive will return a CompletableFuture that will complete with a batch of upto 10 elements.The int parameter is a hint to the subscriber to return a batch and is the maximum number of messages that should be returned, the subscriber could return fewer messages.At most, a subscriber will return a full page of messages in a batch, so calling receive with a value higher than a page size will not return more messages than the page contains. Future Completion By default, the CompletableFuture returned from a call to receive will not complete until a message is received. If the topic is empty (or in the case of a group subscriber all the channels owned by the subscriber are empty) the future will not complete until a new message is published to the topic or channel. This behaviour can be changed so that is a topic or owned channels are empty, the future will complete with a null element value. This is controlled by creating the subscriber with the CompleteOnEmpty option. For example, to create a subscriber where calls to receive return even if the topic is empty: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); Element&lt;String&gt; element = future.get(); The subscriber is created using the CompleteOnEmpty.enabled() option, so it will complete futures even if the topic is empty. The call to future.get() may return null if the topic or owned channels are empty. Multiple calls to Receive and Message Ordering Because the subscriber API is asynchronous, multiple consecutive calls can be made to the receive methods, without waiting for the first call to complete.To maintain message delivery order, the subscriber will complete the futures in the order that the calls were made. Important Any use of the CompletableFuture async API (for example future.thenApplyAsync() , future.handleAsync() etc) to hand of completion handling to another thread will then remove any ordering guarantees for message processing. The same applies to application code that manually hands the returned elements off to other worker threads for processing. It is up to the application code to then handle the futures in such a way that ordering is maintained if that is important to the application&#8217;s use-case. The use of the synchronous CompletableFuture API (for example future.thenApply() , future.handle() etc.) will cause completion of other futures by the subscriber to block until the handler code is complete.To maintain order of completion, the subscribe queues up the futures to be completed by a single daemon thread. For examples: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Void&gt; futureOne = subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); CompletableFuture&lt;Void&gt; futureTwo = subscriber.receive() .thenAccept(element -&gt; { // handle second element... }); In the example above, the code that handles the first element must fully complete before the second future will complete. In use cases where order of processing on the client is not important the full async API can be used. Important Another important aspect of using the async API with subscribers is correct error handling. This is bad code: <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); If the call to receive() fails and the future completes exceptionally, or the handler code in the thenAccept call fails and throws an exception, those exceptions will be lost and not even logged. A better way is to always finish with a handle call or use one of the other methods of the CompletableFuture API to check for exceptions. <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // process second element... }).handle((_void, error) -&gt; { if (error != null) { // something went wrong!!! } return null; }); ",
            "title": "Receiving Messages"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The Element returned from a receive call has a commit() method that can be used to commit the element&#8217;s channel and position. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value CommitResult result = element.commit(); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The commit method is called to commit the position of the element. By committing the element directly, application code does not need to track the channel or positions of received elements. ",
            "title": "Commit a Received Element"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " To commit a Position in a channel directly the Subscriber.commit(int, Position) method can be used. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value int channel = element.getChannel(); Position position = element.commit(); CommitResult result = subscriber.commit(channel, position); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The channel and Position can be obtained for the element The channel and Position can then be committed later by calling commit on the subscriber ",
            "title": "Commit a Position"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " In order to provide at least once delivery guarantees, the subscriber API has methods that allow messages to be committed, so that the server knows they have been processed and will not re-deliver them in the case where a group subscriber fails over or is closed, and a new subscriber in the group takes over the channel ownership. When a subscriber does a commit, it is actually committing a position in a channel of a topic.It effectively says that a specific position in a channel and all earlier positions have been processed.For example if a subscriber reads 10 messages from positions 0 - 9 and commits position 9, then positions 0 - 8 are also committed. There are two ways to commit a position; either using the commit method on an Element returned from a call to receive() , or by calling the commit method on a Subscriber that takes a channel and Position argument. Commit a Received Element The Element returned from a receive call has a commit() method that can be used to commit the element&#8217;s channel and position. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value CommitResult result = element.commit(); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The commit method is called to commit the position of the element. By committing the element directly, application code does not need to track the channel or positions of received elements. Commit a Position To commit a Position in a channel directly the Subscriber.commit(int, Position) method can be used. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value int channel = element.getChannel(); Position position = element.commit(); CommitResult result = subscriber.commit(channel, position); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The channel and Position can be obtained for the element The channel and Position can then be committed later by calling commit on the subscriber ",
            "title": "Committing (Message Acknowledgement)"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The subscriber has a seek method that takes a channel, and a Position that moves the subscriber to the specified position in the channel. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Element&lt;String&gt; firstElement = subscriber.receive().get(); for (int i = 0; i &lt; 10; i++) { Element&lt;String&gt; element = subscriber.receive().get(); // process element... } subscriber.seek(firstElement.getChannel(), firstElement.getPosition()); The example above is a bit contrived, but shows how seek can be used.The first element is received from the topic. Another 10 elements are then processed from the subscriber. The seek method is then used to move the subscriber back to the position of the first message. When seeking, the next message received is the message after the seek position.In the example above, after the seek call the next message received wil not be the same as first element, it will be the next message, so the first message received in the for loop. The subscriber also has methods to seek to the head (re-read the first message) or tail (read the next message published) for a channel without needing to know the head or tail positions. ",
            "title": "Seek to a Position"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Subscribers can also be repositioned to the next message based on a timestamp that the message was published. All messages have a timestamp based on the Coherence cluster time in the storage member that accepted the published message. When seeking using a timestamp, the subscriber is repositioned such that the next message received is the first message after the specified timestamp. The timestamp is specified as a java.time.Instant when seeking to a timestamp. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Instant timestamp = LocalDateTime.of(LocalDate.now(), LocalTime.of(20, 30)) .toInstant(ZoneOffset.UTC); Position position = subscriber.seek(1, timestamp); A java.time.Instant is created for 20:30 today. Seek is called to reposition the subscriber so that the next message received from channel 1 will be the first message published after 20:30. Repositioning to a timestamp in the future will reposition the subscriber at the tail, so the next message received will be the next published message, regardless of the time. It is not possible to seek to a timestamp in the future so that messages are ignored until the time is reached. ",
            "title": "Seek to a Timestamp"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " The common behaviour for a subscriber is to connect and then receive messages in order until all the messages are processed.Sometimes though it is desirable to rewind a subscriber to reprocess previously consumed messages, or to move a subscriber forwards to skip messages. When rewinding a position, whether the action is successful or not depends on how the topic has been configured.If the topic is configured to retain messages (not the default) then previously received messages are still available and can be re-received.For topics that do not retain messages, then messages are removed once all connected subscribers, or subscriber groups, have read the message.In the case of non-retained topics therefore, it may not be possible to rewind as the messages may have been removed.Even in topics that retain consumed messages, the messages may have been removed if the topic is configured with message expiry. If an attempt is made to rewind further back than the first message in the topic, the seek will reposition the subscriber just before the first available message.If an attempt is made to reposition a subscriber much further ahead than the current tail of the topic, the subscriber will be positioned at the tail, so that it receives the next published message. Seek to a Position The subscriber has a seek method that takes a channel, and a Position that moves the subscriber to the specified position in the channel. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Element&lt;String&gt; firstElement = subscriber.receive().get(); for (int i = 0; i &lt; 10; i++) { Element&lt;String&gt; element = subscriber.receive().get(); // process element... } subscriber.seek(firstElement.getChannel(), firstElement.getPosition()); The example above is a bit contrived, but shows how seek can be used.The first element is received from the topic. Another 10 elements are then processed from the subscriber. The seek method is then used to move the subscriber back to the position of the first message. When seeking, the next message received is the message after the seek position.In the example above, after the seek call the next message received wil not be the same as first element, it will be the next message, so the first message received in the for loop. The subscriber also has methods to seek to the head (re-read the first message) or tail (read the next message published) for a channel without needing to know the head or tail positions. Seek to a Timestamp Subscribers can also be repositioned to the next message based on a timestamp that the message was published. All messages have a timestamp based on the Coherence cluster time in the storage member that accepted the published message. When seeking using a timestamp, the subscriber is repositioned such that the next message received is the first message after the specified timestamp. The timestamp is specified as a java.time.Instant when seeking to a timestamp. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Instant timestamp = LocalDateTime.of(LocalDate.now(), LocalTime.of(20, 30)) .toInstant(ZoneOffset.UTC); Position position = subscriber.seek(1, timestamp); A java.time.Instant is created for 20:30 today. Seek is called to reposition the subscriber so that the next message received from channel 1 will be the first message published after 20:30. Repositioning to a timestamp in the future will reposition the subscriber at the tail, so the next message received will be the next published message, regardless of the time. It is not possible to seek to a timestamp in the future so that messages are ignored until the time is reached. ",
            "title": "Seeking - Reposition a Subscriber"
        },
        {
            "location": "/docs/topics/04_subscribers",
            "text": " Subscribers are used to receive messages to a Coherence topic, a subscriber receives messages from a single topic. Creating Subscribers Creating Subscriber Groups Closing Subscribers Receiving Messages Committing Seek to a Position Creating Subscribers The simplest way to create a Subscribers is from the Coherence Session API, by calling the createSubscriber method. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); The code snippet above creates an anonymous Subscriber that subscribes to String messages from the topic named test-topic . Alternatively, a Subscriber can be obtained directly from a NamedTopic instance. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.NamedTopic; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); NamedTopic&lt;String&gt; topic = session.getTopic(\"test-topic\"); Subscriber&lt;String&gt; subscriber = topic.createSubscriber(); Both the Session.createSubscriber() and NamedTopic.createSubscriber() methods also take a var-args array of Subscriber.Option instances to further configure the behaviour of the subscriber. Some of these options are described below. Creating Group Subscribers To create a subscriber that is part of a subscriber group the Subscriber.Name option can be used. Subscriber groups have a unique name and a subscriber joins a group by specifying the group name. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import static com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", inGroup(\"group-one\")); The code above creates a subscriber that subscribes from the test-topic .The subscriber is part of the group named group-one .This is specified by adding a Subscriber.Name option using the static factory method Subscriber.Name.inGroup . Closing Subscribers Subscribers should ideally be closed when application code finishes with them so that any server side and client side resources associated with them are also closed and cleaned up.Orphaned subscribers, where the client application has gone away, will eventually be cleaned up by server side code.Subscriber groups that are durable will remain until manually removed. Subscribers have a close() method, and are in fact auto-closable, so can be used in a try with resources block.For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; Session session = Coherence.getSession(); try (Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\")) { // ... receive messages ... } In the above example, the subscriber is used to receive messages inside the try/catch block.Once the try/catch block exits the subscriber is closed. When a subscriber is closed, it can no longer be used.Calls to subscriber methods after closing will throw an IllegalStateException . Receiving Messages The sole purpose of a subscriber is to receive messages from a topic.This is done by calling the Subscriber.receive() method to receive a single message or Subscriber.receive(int) to receive multiple messages in a batch.Both forms of the receive method are asynchronous, and return a CompletableFuture that will be completed with the result of polling the topic for messages. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\"); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); CompletableFuture&lt;List&lt;Element&lt;String&gt;&gt;&gt; futureBatch subscriber.receive(10); The first call to receive will return a CompletableFuture that will complete with a Subscriber.Element that will contain the message from the topic and meta-data about the element. The second call to receive will return a CompletableFuture that will complete with a batch of upto 10 elements.The int parameter is a hint to the subscriber to return a batch and is the maximum number of messages that should be returned, the subscriber could return fewer messages.At most, a subscriber will return a full page of messages in a batch, so calling receive with a value higher than a page size will not return more messages than the page contains. Future Completion By default, the CompletableFuture returned from a call to receive will not complete until a message is received. If the topic is empty (or in the case of a group subscriber all the channels owned by the subscriber are empty) the future will not complete until a new message is published to the topic or channel. This behaviour can be changed so that is a topic or owned channels are empty, the future will complete with a null element value. This is controlled by creating the subscriber with the CompleteOnEmpty option. For example, to create a subscriber where calls to receive return even if the topic is empty: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Element&lt;String&gt;&gt; future subscriber.receive(); Element&lt;String&gt; element = future.get(); The subscriber is created using the CompleteOnEmpty.enabled() option, so it will complete futures even if the topic is empty. The call to future.get() may return null if the topic or owned channels are empty. Multiple calls to Receive and Message Ordering Because the subscriber API is asynchronous, multiple consecutive calls can be made to the receive methods, without waiting for the first call to complete.To maintain message delivery order, the subscriber will complete the futures in the order that the calls were made. Important Any use of the CompletableFuture async API (for example future.thenApplyAsync() , future.handleAsync() etc) to hand of completion handling to another thread will then remove any ordering guarantees for message processing. The same applies to application code that manually hands the returned elements off to other worker threads for processing. It is up to the application code to then handle the futures in such a way that ordering is maintained if that is important to the application&#8217;s use-case. The use of the synchronous CompletableFuture API (for example future.thenApply() , future.handle() etc.) will cause completion of other futures by the subscriber to block until the handler code is complete.To maintain order of completion, the subscribe queues up the futures to be completed by a single daemon thread. For examples: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CompleteOnEmpty; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", CompleteOnEmpty.enabled()); CompletableFuture&lt;Void&gt; futureOne = subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); CompletableFuture&lt;Void&gt; futureTwo = subscriber.receive() .thenAccept(element -&gt; { // handle second element... }); In the example above, the code that handles the first element must fully complete before the second future will complete. In use cases where order of processing on the client is not important the full async API can be used. Important Another important aspect of using the async API with subscribers is correct error handling. This is bad code: <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // handle first element... }); If the call to receive() fails and the future completes exceptionally, or the handler code in the thenAccept call fails and throws an exception, those exceptions will be lost and not even logged. A better way is to always finish with a handle call or use one of the other methods of the CompletableFuture API to check for exceptions. <markup lang=\"java\" >subscriber.receive() .thenAccept(element -&gt; { // process second element... }).handle((_void, error) -&gt; { if (error != null) { // something went wrong!!! } return null; }); Committing (Message Acknowledgement) In order to provide at least once delivery guarantees, the subscriber API has methods that allow messages to be committed, so that the server knows they have been processed and will not re-deliver them in the case where a group subscriber fails over or is closed, and a new subscriber in the group takes over the channel ownership. When a subscriber does a commit, it is actually committing a position in a channel of a topic.It effectively says that a specific position in a channel and all earlier positions have been processed.For example if a subscriber reads 10 messages from positions 0 - 9 and commits position 9, then positions 0 - 8 are also committed. There are two ways to commit a position; either using the commit method on an Element returned from a call to receive() , or by calling the commit method on a Subscriber that takes a channel and Position argument. Commit a Received Element The Element returned from a receive call has a commit() method that can be used to commit the element&#8217;s channel and position. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value CommitResult result = element.commit(); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The commit method is called to commit the position of the element. By committing the element directly, application code does not need to track the channel or positions of received elements. Commit a Position To commit a Position in a channel directly the Subscriber.commit(int, Position) method can be used. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); CompletableFuture&lt;Element&lt;String&gt;&gt; future = subscriber.receive(); Element&lt;String&gt; element = future.get(); String value = element.getValue(); // process the message value int channel = element.getChannel(); Position position = element.commit(); CommitResult result = subscriber.commit(channel, position); The application calls receive() The element will be returned when the future completes The message value can be obtained from the element Application code processes the message value The channel and Position can be obtained for the element The channel and Position can then be committed later by calling commit on the subscriber Seeking - Reposition a Subscriber The common behaviour for a subscriber is to connect and then receive messages in order until all the messages are processed.Sometimes though it is desirable to rewind a subscriber to reprocess previously consumed messages, or to move a subscriber forwards to skip messages. When rewinding a position, whether the action is successful or not depends on how the topic has been configured.If the topic is configured to retain messages (not the default) then previously received messages are still available and can be re-received.For topics that do not retain messages, then messages are removed once all connected subscribers, or subscriber groups, have read the message.In the case of non-retained topics therefore, it may not be possible to rewind as the messages may have been removed.Even in topics that retain consumed messages, the messages may have been removed if the topic is configured with message expiry. If an attempt is made to rewind further back than the first message in the topic, the seek will reposition the subscriber just before the first available message.If an attempt is made to reposition a subscriber much further ahead than the current tail of the topic, the subscriber will be positioned at the tail, so that it receives the next published message. Seek to a Position The subscriber has a seek method that takes a channel, and a Position that moves the subscriber to the specified position in the channel. <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.CommitResult; import com.tangosol.net.topic.Subscriber.Element; import com.tangosol.net.topic.Subscriber.Name.inGroup; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Element&lt;String&gt; firstElement = subscriber.receive().get(); for (int i = 0; i &lt; 10; i++) { Element&lt;String&gt; element = subscriber.receive().get(); // process element... } subscriber.seek(firstElement.getChannel(), firstElement.getPosition()); The example above is a bit contrived, but shows how seek can be used.The first element is received from the topic. Another 10 elements are then processed from the subscriber. The seek method is then used to move the subscriber back to the position of the first message. When seeking, the next message received is the message after the seek position.In the example above, after the seek call the next message received wil not be the same as first element, it will be the next message, so the first message received in the for loop. The subscriber also has methods to seek to the head (re-read the first message) or tail (read the next message published) for a channel without needing to know the head or tail positions. Seek to a Timestamp Subscribers can also be repositioned to the next message based on a timestamp that the message was published. All messages have a timestamp based on the Coherence cluster time in the storage member that accepted the published message. When seeking using a timestamp, the subscriber is repositioned such that the next message received is the first message after the specified timestamp. The timestamp is specified as a java.time.Instant when seeking to a timestamp. For example: <markup lang=\"java\" >import com.tangosol.net.Session; import com.tangosol.net.topic.Position; import com.tangosol.net.topic.Subscriber; import com.tangosol.net.topic.Subscriber.Element; Session session = Coherence.getSession(); Subscriber&lt;String&gt; subscriber = session.createSubscriber(\"test-topic\", Name.inGroup(\"test-group\")); Instant timestamp = LocalDateTime.of(LocalDate.now(), LocalTime.of(20, 30)) .toInstant(ZoneOffset.UTC); Position position = subscriber.seek(1, timestamp); A java.time.Instant is created for 20:30 today. Seek is called to reposition the subscriber so that the next message received from channel 1 will be the first message published after 20:30. Repositioning to a timestamp in the future will reposition the subscriber at the tail, so the next message received will be the next published message, regardless of the time. It is not possible to seek to a timestamp in the future so that messages are ignored until the time is reached. ",
            "title": "Subscribers"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Classes Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " In this example you will run a test that will demonstrate using Durable Events. The test does the following: Starts 2 Cache Servers using Oracle Bedrock Creates and registers a version aware MapListener Inserts, updates and deletes cache entries Simulates the client being disconnected Issues cache mutations remotely while the client is disconnected Reconnects the client and validate that events generated while the client was disconnected are received To enable Durable Events you must have the following system properties set for cache servers: Enable active persistence by using -Dcoherence.distributed.persistence.mode=active Set the directory to store Durable Events using -Dcoherence.distributed.persistence.events.dir=/my/events/dir Optionally set the directory to store active persistence using -Dcoherence.distributed.persistence.base.dir=/my/persistence/dir Register a versioned MapListener on a NamedMap If you do not set the directory to store active persistence the default directory coherence off the users home directory will be chosen. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Review the Customer class This example uses the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private long id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review how the 2 cache servers are started by Oracle Bedrock <markup lang=\"java\" >/** * Start a Coherence cluster with two cache servers using Oracle Bedrock. * * @throws IOException if any errors creating temporary directory */ @BeforeAll public static void startup() throws IOException { persistenceDir = FileHelper.createTempDir(); File eventsDir = new File(persistenceDir, \"events\"); CoherenceClusterBuilder builder = new CoherenceClusterBuilder() .with(SystemProperty.of(\"coherence.distributed.partitions\", 23), SystemProperty.of(\"coherence.distributed.persistence.mode\", \"active\"), SystemProperty.of(\"coherence.distributed.persistence.base.dir\", persistenceDir.getAbsolutePath()), SystemProperty.of(\"coherence.distributed.persistence.events.dir\", eventsDir.getAbsolutePath()), ClusterName.of(CLUSTER_NAME), RoleName.of(\"storage\"), DisplayName.of(\"storage\"), Multicast.ttl(0)) .include(2, CoherenceClusterMember.class, // testLogs, LocalStorage.enabled()); cluster = builder.build(); Eventually.assertDeferred(() -&gt; cluster.getClusterSize(), is(2)); for (CoherenceClusterMember member : cluster) { Eventually.assertDeferred(member::isReady, is(true)); } } Set the partition count to 23 to reduce the startup time Set active persistence mode Set the base directory to store persistence files Set the base directory to store persistence events Review the DurableEventsTest class <markup lang=\"java\" >/** * Runs a test to simulate a client registering a versioned {@link MapListener}, * being disconnected, reconnecting, and then receiving all the events that were * missed while the client was disconnected. */ @Test public void testDurableEvents() throws Exception { AtomicInteger eventCount = new AtomicInteger(); String cacheName = \"customers\"; System.setProperty(\"coherence.cluster\", CLUSTER_NAME); System.setProperty(\"coherence.role\", \"client\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); try (Coherence coherence = Coherence.clusterMember()) { coherence.start().get(5, TimeUnit.MINUTES); NamedMap&lt;Long, Customer&gt; customers = coherence.getSession().getMap(cacheName); MapListener&lt;Long, Customer&gt; mapListener = new SimpleMapListener&lt;Long, Customer&gt;() .addEventHandler(System.out::println) .addEventHandler((e) -&gt; eventCount.incrementAndGet()) .versioned(); customers.addMapListener(mapListener); Logger.info(\"Added Map Listener, generating 3 events\"); // generate 3 events, insert, update and delete Customer customer = new Customer(100L, \"Customer 100\", \"Address\", Customer.GOLD, 5000); customers.put(customer.getId(), customer); customers.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customers.remove(100L); // wait until we receive first three events Eventually.assertDeferred(eventCount::get, is(3)); // cause a service distribution for PartitionedCache service to simulate disc Logger.info(\"Disconnecting client\"); causeServiceDisruption(customers); Logger.info(\"Remotely insert, update and delete a new customer\"); // do a remote invocation to insert, update and delete a customer. This is done // remotely via Oracle Bedrock as not to reconnect the client cluster.getAny().invoke(() -&gt; { NamedMap&lt;Long, Customer&gt; customerMap = CacheFactory.getCache(cacheName); Customer newCustomer = new Customer(100L, \"Customer 101\", \"Customer address\", Customer.SILVER, 100); customerMap.put(newCustomer.getId(), newCustomer); customerMap.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customerMap.remove(100L); return null; }); // Events should still only be 3 as client has not yet reconnected Eventually.assertDeferred(eventCount::get, is(3)); Logger.info(\"Issuing size to reconnect client\"); // issue an operation that will cause a service restart and listener to be re-registered customers.size(); // we should now see the 3 events we missed because we were disconnected Eventually.assertDeferred(eventCount::get, is(6)); } } Set system properties for the client Create a new SimpleMapListener Add an event handler to output the events received Add an event handler to increment the number of events received Indicate that this MapListener is versioned Add the MapListener to the NamedMap Simulate the client being disconnected by stopping the service for the NamedMap Generate 3 new events remotely on one of the members Issue an operator that will cause the client to restart and re-register the listener Assert that we now see the additional 3 events that were generated while the client was disconnected ",
            "title": "Review the Classes"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " You can run the test in one of three ways: Using your IDE to run DurableEventsTest class Using Maven via ./mvnw clean verify Using Gradle via ./gradlew test After initial cache server startup, you will see output similar to the following: Timestamps have been removed and output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=3): Added Map Listener, generating 3 events ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, partition=20, version=1} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, new value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=2} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=3} &lt;Info&gt; (thread=main, member=3): Disconnecting client &lt;Info&gt; (thread=main, member=3): Remotely insert, update and delete a new customer &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache left the cluster &lt;Info&gt; (thread=main, member=3): Issuing size to reconnect client &lt;Info&gt; (thread=main, member=3): Restarting NamedCache: customers &lt;Info&gt; (thread=main, member=3): Restarting Service: PartitionedCache &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache joined the cluster with senior service member 1 ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, partition=20, version=4} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, new value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=5} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=6} Adding the versioned SimpleMapListener Output of three events while the client is connected Message indicating we are disconnecting client Service for the client leaving as it is disconnected Restarting the cache and service due to size() request which will also automatically re-register the MapListener Client now receives the events it missed during disconnect ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " In this example you ran a test that demonstrated using Durable Events by: Starting 2 Cache Servers using Oracle Bedrock Creating and registering a version aware MapListener Inserting, updating and deleting cache entries Simulating the client being disconnected Issuing cache mutations remotely while the client is disconnected Reconnecting the client and validate that events generated while the client was disconnected are received ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Durable Events Documentation Client Events Develop Applications using Map Events ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/145-durable-events/README",
            "text": " Coherence provides the MapListener interface as described in Client Events , where clients can sign up for events from any Coherence NamedMap . With traditional client events, if a client disconnects for any reason and then reconnects and automatically re-registers a MapListener , it will miss any events that were sent during that disconnected time. Durable Events is a new (experimental) feature that allows clients to create a versioned listener which will allow a client, if disconnected, to receive events missed while they were in a disconnected state. As for standard `MapListener`s you are able to register for all events, events based upon a filter or events for a specific key. More advanced use cases for Durable Events include the ability to replay all events for a NamedMap . Please see Durable Events Documentation for more information on Durable Events. Durable events are an experimental feature only and should not be used in product as yet. Durable Events are not yet supported for Coherence*Extend clients. Table of Contents What You Will Build What You Need Building the Example Code Review the Classes Run the Examples Summary See Also What You Will Build In this example you will run a test that will demonstrate using Durable Events. The test does the following: Starts 2 Cache Servers using Oracle Bedrock Creates and registers a version aware MapListener Inserts, updates and deletes cache entries Simulates the client being disconnected Issues cache mutations remotely while the client is disconnected Reconnects the client and validate that events generated while the client was disconnected are received To enable Durable Events you must have the following system properties set for cache servers: Enable active persistence by using -Dcoherence.distributed.persistence.mode=active Set the directory to store Durable Events using -Dcoherence.distributed.persistence.events.dir=/my/events/dir Optionally set the directory to store active persistence using -Dcoherence.distributed.persistence.base.dir=/my/persistence/dir Register a versioned MapListener on a NamedMap If you do not set the directory to store active persistence the default directory coherence off the users home directory will be chosen. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can include the -DskipTests for Maven or -x test for Gradle, to skip the tests for now. Review the Classes Review the Customer class This example uses the Customer class which has the following fields: <markup lang=\"java\" >/** * Customer id. */ private long id; /** * Customers name. */ private String name; /** * Customers address. */ private String address; /** * Customers type, BRONZE, SILVER or GOLD. */ private String customerType; /** * Credit limit. */ private long creditLimit; Review how the 2 cache servers are started by Oracle Bedrock <markup lang=\"java\" >/** * Start a Coherence cluster with two cache servers using Oracle Bedrock. * * @throws IOException if any errors creating temporary directory */ @BeforeAll public static void startup() throws IOException { persistenceDir = FileHelper.createTempDir(); File eventsDir = new File(persistenceDir, \"events\"); CoherenceClusterBuilder builder = new CoherenceClusterBuilder() .with(SystemProperty.of(\"coherence.distributed.partitions\", 23), SystemProperty.of(\"coherence.distributed.persistence.mode\", \"active\"), SystemProperty.of(\"coherence.distributed.persistence.base.dir\", persistenceDir.getAbsolutePath()), SystemProperty.of(\"coherence.distributed.persistence.events.dir\", eventsDir.getAbsolutePath()), ClusterName.of(CLUSTER_NAME), RoleName.of(\"storage\"), DisplayName.of(\"storage\"), Multicast.ttl(0)) .include(2, CoherenceClusterMember.class, // testLogs, LocalStorage.enabled()); cluster = builder.build(); Eventually.assertDeferred(() -&gt; cluster.getClusterSize(), is(2)); for (CoherenceClusterMember member : cluster) { Eventually.assertDeferred(member::isReady, is(true)); } } Set the partition count to 23 to reduce the startup time Set active persistence mode Set the base directory to store persistence files Set the base directory to store persistence events Review the DurableEventsTest class <markup lang=\"java\" >/** * Runs a test to simulate a client registering a versioned {@link MapListener}, * being disconnected, reconnecting, and then receiving all the events that were * missed while the client was disconnected. */ @Test public void testDurableEvents() throws Exception { AtomicInteger eventCount = new AtomicInteger(); String cacheName = \"customers\"; System.setProperty(\"coherence.cluster\", CLUSTER_NAME); System.setProperty(\"coherence.role\", \"client\"); System.setProperty(\"coherence.distributed.localstorage\", \"false\"); try (Coherence coherence = Coherence.clusterMember()) { coherence.start().get(5, TimeUnit.MINUTES); NamedMap&lt;Long, Customer&gt; customers = coherence.getSession().getMap(cacheName); MapListener&lt;Long, Customer&gt; mapListener = new SimpleMapListener&lt;Long, Customer&gt;() .addEventHandler(System.out::println) .addEventHandler((e) -&gt; eventCount.incrementAndGet()) .versioned(); customers.addMapListener(mapListener); Logger.info(\"Added Map Listener, generating 3 events\"); // generate 3 events, insert, update and delete Customer customer = new Customer(100L, \"Customer 100\", \"Address\", Customer.GOLD, 5000); customers.put(customer.getId(), customer); customers.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customers.remove(100L); // wait until we receive first three events Eventually.assertDeferred(eventCount::get, is(3)); // cause a service distribution for PartitionedCache service to simulate disc Logger.info(\"Disconnecting client\"); causeServiceDisruption(customers); Logger.info(\"Remotely insert, update and delete a new customer\"); // do a remote invocation to insert, update and delete a customer. This is done // remotely via Oracle Bedrock as not to reconnect the client cluster.getAny().invoke(() -&gt; { NamedMap&lt;Long, Customer&gt; customerMap = CacheFactory.getCache(cacheName); Customer newCustomer = new Customer(100L, \"Customer 101\", \"Customer address\", Customer.SILVER, 100); customerMap.put(newCustomer.getId(), newCustomer); customerMap.invoke(100L, Processors.update(Customer::setAddress, \"New Address\")); customerMap.remove(100L); return null; }); // Events should still only be 3 as client has not yet reconnected Eventually.assertDeferred(eventCount::get, is(3)); Logger.info(\"Issuing size to reconnect client\"); // issue an operation that will cause a service restart and listener to be re-registered customers.size(); // we should now see the 3 events we missed because we were disconnected Eventually.assertDeferred(eventCount::get, is(6)); } } Set system properties for the client Create a new SimpleMapListener Add an event handler to output the events received Add an event handler to increment the number of events received Indicate that this MapListener is versioned Add the MapListener to the NamedMap Simulate the client being disconnected by stopping the service for the NamedMap Generate 3 new events remotely on one of the members Issue an operator that will cause the client to restart and re-register the listener Assert that we now see the additional 3 events that were generated while the client was disconnected Run the Examples You can run the test in one of three ways: Using your IDE to run DurableEventsTest class Using Maven via ./mvnw clean verify Using Gradle via ./gradlew test After initial cache server startup, you will see output similar to the following: Timestamps have been removed and output has been formatted for easier reading. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=3): Added Map Listener, generating 3 events ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, partition=20, version=1} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 100', address='Address', customerType='GOLD', balance=5000}, new value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=2} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 100', address='New Address', customerType='GOLD', balance=5000}, partition=20, version=3} &lt;Info&gt; (thread=main, member=3): Disconnecting client &lt;Info&gt; (thread=main, member=3): Remotely insert, update and delete a new customer &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache left the cluster &lt;Info&gt; (thread=main, member=3): Issuing size to reconnect client &lt;Info&gt; (thread=main, member=3): Restarting NamedCache: customers &lt;Info&gt; (thread=main, member=3): Restarting Service: PartitionedCache &lt;Info&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache joined the cluster with senior service member 1 ConverterCollections$ConverterMapEvent{SafeNamedCache inserted: key=100, value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, partition=20, version=4} ConverterCollections$ConverterMapEvent{SafeNamedCache updated: key=100, old value=Customer{id=100, name='Customer 101', address='Customer address', customerType='SILVER', balance=100}, new value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=5} ConverterCollections$ConverterMapEvent{SafeNamedCache deleted: key=100, value=Customer{id=100, name='Customer 101', address='New Address', customerType='SILVER', balance=100}, partition=20, version=6} Adding the versioned SimpleMapListener Output of three events while the client is connected Message indicating we are disconnecting client Service for the client leaving as it is disconnected Restarting the cache and service due to size() request which will also automatically re-register the MapListener Client now receives the events it missed during disconnect Summary In this example you ran a test that demonstrated using Durable Events by: Starting 2 Cache Servers using Oracle Bedrock Creating and registering a version aware MapListener Inserting, updating and deleting cache entries Simulating the client being disconnected Issuing cache mutations remotely while the client is disconnected Reconnecting the client and validate that events generated while the client was disconnected are received See Also Durable Events Documentation Client Events Develop Applications using Map Events ",
            "title": "Durable Events"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units and the back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays CacheMBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays CacheMBean metrics for the front cache to show cache pruning happening Displays StorageManagerMBean metrics to show listener registrations There are two tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { final int MAX = 100; // Create the Coherence instance from the configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.create(\"near-cache-config.xml\")) .build(); Coherence coherence = Coherence.clusterMember(cfg); coherence.start().join(); // retrieve a session Session session = coherence.getSession(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up Base.sleep(3000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 3 seconds to ensure JMX stats are up to date Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics ",
            "title": "Review the Example Code"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Run the examples using one of the test classes below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured eviction policies Explored MBeans related to near caching ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/130-near-caching/README",
            "text": " This guide walks you through how to use near caching within Coherence by providing various examples and configurations that showcase the different features available. A near cache is a hybrid cache; it typically fronts a distributed cache or a remote cache with a local cache. Near cache invalidates front cache entries, using a configured invalidation strategy, and provides excellent performance and synchronization. Near cache backed by a partitioned cache offers zero-millisecond local access for repeat data access, while enabling concurrency and ensuring coherency and fail over, effectively combining the best attributes of replicated and partitioned caches. See the Coherence Documentation for detailed information on near caches. Table of Contents What You Will Build What You Need Review the Example Code Review the Tests Run the Examples Summary See Also What You Will Build In this example you will run a number of tests and that show the following features of near caches: Configuring near caches Setting near cache size limits Changing the invalidation strategy Configuring eviction policies Exploring MBeans related to near caching What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Running the Examples This example can be run directly in your IDE, but you can also run 1 or more cache servers and then run the example class. Running Cache Servers <markup lang=\"bash\" >./mvnw exec:exec -P server or <markup lang=\"bash\" >./gradlew runServer -x test Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Example Code The example code comprises the SimpleNearCachingExample class, which uses the near-cache-config.xml configuration to define a near cache. The front cache is configured with 100 entries as the high-units and the back cache is a distributed cache. When a near cache has reached it&#8217;s high-units limit, it prunes itself back to the value of the low-units element (or not less than 80% of high-units if not set). The entries chosen are done so according to the configured eviction-policy . There are a number of eviction policies that can be used including: Least Recently Used (LRU), Least Frequently Used (LFU), Hybrid or custom. The test class carries out the following steps: Inserts 100 entries into the cache Issues a get on each of the 100 entries and displays the time taken (populates the near cache&#8217;s front cache) Displays CacheMBean metrics for the front cache Carries out a second get on the 100 entries and notes the difference in the time to retrieve the entries Inserts an additional 10 entries then issue gets for those entries, which will cause cache pruning Displays CacheMBean metrics for the front cache to show cache pruning happening Displays StorageManagerMBean metrics to show listener registrations There are two tests that exercise the above SimpleNearCachingExample class and using different caches as well as different invalidation strategies set via a system property. They are described in more detail in the following sections. com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Review the Cache Config <markup lang=\"java\" >&lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;size-cache-*&lt;/cache-name&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-name&gt;front-limit-entries&lt;/param-name&gt; &lt;param-value&gt;100&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;near-scheme&gt; &lt;scheme-name&gt;near-scheme&lt;/scheme-name&gt; &lt;front-scheme&gt; &lt;local-scheme&gt; &lt;eviction-policy&gt;LRU&lt;/eviction-policy&gt; &lt;high-units&gt;{front-limit-entries 10}&lt;/high-units&gt; &lt;/local-scheme&gt; &lt;/front-scheme&gt; &lt;back-scheme&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;sample-distributed&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCache&lt;/service-name&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;/distributed-scheme&gt; &lt;/back-scheme&gt; &lt;invalidation-strategy system-property=\"test.invalidation.strategy\"&gt;all&lt;/invalidation-strategy&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/near-scheme&gt; Define cache mapping for caches matching size-cache-* to the near-scheme using macros to set the front limit to 100 Define an eviction policy to apply when high-units are reached Define front scheme high-units using the macro and defaulting to 10 if not set Define back scheme as standard distributed scheme System property to set the invalidation strategy for each test Review the SimpleNearCachingExample class Constructor <markup lang=\"java\" >/** * Construct the example. * * @param cacheName cache name * @param invalidationStrategy invalidation strategy to use */ public SimpleNearCachingExample(String cacheName, String invalidationStrategy) { this.cacheName = cacheName; if (invalidationStrategy != null) { System.setProperty(\"test.invalidation.strategy\", invalidationStrategy); } System.setProperty(\"coherence.management.refresh.expiry\", \"1s\"); System.setProperty(\"coherence.management\", \"all\"); } Main Example The runExample() method contains the code that exercises the near cache. A loop in the test runs twice to show the difference second time around with the near cache populated. <markup lang=\"java\" >/** * Run the example. */ public void runExample() throws Exception { final int MAX = 100; // Create the Coherence instance from the configuration CoherenceConfiguration cfg = CoherenceConfiguration.builder() .withSession(SessionConfiguration.create(\"near-cache-config.xml\")) .build(); Coherence coherence = Coherence.clusterMember(cfg); coherence.start().join(); // retrieve a session Session session = coherence.getSession(); NamedMap&lt;Integer, String&gt; map = session.getMap(cacheName); map.clear(); Logger.info(\"Running test with cache \" + cacheName); // sleep so we don't get distribution messages intertwined with test output Base.sleep(5000L); // fill the map with MAX values putValues(map, 0, MAX); // execute two times to see the difference in access times and MBeans once the // near cache is populated on the first iteration for (int j = 1; j &lt;= 2; j++) { // issue MAX get operations and get the total time taken long start = System.nanoTime(); getValues(map, 0, MAX); long duration = (System.nanoTime() - start); Logger.info(\"Iteration #\" + j + \" Total time for gets \" + String.format(\"%.3f\", duration / 1_000_000f) + \"ms\"); // Wait for some time for the JMX stats to catch up Base.sleep(3000L); logJMXNearCacheStats(); } // issue 10 more puts putValues(map, MAX, 10); // issue 10 more gets and the high-units will be hit and cache pruning will happen when using size cache getValues(map, MAX, 10); Logger.info(\"After extra 10 values put and get\"); logJMXNearCacheStats(); logJMXStorageStats(); } Populate the cache with 100 entries Issue a get for each of the 100 entries Sleep for 3 seconds to ensure JMX stats are up to date Display the Cache MBean front cache metrics Issue 10 more puts and gets which will cause the front cache to be pruned Display the Cache MBean front cache metrics and StorageManager metrics Review the Tests The main SimpleNearCachingExample class is exercised by running the following tests : SimpleNearCachingExampleALLTest - uses all invalidation strategy and high units of 100 SimpleNearCachingExamplePRESENTTest - uses present invalidation strategy and high units of 100 There are a number of invalidation strategies, described here , but we will utilize the following for the tests above: all - This strategy instructs a near cache to listen to all back cache events. This strategy is optimal for read-heavy tiered access patterns where there is significant overlap between the different instances of front caches. present - This strategy instructs a near cache to listen to the back cache events related only to the items currently present in the front cache. This strategy works best when each instance of a front cache contains distinct subset of data relative to the other front cache instances (for example, sticky data access patterns). The default strategy is auto , which is identical to the present strategy. Review the SimpleNearCachingExampleALLTest <markup lang=\"java\" >public class SimpleNearCachingExampleALLTest { @Test public void testNearCacheAll() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-all\", \"all\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-all , which matches the size limited near cache and invalidation strategy of all . Review the SimpleNearCachingExamplePRESENTTest <markup lang=\"java\" >public class SimpleNearCachingExamplePRESENTTest { @Test public void testNearCachePresent() throws Exception { System.setProperty(\"coherence.log.level\", \"3\"); SimpleNearCachingExample example = new SimpleNearCachingExample(\"size-cache-present\", \"present\"); example.runExample(); Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } } This test runs with a cache called size-cache-present , which matches the size limited near cache and invalidation strategy of `present. Run the Examples Run the examples using one of the test classes below: Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.nearcaching.SimpleNearCachingExampleALLTest or com.oracle.coherence.guides.nearcaching.SimpleNearCachingExamplePRESENTTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test If you run one or more cache servers as described earlier, you will see additional StorageManager MBean output below. SimpleNearCachingExampleALLTest Output This test will generate output similar to the following: (timestamps have been removed from output) <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-all &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.094ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.143ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.37 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-all,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=109 &lt;Info&gt; (thread=main, member=1): Name: Size, value=90 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5190476190476191 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.36633663366336633 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-all,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=1 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Iteration #1 for gets takes 38.094ms which includes the time to populate the front cache The Cache MBean object name for the front cache and various metrics Iteration #2 for gets takes only 0.143ms which is considerably quicker due to the entries being in the front cache The Hit Probability is 0.5 or 50% as 100 out of 200 entries were read from the front cache After the extra puts and gets, we can see that the cache was pruned the size of the front cache is now 90 Number of prune operations Because we are using the all invalidation strategy there is only 1 listener registered for all the entries SimpleNearCachingExamplePRESENTTest Output The output is similar to the above output, but you will notice that the number of listeners registered are higher as we are using the Present strategy that will register a listener for each entry in the front of the near cache. <markup lang=\"bash\" >&lt;Info&gt; (thread=main, member=1): Running test with cache size-cache-present &lt;Info&gt; (thread=main, member=1): Iteration #1 Total time for gets 38.474ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=100 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=0 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.0 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): Iteration #2 Total time for gets 0.236ms &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=200 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=100 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=100 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.5 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.39 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=0 &lt;Info&gt; (thread=main, member=1): After extra 10 values put and get &lt;Info&gt; (thread=main, member=1): Coherence:type=Cache,service=DistributedCache,name=size-cache-present,nodeId=1,tier=front,loader=414493378 &lt;Info&gt; (thread=main, member=1): Name: TotalGets, value=210 &lt;Info&gt; (thread=main, member=1): Name: TotalPuts, value=110 &lt;Info&gt; (thread=main, member=1): Name: CacheHits, value=100 &lt;Info&gt; (thread=main, member=1): Name: Size, value=89 &lt;Info&gt; (thread=main, member=1): Name: HitProbability, value=0.47619047619047616 &lt;Info&gt; (thread=main, member=1): Name: AverageMissMillis, value=0.4818181818181818 &lt;Info&gt; (thread=main, member=1): Name: CachePrunes, value=1 &lt;Info&gt; (thread=main, member=1): Coherence:type=StorageManager,service=DistributedCache,cache=size-cache-present,nodeId=1 &lt;Info&gt; (thread=main, member=1): Name: ListenerRegistrations, value=110 &lt;Info&gt; (thread=main, member=1): Name: InsertCount, value=110 Number of listener registrations Summary In this example you have seen how to use near caching within Coherence by covering the following: Configured near caches Set near cache size limits Changed the invalidation strategy Configured eviction policies Explored MBeans related to near caching See Also Understanding Near Caches Defining Near Cache Schemes Near Cache Invalidation Strategies Understanding Local Caches Near Cache local-scheme Configuration Near Cache and Cluster-node Affinity Concurrent Near Cache Misses on a Specific Hot Key ",
            "title": "Near Caching"
        },
        {
            "location": "/docs/topics/05_persistence",
            "text": " Coherence topics store data in caches, and as such the Coherence persistence feature can be used to persist that data to disc. As well as the actual topic message data, the metadata associated with a topic will also be stored in caches. This data tracks information such as the head and tail of a topic, it also tracks subscriber groups and subscriber lifetimes. When persistence has been enabled, the metadata caches will also be persisted. Both active and on-demand persistence will work with topics, and in fact, a recommendation is to use active persistence, as data loss can severely impact topic functionality. Warning Care must be taken if recovering cache data from a persistence snapshot. A snapshot of topics caches will also contain metadata for the topic, this includes the commits, heads and tails for subscribers and subscriber groups. When recovering a snapshot for a Coherence topics cache service, all the metadata will also be recovered. This will reset the state of subscribers and subscriber groups, as well as topic heads, and tails, which may affect publishers. When recovering a snapshot it is important that no subscribers and publishers have been connected. Publishers and subscribers have their own state and on recovery this state will be out of date with the actual topic state in the topic metadata. This can cause subscribers to read the wrong data, or a publisher to attempt to publish to the wrong position. This may not immediately be apparent as it may not cause an exception to be thrown so data could be corrupted. ",
            "title": "Topics and Persistence"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Note Topics was first introduced in 14.1.1.0 and was considerable enhanced to improve message delivery guarantees in CE 21.06. Whilst the changes made in 21.06 are API compatible, the underlying data-structures and entry processors used are not backwards compatible, meaning that a rolling upgrade of a cluster using Topics from 14.1.1.0 (or CE version prior to 21.06) upgrading to CE 21.06 will not work. ",
            "title": "Version Compatibility"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " The Topics API enables the building of data pipelines between loosely coupled publishers and consumers. One or more publishers publish a stream of values to a topic. One or more subscribers consume the stream of values from a topic. The topic values are spread evenly across all Oracle Coherence data servers, enabling high throughput processing in a distributed and fault tolerant manner. Version Compatibility Note Topics was first introduced in 14.1.1.0 and was considerable enhanced to improve message delivery guarantees in CE 21.06. Whilst the changes made in 21.06 are API compatible, the underlying data-structures and entry processors used are not backwards compatible, meaning that a rolling upgrade of a cluster using Topics from 14.1.1.0 (or CE version prior to 21.06) upgrading to CE 21.06 will not work. ",
            "title": "Overview"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " In order to scale publishers and subscribers whilst still guaranteeing message ordering, Coherence Topics introduces the concept of channels. A channel is similar in idea to how Coherence partitions data in distributed NamedMap but to avoid confusion the name partition was not reused. Channels are an important part of the operation of both publishers and subscribers. A publisher is configured to control what ordering guarantees exist for the messages it publishes when they are received by subscribers. This is achieved by publishing messages to a specific channel. All messages published to a channel will be received by subscribers in the same order. Messages published to different channels may be interleaved as they are received by subscribers. The number of channels that a topic has allows publishers to scale better as they avoid contention that may occur with many publishers publishing to a single channel. Message consumption can be scaled because multiple subscribers (in a group) will subscribe to different channels, so scaling up receiving of messages, whilst maintaining order. The channel count for a topic is configurable, ideally a small prime (the default is 17). There are pros and cons with very small or very large channel counts, depending on the application use case and what sort of scaling or ordering guarantees it requires. ",
            "title": "Channels"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Every element published to a topic has a position. A position is an opaque data structure used by the underlying NamedTopic implementation to track the position of an element in a channel and maintain ordering of messages. Positions are then used by subscribers to track the elements that they have received and when committing a position to determine which preceding elements are also committed and to then recover to the correct position in the topic when subscribers reconnect or recover from failure. Whilst a position data structure is opaque, they are serializable, meaning that they can be stored into a separate data store by application code that wants to manually track message element processing. The combination of channel and position should be unique for each message element published and received. ",
            "title": "Position"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " A NamedTopic is the name of the Coherence data structure that stores data as an ordered stream of messages. Generally most application code does not need to interact directly with a NamedTopic , but instead with a Publisher or Subscriber to either publish to or subscribe to a NamedTopic . See: Configuring NamedTopics ",
            "title": "NamedTopic"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Some core concepts of Coherence Topics are described below. Channels In order to scale publishers and subscribers whilst still guaranteeing message ordering, Coherence Topics introduces the concept of channels. A channel is similar in idea to how Coherence partitions data in distributed NamedMap but to avoid confusion the name partition was not reused. Channels are an important part of the operation of both publishers and subscribers. A publisher is configured to control what ordering guarantees exist for the messages it publishes when they are received by subscribers. This is achieved by publishing messages to a specific channel. All messages published to a channel will be received by subscribers in the same order. Messages published to different channels may be interleaved as they are received by subscribers. The number of channels that a topic has allows publishers to scale better as they avoid contention that may occur with many publishers publishing to a single channel. Message consumption can be scaled because multiple subscribers (in a group) will subscribe to different channels, so scaling up receiving of messages, whilst maintaining order. The channel count for a topic is configurable, ideally a small prime (the default is 17). There are pros and cons with very small or very large channel counts, depending on the application use case and what sort of scaling or ordering guarantees it requires. Position Every element published to a topic has a position. A position is an opaque data structure used by the underlying NamedTopic implementation to track the position of an element in a channel and maintain ordering of messages. Positions are then used by subscribers to track the elements that they have received and when committing a position to determine which preceding elements are also committed and to then recover to the correct position in the topic when subscribers reconnect or recover from failure. Whilst a position data structure is opaque, they are serializable, meaning that they can be stored into a separate data store by application code that wants to manually track message element processing. The combination of channel and position should be unique for each message element published and received. NamedTopic A NamedTopic is the name of the Coherence data structure that stores data as an ordered stream of messages. Generally most application code does not need to interact directly with a NamedTopic , but instead with a Publisher or Subscriber to either publish to or subscribe to a NamedTopic . See: Configuring NamedTopics ",
            "title": "Concepts"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " A Publisher publishes messages to topic. Each Publisher is created with a configurable ordering option that determines the ordering guarantees for how those messages are then received by Subscribers . The topic values are spread evenly across all Coherence storage enabled cluster members, enabling high throughput processing in a distributed and fault tolerant manner. Multiple publishers can be created to publish messages to the same topic. See: Publishers ",
            "title": "Publishers"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " A Subscriber subscribes to a NamedTopic and receives messages from that topic. The subscriber mechanism is based on polling, as Subscriber calls its receive method to fetch the next message, messages are not pushed to a subscriber. The Subscriber API is asynchronous so applications can receive and process messages in a non-blocking \"push-like\" fashion using the CompletableFuture API. There are two types of Subscriber , anonymous subscribers and group subscribers. An anonymous subscriber connects to a topic and receives all messages published to that topic. Anonymous subscribers are not durable, if they are closed (or fail) then when they reconnect they are effectively seen as new subscribers and will restart processing messages from the topic&#8217;s tail (or from the head if the topic is configured to retain messages). A group subscriber is part of a group of one or more subscribers that all belong to the same named subscriber group. In a subscriber group each message is only delivered to one of the members of the group. This allows message processing to be scaled up by using multiple subscribers that will process messages from the same topic, whilst maintaining the publishers ordering. Message delivery in a subscriber group is controlled by assigning ownership of the channels of the topic to each subscriber in the group, so that each subscriber is polling a sub-set of channels. As subscribers in a group are created, or closed, or fail, their channel ownership is reallocated across the remaining subscribers. When new subscribers are created in a group, channels from the existing subscribers are redistributed to the new subscriber. This all happens automatically to try to maintain an evenly balanced distribution of channels to group subscribers. Obviously if more subscribers are created in a group than there are channels in the topic then some of those subscribers cannot be allocated a channel as there are not enough channels to go around. A subscriber group is durable, if all the subscribers in a group are closed, next time a subscriber in the group re-subscribes it will start processing messages from next message after the last committed position. See: Subscribers ",
            "title": "Subscribers"
        },
        {
            "location": "/docs/topics/01_introduction",
            "text": " Coherence Topics introduces publish and subscribe messaging, also often referred to as streaming, functionality in Oracle Coherence. Overview The Topics API enables the building of data pipelines between loosely coupled publishers and consumers. One or more publishers publish a stream of values to a topic. One or more subscribers consume the stream of values from a topic. The topic values are spread evenly across all Oracle Coherence data servers, enabling high throughput processing in a distributed and fault tolerant manner. Version Compatibility Note Topics was first introduced in 14.1.1.0 and was considerable enhanced to improve message delivery guarantees in CE 21.06. Whilst the changes made in 21.06 are API compatible, the underlying data-structures and entry processors used are not backwards compatible, meaning that a rolling upgrade of a cluster using Topics from 14.1.1.0 (or CE version prior to 21.06) upgrading to CE 21.06 will not work. Concepts Some core concepts of Coherence Topics are described below. Channels In order to scale publishers and subscribers whilst still guaranteeing message ordering, Coherence Topics introduces the concept of channels. A channel is similar in idea to how Coherence partitions data in distributed NamedMap but to avoid confusion the name partition was not reused. Channels are an important part of the operation of both publishers and subscribers. A publisher is configured to control what ordering guarantees exist for the messages it publishes when they are received by subscribers. This is achieved by publishing messages to a specific channel. All messages published to a channel will be received by subscribers in the same order. Messages published to different channels may be interleaved as they are received by subscribers. The number of channels that a topic has allows publishers to scale better as they avoid contention that may occur with many publishers publishing to a single channel. Message consumption can be scaled because multiple subscribers (in a group) will subscribe to different channels, so scaling up receiving of messages, whilst maintaining order. The channel count for a topic is configurable, ideally a small prime (the default is 17). There are pros and cons with very small or very large channel counts, depending on the application use case and what sort of scaling or ordering guarantees it requires. Position Every element published to a topic has a position. A position is an opaque data structure used by the underlying NamedTopic implementation to track the position of an element in a channel and maintain ordering of messages. Positions are then used by subscribers to track the elements that they have received and when committing a position to determine which preceding elements are also committed and to then recover to the correct position in the topic when subscribers reconnect or recover from failure. Whilst a position data structure is opaque, they are serializable, meaning that they can be stored into a separate data store by application code that wants to manually track message element processing. The combination of channel and position should be unique for each message element published and received. NamedTopic A NamedTopic is the name of the Coherence data structure that stores data as an ordered stream of messages. Generally most application code does not need to interact directly with a NamedTopic , but instead with a Publisher or Subscriber to either publish to or subscribe to a NamedTopic . See: Configuring NamedTopics Publishers A Publisher publishes messages to topic. Each Publisher is created with a configurable ordering option that determines the ordering guarantees for how those messages are then received by Subscribers . The topic values are spread evenly across all Coherence storage enabled cluster members, enabling high throughput processing in a distributed and fault tolerant manner. Multiple publishers can be created to publish messages to the same topic. See: Publishers Subscribers A Subscriber subscribes to a NamedTopic and receives messages from that topic. The subscriber mechanism is based on polling, as Subscriber calls its receive method to fetch the next message, messages are not pushed to a subscriber. The Subscriber API is asynchronous so applications can receive and process messages in a non-blocking \"push-like\" fashion using the CompletableFuture API. There are two types of Subscriber , anonymous subscribers and group subscribers. An anonymous subscriber connects to a topic and receives all messages published to that topic. Anonymous subscribers are not durable, if they are closed (or fail) then when they reconnect they are effectively seen as new subscribers and will restart processing messages from the topic&#8217;s tail (or from the head if the topic is configured to retain messages). A group subscriber is part of a group of one or more subscribers that all belong to the same named subscriber group. In a subscriber group each message is only delivered to one of the members of the group. This allows message processing to be scaled up by using multiple subscribers that will process messages from the same topic, whilst maintaining the publishers ordering. Message delivery in a subscriber group is controlled by assigning ownership of the channels of the topic to each subscriber in the group, so that each subscriber is polling a sub-set of channels. As subscribers in a group are created, or closed, or fail, their channel ownership is reallocated across the remaining subscribers. When new subscribers are created in a group, channels from the existing subscribers are redistributed to the new subscriber. This all happens automatically to try to maintain an evenly balanced distribution of channels to group subscribers. Obviously if more subscribers are created in a group than there are channels in the topic then some of those subscribers cannot be allocated a channel as there are not enough channels to go around. A subscriber group is durable, if all the subscribers in a group are closed, next time a subscriber in the group re-subscribes it will start processing messages from next message after the last committed position. See: Subscribers ",
            "title": "Introduction to Coherence Topics"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Simply Starting Coherence - Running Coherence as the main class. The Coherence Instance - Accessing and using the bootstrapped Coherence instance Ensure Coherence is Started - Obtaining a fully running Coherence instance Coherence Sessions - Obtaining Coherence Session instances and other Coherence resources Application Initialization - Initializing application code without needing a custom main class Bootstrap Coherence - Starting Coherence from Application Code&gt;&gt; Simple Cluster Member - Start a simple cluster member Configured Cluster Member - Configure and start a simple cluster member ",
            "title": "Contents"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Whether you are running a Coherence cluster member or client, you&#8217;ll need to configure and bootstrap Coherence. Coherence does not actually need any configuring or bootstrapping, you could just do something like CacheFactory.getCache(\"foo\"); , but when there is an alternative, static method calls to get Coherence resources are poor coding practice (especially when it comes to unit testing with mocks and stubs). Coherence CE v20.12 introduced a new bootstrap API for Coherence, which this guide is going to cover. Not only does the bootstrap API make it simpler to start Coherence, it makes some other uses cases simpler, for example where a client application needs to connect to multiple clusters. A number of the integrations between Coherence and application frameworks, such as Coherence Spring , Coherence Micronaut , Coherence CDI and Helidon , use the bootstrap API under the covers to initialize Coherence when using those frameworks. When using these types of \"DI\" frameworks, Coherence and Session instances and other Coherence resources can just be injected into application code without even needing to directly access the bootstrap API. Contents Simply Starting Coherence - Running Coherence as the main class. The Coherence Instance - Accessing and using the bootstrapped Coherence instance Ensure Coherence is Started - Obtaining a fully running Coherence instance Coherence Sessions - Obtaining Coherence Session instances and other Coherence resources Application Initialization - Initializing application code without needing a custom main class Bootstrap Coherence - Starting Coherence from Application Code&gt;&gt; Simple Cluster Member - Start a simple cluster member Configured Cluster Member - Configure and start a simple cluster member ",
            "title": "Bootstrap Coherence"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " This guide will look at some ways to bootstrap a Coherence application. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " A Coherence application is either a cluster member, or it is a client. Historically a client would be a Coherence*Extend client, but more recently Coherence has also introduced a gRPC client. Prior to CE v20.12, applications typically used Coherence in a couple of ways; either cluster members that started by running DefaultCacheServer , or by running a custom main class and obtaining Coherence resources directly from a Session or ConfigurableCacheFactory instance - possibly using static methods on com.tangosol.net.CacheFactory . By far the majority of applications only had a single ConfigurableCacheFactory instance, but occasionally an application would add more (for example an Extend client connecting to multiple cluster). Adding of additional ConfigurableCacheFactory required custom start-up code and management code. In an effort to make it possible to build more modular applications with multiple ConfigurableCacheFactory or Session instances a new bootstrap API was added. ",
            "title": "A Brief History"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " By default, Coherence will run as a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. For example, when using the default coherence-cache-config.xml file from coherence.jar` it is possible to run `Coherence as an Extend client by setting the coherence.client system property (or COHERENCE_CLIENT environment variable) to a value of remote . <markup lang=\"bash\" >java -cp coherence.jar -Dcoherence.client=remote com.tangosol.net.Coherence ",
            "title": "Running Coherence as an Extend Client"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The Coherence class is the main entry point into a Coherence application. A Coherence server can be started by simply running the Coherence.main() method. From Coherence CE v22.06, this is the default way that Coherence starts using java -jar coherence.jar . An important point when using the Coherence class to start Coherence is that this will automatically include starting some of the additional Coherence extensions if they are on the class path, or module path. For example, starting the Coherence health check http endpoints, or if the Coherence Concurrent module is on the class path, its services will automatically be started. The same applies to the Coherence gRPC server, Coherence metrics and Coherence REST management. <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.net.Coherence <markup lang=\"bash\" >java -jar coherence.jar Or with Java modules <markup lang=\"bash\" >java -p coherence.jar -m com.oracle.coherence Functionally this is almost identical to the old way of running DefaultCacheServer , but will now use the new bootstrap API to configure and start Coherence. When run in this way Coherence will use the default configuration file coherence-cache-config.xml , either from coherence.jar or elsewhere on the classpath. The name of this configuration file can be overridden as normal with the coherence.cacheconfig system property. Running the Coherence class, or using the bootstrap API, will also start various system services, such as the health check http endpoints. Running Coherence as an Extend Client By default, Coherence will run as a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. For example, when using the default coherence-cache-config.xml file from coherence.jar` it is possible to run `Coherence as an Extend client by setting the coherence.client system property (or COHERENCE_CLIENT environment variable) to a value of remote . <markup lang=\"bash\" >java -cp coherence.jar -Dcoherence.client=remote com.tangosol.net.Coherence ",
            "title": "Starting Coherence"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Sometimes, application code may need to ensure Coherence has fully started before running. A Coherence instance has a whenStarted() method that returns a CompletableFuture that will be completed when the Coherence instance has finished starting. The example below obtains the default Coherence instance and waits up to five minuts for the instance to be running. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance() .whenStarted() .get(5, TimeUnit.MINUTES); ",
            "title": "Ensure Coherence is Started"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Each Coherence instance will be running one or more uniquely named Session instances, depending on how it was configured. By running Coherence.main() the default Coherence instance will be running the default Session . A Session can be obtained from a Coherence instance using a number of methods. The example below obtains the default Coherence Session from the default Coherence instance. This method would be used if Coherence has been started using the default Coherence.main() method. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); A Session can also be obtained using its name. The example below obtains the Session named \"foo\". <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(\"foo\"); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); It is also possible to use the static Coherence.findSession() method to find a Session by name across all configured Coherence instances. This method returns an optional containing the Session or empty if no Session exists with the requested name. <markup lang=\"java\" > Optional&lt;Session&gt; optional = Coherence.findSession(\"foo\"); if (optional.isPresent()) { Session session = optional.get(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); } ",
            "title": "Obtain a Coherence Session"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Once a Coherence instance has been started, using either the Coherence.main() method, or one of the other ways described below, application code can obtain the running Coherence instance and obtain a Coherence Session which can then be used to access Coherence resources such as NamedMap , NamedCache , NamedTopic etc. More than one Coherence instance can be running simultaneously (but in the case of a cluster member, all these instances will be a single cluster member, they are not able to be parts of separate clusters). Each Coherence instance has a unique name and can be accessed by name. If Coherence has been started using Coherence.main() there will be a single instance of Coherence with the default name. The simplest way to access the default Coherence instance is using the static accessor. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Coherence instances can also be obtained by name, the default instance&#8217;s name can be accessed using the static field Coherence.DEFAULT_NAME : <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(Coherence.DEFAULT_NAME); Ensure Coherence is Started Sometimes, application code may need to ensure Coherence has fully started before running. A Coherence instance has a whenStarted() method that returns a CompletableFuture that will be completed when the Coherence instance has finished starting. The example below obtains the default Coherence instance and waits up to five minuts for the instance to be running. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance() .whenStarted() .get(5, TimeUnit.MINUTES); Obtain a Coherence Session Each Coherence instance will be running one or more uniquely named Session instances, depending on how it was configured. By running Coherence.main() the default Coherence instance will be running the default Session . A Session can be obtained from a Coherence instance using a number of methods. The example below obtains the default Coherence Session from the default Coherence instance. This method would be used if Coherence has been started using the default Coherence.main() method. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); A Session can also be obtained using its name. The example below obtains the Session named \"foo\". <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Session session = coherence.getSession(\"foo\"); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); It is also possible to use the static Coherence.findSession() method to find a Session by name across all configured Coherence instances. This method returns an optional containing the Session or empty if no Session exists with the requested name. <markup lang=\"java\" > Optional&lt;Session&gt; optional = Coherence.findSession(\"foo\"); if (optional.isPresent()) { Session session = optional.get(); NamedMap&lt;String, String&gt; map = session.getMap(\"test\"); } ",
            "title": "Using a Coherence Instance"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Sometimes an application needs to perform some initialization when it starts up. Before the new bootstrap API existed, this was a common reason for applications having to add a custom main class. The Coherence class has an inner interface LifecycleListener that application code can implement to be notified of Coherence start-up and shutdown events. Instances of LifecycleListener are automatically discovered by Coherence at runtime using the Java ServiceLoader , which means that an applications can be initialised without needing a custom main class, but instead by just implementing a LifecycleListener . This is particularly useful where an application is made up of modules that may or may not be on the class path or module path at runtime. A module just needs to implement a Coherence LifecycleListener as a Java service and whenever it is on the class path it will be initialized. For example, an application that needs to start a web-server could implement LifecycleListener as shown below. The STARTED event type is fired after a Coherence instance is started, the STOPPING event type is fired before a Coherence instance is stopped. <markup lang=\"java\" >import com.tangosol.net.Coherence; import com.tangosol.net.events.CoherenceLifecycleEvent; public class WebServerController implements Coherence.LifecycleListener { @Override public void onEvent(CoherenceLifecycleEvent event) { switch (event.getType()) { case STARTED: server.start(); break; case STOPPING: server.stop(); break; } } private final HttpServer server = new HttpServer(); } The event also contains the Coherence instance that raised the event, so this could then be used to obtain a Session and other Coherence resources that are needed as part of the application initialisation. Adding the WebServerController class above to a META-INF/services file or module-info file will make it discoverable by Coherence. <markup lang=\"java\" title=\"META_INF/services/com.tangosol.net.Coherence$LifecycleListener\" >com.oracle.coherence.guides.bootstrap.WebServerController; <markup lang=\"java\" title=\"module-info.java\" >open module com.oracle.coherence.guides.bootstrap { requires com.oracle.coherence; exports com.oracle.coherence.guides.bootstrap; provides com.tangosol.net.Coherence.LifecycleListener with com.oracle.coherence.guides.bootstrap.WebServerController; } ",
            "title": "Initialize Application Code"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The simplest way to start Coherence as a cluster member in application code is shown below: <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember(); coherence.start(); The start() method returns a CompletableFuture so application code that needs to wit for start-up to complete can use the future for this purpose. The example below ensures Coherence is started as a cluster member (waiting a maximum of five minutes) before proceeding. <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember() .start() .get(5, TimeUnit.MINUTES); Running Coherence in this way will create a single Session using the default cache configuration file (or another file specified using the -Dcoherence.cacheconfig system property). By default, this will be a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. ",
            "title": "Run a Simple Cluster Member"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " The bootstrap API allows the Coherence instance to be configured before starting, for example adding one or more session configurations. In the example below, a Coherence cluster member instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.clusterMember(config) .start() .join(); There are various other methods on the configuration builders, for example configuring parameters to pass into the cache configuration files, configuring interceptors, etc. ",
            "title": "Configure a Cluster Member"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " If the application code will is an Extend client, then Coherence can be bootstrapped in client mode. The example below starts Coherence as an Extend client, which will use the Coherence NameService to locate the cluster and look up the Extend Proxy to connect to. This works by configuring the client to have the same cluster name and same well-known address list (or multicast settings) as the cluster being connected to, either using System properties or environment variables. <markup lang=\"java\" > Coherence coherence = Coherence.client(); coherence.start(); Alternatively, instead of using the NameService a fixed address and port can be configured for the Extend client to use. If the System property coherence.extend.address is set to the IP address or host name of the Extend proxy, and coherence.extend.port is set to the port of the Extend proxy (or the corresponding environment variables COHERENCE_EXTEND_ADDRESS and COHERENCE_EXTEND_PORT ) then Coherence can be bootstrapped as shown below. <markup lang=\"java\" > Coherence coherence = Coherence.fixedClient(); coherence.start(); Coherence will then be bootstrapped as an Extend client and connect to the proxy on the configured address and port. Note The code snippets above work with the default cache configuration file. The default cache configuration file in the coherence.jar is configured with certain injectable property values, which are configured by the bootstrap API when running as a client. Using other cache configuration files that are not configured with these properties would mean \"client\" mode is effectively ignored. The Coherence instance will still be started and will run correctly, the client mode properties will just have no affect. ",
            "title": "Run Coherence as an Extend Client"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " Coherence can be configured in client mode in code. In the example below, a Coherence client instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.client(config) .start() .join(); Using Coherence Extend and application can configure in this way, with multiple Session instances, where each session will connect as an Extend client to a different Coherence cluster. Each configured session is given a different name and scope. The required sessions can then be obtained from the running Coherence instance by application code at runtime. ",
            "title": "Configure an Extend Client"
        },
        {
            "location": "/examples/guides/050-bootstrap/README",
            "text": " If your application needs to control start-up and shutdown of Coherence, then the bootstrap API can be called from application code. This is often useful in integration JUnit test code too, where a test class may need to configure and start Coherence for a set of tests. It is possible for application code to run multiple Coherence instances, which each manage one or more scoped Coherence sessions. Where multiple Coherence cluster member instances are created, they will still all be part of a single Coherence cluster member, they cannot be part of separate clusters. Run a Simple Cluster Member The simplest way to start Coherence as a cluster member in application code is shown below: <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember(); coherence.start(); The start() method returns a CompletableFuture so application code that needs to wit for start-up to complete can use the future for this purpose. The example below ensures Coherence is started as a cluster member (waiting a maximum of five minutes) before proceeding. <markup lang=\"java\" > Coherence coherence = Coherence.clusterMember() .start() .get(5, TimeUnit.MINUTES); Running Coherence in this way will create a single Session using the default cache configuration file (or another file specified using the -Dcoherence.cacheconfig system property). By default, this will be a storage enabled cluster member, unless Coherence system properties or environment variables have been used to override this. Configure a Cluster Member The bootstrap API allows the Coherence instance to be configured before starting, for example adding one or more session configurations. In the example below, a Coherence cluster member instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.clusterMember(config) .start() .join(); There are various other methods on the configuration builders, for example configuring parameters to pass into the cache configuration files, configuring interceptors, etc. Run Coherence as an Extend Client If the application code will is an Extend client, then Coherence can be bootstrapped in client mode. The example below starts Coherence as an Extend client, which will use the Coherence NameService to locate the cluster and look up the Extend Proxy to connect to. This works by configuring the client to have the same cluster name and same well-known address list (or multicast settings) as the cluster being connected to, either using System properties or environment variables. <markup lang=\"java\" > Coherence coherence = Coherence.client(); coherence.start(); Alternatively, instead of using the NameService a fixed address and port can be configured for the Extend client to use. If the System property coherence.extend.address is set to the IP address or host name of the Extend proxy, and coherence.extend.port is set to the port of the Extend proxy (or the corresponding environment variables COHERENCE_EXTEND_ADDRESS and COHERENCE_EXTEND_PORT ) then Coherence can be bootstrapped as shown below. <markup lang=\"java\" > Coherence coherence = Coherence.fixedClient(); coherence.start(); Coherence will then be bootstrapped as an Extend client and connect to the proxy on the configured address and port. Note The code snippets above work with the default cache configuration file. The default cache configuration file in the coherence.jar is configured with certain injectable property values, which are configured by the bootstrap API when running as a client. Using other cache configuration files that are not configured with these properties would mean \"client\" mode is effectively ignored. The Coherence instance will still be started and will run correctly, the client mode properties will just have no affect. Configure an Extend Client Coherence can be configured in client mode in code. In the example below, a Coherence client instance is created using a configuration. The configuration in this case does not specify a name, so the default name will be used. The configuration contains two Session configurations. The first is named \"foo\" and uses the cache configuration loaded from foo-cache-config.xml with the scope name \"Foo\". The second Session will be the default session using the default cache configuration file. <markup lang=\"java\" > SessionConfiguration sessionConfiguration = SessionConfiguration.builder() .named(\"Foo\") .withScopeName(\"Foo\") .withConfigUri(\"foo-cache-config.xml\") .build(); CoherenceConfiguration config = CoherenceConfiguration.builder() .withSession(sessionConfiguration) .withSession(SessionConfiguration.defaultSession()) .build(); Coherence coherence = Coherence.client(config) .start() .join(); Using Coherence Extend and application can configure in this way, with multiple Session instances, where each session will connect as an Extend client to a different Coherence cluster. Each configured session is given a different name and scope. The required sessions can then be obtained from the running Coherence instance by application code at runtime. ",
            "title": "Bootstrap Coherence in Application Code"
        },
        {
            "location": "/docs/README",
            "text": " To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs ",
            "title": "Build the Docs"
        },
        {
            "location": "/docs/README",
            "text": " To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "View the Docs"
        },
        {
            "location": "/docs/README",
            "text": " This is the module that builds the Coherence documentation. The module is not part of the default build and must be built separately. Build the Docs To build the docs, run the following Maven command from the top-level prj/ directory: <markup lang=\"shell\" >mvn clean install -DskipTests -pl docs -P docs View the Docs To view the documentation to see what it looks like after building run the following command from the top-level prj/ directory: <markup lang=\"shell\" >mvn exec:exec -pl docs -P docs Docs can be viewd at http://localhost:8080 This requires Python to be installed and runs a small Python http server from the directory where the docs have been built to. ",
            "title": "Coherence Documentation Module"
        },
        {
            "location": "/docs/README",
            "text": " When putting version numbers in .adoc files, we use attribute substitutions. Attributes are set in the sitegen.yaml file, for example <markup lang=\"yaml\" >engine: asciidoctor: images-dir: \"docs/images\" libraries: - \"asciidoctor-diagram\" attributes: plantumlconfig: \"_plantuml-config.txt\" coherence-maven-group-id: \"${coherence.group.id}\" version-coherence: \"${revision}\" version-commercial-docs: \"14.1.1.0\" version-helidon: \"${helidon.version}\" The format of an attribute is name followed by a colon, and the attribute value in quotes, so above the value of the version-commercial-docs attribute is 14.1.1.0 . Attributes can be taken from Maven build properties by using the normal Maven property replacement string as the value. For example the version-coherence attribute&#8217;s value will be the Maven revision property value. In the .adoc files the attributes are then substituted by putting the attribute name in curly brackets. For example: The current commercial Coherence version is 14.1.1.2206. would become The current commercial Coherence version is 14.1.1.0. ",
            "title": "Version Numbers"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " First and foremost, Coherence provides a fundamental service that is responsible for all facets of clustering and is a common denominator / building block for all other Coherence services. This service, referred to as 'service 0' internally, ensures the mesh of members is maintained and responsive, taking action to collaboratively evict, shun, or in some cases voluntarily depart the cluster when deemed necessary. As members join and leave the cluster, other Coherence services are notified thus allows those services to react accordingly. This part of the Coherence product has been in production for 10+ years, being the subject of some extensive and imaginative testing. While it has been discussed here it certainly is not something that customers, generally, interact with directly but is valuable to be aware of. Coherence services build on top of the clustering service, with the key implementations to be aware of are PartitionedService, InvocationService, and ProxyService. In the majority of cases customers will deal with caches; a cache will be represented by an implementation of NamedCache&lt;K,V&gt; . Cache is an unfortunate name, as many Coherence customers use Coherence as a system-of-record rather than a lossy store of data. A cache is hosted by a service, generally the PartitionedService, and is the entry point to storing, retrieving, aggregating, querying, and streaming data. There are a number of features that caches provide: Fundamental key-based access : get/put getAll/putAll Client-side and storage-side events MapListeners to asynchronously notify clients of changes to data EventInterceptors (either sync or async) to be notified storage level events, including mutations, partition transfer, failover, etc NearCaches - locally cached data based on previous requests with local content invalidated upon changes in storage tier ViewCaches - locally stored view of remote data that can be a subset based on a predicate and is kept in sync real time Queries - distributed, parallel query evaluation to return matching key, values or entries with potential to optimize performance with indices Aggregations - a map/reduce style aggregation where data is aggregated in parallel on all storage nodes and results streamed back to the client for aggregation of those results to produce a final result Data local processing - an ability to send a function to the relevant storage node to execute processing logic for the appropriate entries with exclusive access Partition local transactions - an ability to perform scalable transactions by associating data (thus being on the same partition) and manipulating other entries on the same partition potentially across caches Non-blocking / async NamedCache API C&#43;&#43; and .NET clients - access the same NamedCache API from either C&#43;&#43; or .NET Portable Object Format - optimized serialization format, with the ability to navigate the serialized form for optimized queries, aggregations, or data processing Integration with Databases - Database &amp; third party data integration with CacheStores including both synchronous or asynchronous writes CohQL - ansi-style query language with a console for adhoc queries Topics - distributed topics implementation offering pub/sub messaging with the storage capacity the cluster and parallelizable subscribers There are also a number of non-functional features that Coherence provides: Rock solid clustering - highly tuned and robust clustering stack that allows Coherence to scale to thousands of members in a cluster with thousands of partitions and terabytes of data being accessed, mutated, queried and aggregated concurrently Safety first - resilient data management that ensures backup copies are on distinct machines, racks, or sites and the ability to maintain multiple backups 24/7 Availability - zero down time with rolling redeploy of cluster members to upgrade application or product versions Backwards and forwards compatibility of product upgrades, including major versions Persistent Caches - with the ability to use local file system persistence (thus avoid extra network hops) and leverage Coherence consensus protocols to perform distributed disk recovery when appropriate Distributed State Snapshot - ability to perform distributed point-in-time snapshot of cluster state, and recover snapshot in this or a different cluster (leverages persistence feature) Lossy redundancy - ability to reduce the redundancy guarantee by making backups and/or persistence asynchronous from a client perspective Single Mangement View - provides insight into the cluster with a single JMX server that provides a view of all members of the cluster Management over REST - all JMX data and operations can be performed over REST, including cluster wide thread dumps and heapdumps Non-cluster Access - access to the cluster from the outside via proxies, for distant (high latency) clients and for non-java languages such as C&#43;&#43; and .NET Kubernetes friendly - seamlessly and safely deploy applications to k8s with our own operator ",
            "title": "Introduction"
        },
        {
            "location": "/docs/about/02_introduction",
            "text": " Coherence Community Edition does not include the following Oracle Coherence commercial edition functionality Management of Coherence via the Oracle WebLogic Management Framework WebLogic Server Multi-tenancy support Deployment of Grid Archives (GARs) HTTP session management for application servers (Coherence*Web) GoldenGate HotCache TopLink-based CacheLoaders and CacheStores Elastic Data Federation and WAN (wide area network) support Transaction Framework CommonJ work manager ",
            "title": "Coherence Community Edition Disabled and Excluded Functionality"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Configuration Review the Test Classes Run the Examples Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " In this example you will run tests that show a number of ways to configure secure communication via SSL by defining various SSL socket providers. The tests carry out the following, for a variety of socket providers: Generate keys and self-signed certificates to be used in the test Start 2 cache servers, one having a Proxy service enabled passing properties to point to the newly created keys and certificates Run a basic put/get test over SSL via Coherence*Extend passing properties to point to the newly created keys and certificates Each test showcases the different methods of configuring SSL: Using Java key stores Referring directly to keys and certificates on the file-system Using custom loaders to load key stores, private keys and certificates Custom loaders can also be configured to be refreshed based upon intervals. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " When configuring SSL, you define a &lt;socket-provider&gt; in the Coherence operational configuration and refer to this in your operational and cache configuration. The socket providers for this test are explained further below. To enable SSL for cluster communication, add a reference to the socket provider in your &lt;unicast-listener&gt; element as shown below: <markup lang=\"xml\" >&lt;unicast-listener&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;well-known-addresses&gt; &lt;address system-property=\"coherence.wka\"&gt;127.0.0.1&lt;/address&gt; &lt;/well-known-addresses&gt; &lt;/unicast-listener&gt; To enable SSL on a Proxy server, specify a &lt;socket-provider&gt; in the &lt;tcp-acceptor&gt; element of the proxy scheme as shown below: <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;local-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart system-property=\"test.proxy.enabled\"&gt;false&lt;/autostart&gt; &lt;/proxy-scheme&gt; Finally, to enable SSL on a Coherence*Extend client, specify a &lt;socket-provider&gt; in the &lt;tcp-initiator&gt; element of the &lt;remote-cache-scheme&gt; as shown below: <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; For this example, we define a number of socket providers in the operational configuration src/test/resources/tangosol-coherence-ssl.xml . Each test which is run sets the system property test.socket.provider to one of the following values to test the configuration: sslKeyStore - configure using Java key store and trust store sslKeyAndCert - configure using keys and certificates on the file system sslCustomKeyAndCert - configure using custom private key a certificate loaders (This is especially useful in Kubernetes environments to load from secrets) sslCustomKeyStore - configure using a custom key store loader when specifying a trust store, you get two-way SSL. Each configuration option is outlined below: sslKeyStore - configure SSL socket provider using Java key store and trust store <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyStore\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"test.server.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/url&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using Java key store Trust manager using Java key store sslKeyAndCert - configure SSL socket provider using key and certificate files only <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"test.server.key\"/&gt; &lt;cert system-property=\"test.server.cert\"/&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"test.server.ca.cert\"/&gt; &lt;cert system-property=\"test.client.ca.cert\"/&gt; &lt;/trust-manager&gt; &lt;!-- &lt;refresh-period&gt;24h&lt;/refresh-period&gt; --&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using key and certificate directly Trust manager using key and certificate directly Optional refresh period for keys and certificates sslCustomKeyAndCert - configure SSL socket provider using custom private key and certificate loaders <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomPrivateKeyLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.key\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-loader&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.ca.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom private key loader Identity manager using custom certificate key loader Trust manager using custom certificate key loader sslCustomKeyStore - configure SSL socket provider using a custom key store loader <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyStore\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.keystore\"&gt;file:client.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom key store loader Identity manager using custom key store loader ",
            "title": "Socket Provider Definitions"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " Socket Provider Definitions When configuring SSL, you define a &lt;socket-provider&gt; in the Coherence operational configuration and refer to this in your operational and cache configuration. The socket providers for this test are explained further below. To enable SSL for cluster communication, add a reference to the socket provider in your &lt;unicast-listener&gt; element as shown below: <markup lang=\"xml\" >&lt;unicast-listener&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;well-known-addresses&gt; &lt;address system-property=\"coherence.wka\"&gt;127.0.0.1&lt;/address&gt; &lt;/well-known-addresses&gt; &lt;/unicast-listener&gt; To enable SSL on a Proxy server, specify a &lt;socket-provider&gt; in the &lt;tcp-acceptor&gt; element of the proxy scheme as shown below: <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;local-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart system-property=\"test.proxy.enabled\"&gt;false&lt;/autostart&gt; &lt;/proxy-scheme&gt; Finally, to enable SSL on a Coherence*Extend client, specify a &lt;socket-provider&gt; in the &lt;tcp-initiator&gt; element of the &lt;remote-cache-scheme&gt; as shown below: <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; For this example, we define a number of socket providers in the operational configuration src/test/resources/tangosol-coherence-ssl.xml . Each test which is run sets the system property test.socket.provider to one of the following values to test the configuration: sslKeyStore - configure using Java key store and trust store sslKeyAndCert - configure using keys and certificates on the file system sslCustomKeyAndCert - configure using custom private key a certificate loaders (This is especially useful in Kubernetes environments to load from secrets) sslCustomKeyStore - configure using a custom key store loader when specifying a trust store, you get two-way SSL. Each configuration option is outlined below: sslKeyStore - configure SSL socket provider using Java key store and trust store <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyStore\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"test.server.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/url&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using Java key store Trust manager using Java key store sslKeyAndCert - configure SSL socket provider using key and certificate files only <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"test.server.key\"/&gt; &lt;cert system-property=\"test.server.cert\"/&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"test.server.ca.cert\"/&gt; &lt;cert system-property=\"test.client.ca.cert\"/&gt; &lt;/trust-manager&gt; &lt;!-- &lt;refresh-period&gt;24h&lt;/refresh-period&gt; --&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using key and certificate directly Trust manager using key and certificate directly Optional refresh period for keys and certificates sslCustomKeyAndCert - configure SSL socket provider using custom private key and certificate loaders <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomPrivateKeyLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.key\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-loader&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.ca.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom private key loader Identity manager using custom certificate key loader Trust manager using custom certificate key loader sslCustomKeyStore - configure SSL socket provider using a custom key store loader <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyStore\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.keystore\"&gt;file:client.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom key store loader Identity manager using custom key store loader ",
            "title": "Review the Configuration"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " The example code comprises the following classes, which are explained below: AbstractSSLExampleTest - abstract test implementation to create SSL configuration files and startup cluster using the required socket provider KeyStoreSSLExampleTest - test with socket provider using Java key-store KeyStoreAndCertSSLExampleTest - test with socket provider using key and certificate files only CustomKeyStoreSSLExampleTest - test with socket provider using a custom private key and certificate loaders CustomCertificateLoader - a custom certificate loader class CustomKeyStoreLoader - a custom key store loader class CustomPrivateKeyLoader - a custom private key loader class The tests use the Oracle Bedrock KeyTool utility to generate the require keys, stores and certificates. You should use your own generated artefacts and not use these for production usage. Review the AbstractSSLExampleTest class This abstract class contains various utilities used by all tests. A few snippets are included below: Generate Test Certificates <markup lang=\"java\" >/** * Create the required certificates. * * @throws Exception if any errors creating certificates. */ @BeforeAll public static void setupSSL() throws Exception { // only initialize once if (tmpDir == null) { KeyTool.assertCanCreateKeys(); tmpDir = FileHelper.createTempDir(); serverCACert = KeyTool.createCACert(tmpDir, \"server-ca\", \"PKCS12\"); serverKeyAndCert = KeyTool.createKeyCertPair(tmpDir, serverCACert, \"server\"); clientCACert = KeyTool.createCACert(tmpDir, \"client-ca\", \"PKCS12\"); clientKeyAndCert = KeyTool.createKeyCertPair(tmpDir, clientCACert, \"client\"); } } Set Cache Server Options <markup lang=\"java\" >/** * Create options to start cache servers. * * @param clusterPort cluster port * @param proxyPort proxy port * @param socketProvider socket provider to use * @param memberName member name * * @return new {@link OptionsByType} */ protected static OptionsByType createCacheServerOptions(int clusterPort, int proxyPort, String socketProvider, String memberName) { OptionsByType optionsByType = OptionsByType.empty(); optionsByType.addAll(JMXManagementMode.ALL, JmxProfile.enabled(), LocalStorage.enabled(), WellKnownAddress.of(hostName), Multicast.ttl(0), CacheConfig.of(SERVER_CACHE_CONFIG), OperationalOverride.of(OVERRIDE), Logging.at(6), ClusterName.of(\"ssl-cluster\"), MemberName.of(memberName), SystemProperty.of(\"test.socket.provider\", socketProvider), SystemProperty.of(\"test.server.keystore\", serverKeyAndCert.getKeystoreURI()), SystemProperty.of(\"test.trust.keystore\", serverCACert.getKeystoreURI()), SystemProperty.of(\"test.server.keystore.password\", serverKeyAndCert.storePasswordString()), SystemProperty.of(\"test.server.key.password\", serverKeyAndCert.keyPasswordString()), SystemProperty.of(\"test.trust.keystore.password\", serverCACert.storePasswordString()), SystemProperty.of(\"test.client.ca.cert\", clientCACert.getCertURI()), SystemProperty.of(\"test.server.key\", serverKeyAndCert.getKeyPEMNoPassURI()), SystemProperty.of(\"test.server.cert\", serverKeyAndCert.getCertURI()), SystemProperty.of(\"test.server.ca.cert\", serverCACert.getCertURI()), ClusterPort.of(clusterPort)); // enable proxy server if a proxy port is not -1 if (proxyPort != -1) { optionsByType.addAll(SystemProperty.of(\"test.extend.address\", hostName), SystemProperty.of(\"test.extend.port\", proxyPort), SystemProperty.of(\"test.proxy.enabled\", \"true\") ); } return optionsByType; } Run the Simple Test <markup lang=\"java\" >/** * Run a simple test using Coherence*Extend with the given socket-provider to validate * that SSL communications for the cluster and proxy are working. * * @param socketProvider socket provider to use */ protected void runTest(String socketProvider) { _startup(socketProvider); NamedCache&lt;Integer, String&gt; cache = getCache(socketProvider); cache.clear(); cache.put(1, \"one\"); assertEquals(\"one\", cache.get(1)); } Review the KeyStoreSSLExampleTest class which tests with a socket provider using Java key-store <markup lang=\"java\" >/** * Test SSL using Java key-store and trust-store. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyStoreSocketProvider() { runTest(\"sslKeyStore\"); } } Specify the SSL socket provider Review the KeyStoreAndCertSSLExampleTest class which tests with socket provider using key and certificate files only <markup lang=\"java\" >/** * Test SSL using Key and Certificate. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyAndCertSocketProvider() throws Exception { runTest(\"sslKeyAndCert\"); } } Specify the SSL socket provider Review the CustomKeyStoreSSLExampleTest class which tests with socket provider using a custom private key and certificate loader <markup lang=\"java\" >/** * Test SSL using custom key-store loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyStoreSocketProvider() { runTest(\"sslCustomKeyStore\"); } } Specify the SSL socket provider Review the CustomKeyAndCertSSLExampleTest class which tests with socket provider using a custom key store loader <markup lang=\"java\" >/** * Test SSL using custom key and certificate loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyAndCertSocketProvider() { runTest(\"sslCustomKeyAndCert\"); } } Specify the SSL socket provider Review the CustomCertificateLoader <markup lang=\"java\" >/** * An example implementation of a {@link CertificateLoader} which loads a certificate from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomCertificateLoader extends AbstractCertificateLoader { public CustomCertificateLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomKeyStoreLoader <markup lang=\"java\" >/** * An example implementation of a {@link KeyStoreLoader} which loads a key store from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreLoader extends AbstractKeyStoreLoader { public CustomKeyStoreLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomPrivateKeyLoader <markup lang=\"java\" >/** * An example implementation of a {@link PrivateKeyLoader} which loads a private key from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomPrivateKeyLoader extends AbstractPrivateKeyLoader { public CustomPrivateKeyLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } ",
            "title": "Review the Test Classes"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes in the com.oracle.coherence.guides.ssl package. Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test has run you will see output from the various parts of the test code. See below for some key items to look out for. Messages indicating the cluster is using SSL socket provider <markup lang=\"bash\" >TCMP bound to /127.0.0.1:51684 using TCPDatagramSocketProvider[Delegate: SSLSocketProvider(SSLSocketProvider())] Cluster members connecting using tmbs (TCP Message bus over SSL) <markup lang=\"bash\" >tmbs://127.0.0.1:52311.51395 opening connection with tmbs://127.0.0.1:52315.47215 using SSLSocket(null /127.0.0.1:866404240, buffered{clear=0 encrypted=0 out=0}, handshake=NOT_HANDSHAKING, jobs=0 Cluster musing two way (key and trust stores) for communication <markup lang=\"bash\" > instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/.../examples/guides/210-ssl/target/test-classes/certs/server.jks, trust=SunX509//.../examples/guides/210-ssl/target/test-classes/certs/server-ca-ca.jks) ",
            "title": "Run the Examples"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " In this guide you learned how to secure Coherence communication between cluster members as well as Coherence*Extend clients. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " Introduction to Coherence Security Using SSL to Secure Communication ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/210-ssl/README",
            "text": " This guide walks you through how to secure Coherence communication between cluster members as well as Coherence*Extend clients. Oracle Coherence supports Secure Sockets Layer (SSL) to secure TCMP communication between cluster nodes and to secure the TCP communication between Oracle Coherence*Extend clients and proxies. Oracle Coherence supports the Transport Layer Security (TLS) protocol, which superseded the SSL protocol; however, the term SSL is used in this documentation because it is the more widely recognized term. See the Coherence documentation links below for more detailed information on Coherence Security. Introduction to Coherence Security Using SSL to Secure Communication Table of Contents What You Will Build What You Need Building the Example Code Review the Configuration Review the Test Classes Run the Examples Summary See Also What You Will Build In this example you will run tests that show a number of ways to configure secure communication via SSL by defining various SSL socket providers. The tests carry out the following, for a variety of socket providers: Generate keys and self-signed certificates to be used in the test Start 2 cache servers, one having a Proxy service enabled passing properties to point to the newly created keys and certificates Run a basic put/get test over SSL via Coherence*Extend passing properties to point to the newly created keys and certificates Each test showcases the different methods of configuring SSL: Using Java key stores Referring directly to keys and certificates on the file-system Using custom loaders to load key stores, private keys and certificates Custom loaders can also be configured to be refreshed based upon intervals. What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Configuration Socket Provider Definitions When configuring SSL, you define a &lt;socket-provider&gt; in the Coherence operational configuration and refer to this in your operational and cache configuration. The socket providers for this test are explained further below. To enable SSL for cluster communication, add a reference to the socket provider in your &lt;unicast-listener&gt; element as shown below: <markup lang=\"xml\" >&lt;unicast-listener&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;well-known-addresses&gt; &lt;address system-property=\"coherence.wka\"&gt;127.0.0.1&lt;/address&gt; &lt;/well-known-addresses&gt; &lt;/unicast-listener&gt; To enable SSL on a Proxy server, specify a &lt;socket-provider&gt; in the &lt;tcp-acceptor&gt; element of the proxy scheme as shown below: <markup lang=\"xml\" >&lt;proxy-scheme&gt; &lt;service-name&gt;Proxy&lt;/service-name&gt; &lt;acceptor-config&gt; &lt;tcp-acceptor&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;local-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/local-address&gt; &lt;/tcp-acceptor&gt; &lt;/acceptor-config&gt; &lt;autostart system-property=\"test.proxy.enabled\"&gt;false&lt;/autostart&gt; &lt;/proxy-scheme&gt; Finally, to enable SSL on a Coherence*Extend client, specify a &lt;socket-provider&gt; in the &lt;tcp-initiator&gt; element of the &lt;remote-cache-scheme&gt; as shown below: <markup lang=\"xml\" >&lt;remote-cache-scheme&gt; &lt;scheme-name&gt;remote&lt;/scheme-name&gt; &lt;service-name&gt;RemoteCache&lt;/service-name&gt; &lt;proxy-service-name&gt;Proxy&lt;/proxy-service-name&gt; &lt;initiator-config&gt; &lt;tcp-initiator&gt; &lt;socket-provider system-property=\"test.socket.provider\"&gt;provider&lt;/socket-provider&gt; &lt;remote-addresses&gt; &lt;socket-address&gt; &lt;address system-property=\"test.extend.address\"/&gt; &lt;port system-property=\"test.extend.port\"/&gt; &lt;/socket-address&gt; &lt;/remote-addresses&gt; &lt;/tcp-initiator&gt; &lt;/initiator-config&gt; &lt;/remote-cache-scheme&gt; For this example, we define a number of socket providers in the operational configuration src/test/resources/tangosol-coherence-ssl.xml . Each test which is run sets the system property test.socket.provider to one of the following values to test the configuration: sslKeyStore - configure using Java key store and trust store sslKeyAndCert - configure using keys and certificates on the file system sslCustomKeyAndCert - configure using custom private key a certificate loaders (This is especially useful in Kubernetes environments to load from secrets) sslCustomKeyStore - configure using a custom key store loader when specifying a trust store, you get two-way SSL. Each configuration option is outlined below: sslKeyStore - configure SSL socket provider using Java key store and trust store <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyStore\"&gt; &lt;ssl&gt; &lt;protocol&gt;TLS&lt;/protocol&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;url system-property=\"test.server.keystore\"&gt;file:server.jks&lt;/url&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;url system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/url&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;socket-provider&gt;tcp&lt;/socket-provider&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using Java key store Trust manager using Java key store sslKeyAndCert - configure SSL socket provider using key and certificate files only <markup lang=\"xml\" >&lt;socket-provider id=\"sslKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key system-property=\"test.server.key\"/&gt; &lt;cert system-property=\"test.server.cert\"/&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert system-property=\"test.server.ca.cert\"/&gt; &lt;cert system-property=\"test.client.ca.cert\"/&gt; &lt;/trust-manager&gt; &lt;!-- &lt;refresh-period&gt;24h&lt;/refresh-period&gt; --&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using key and certificate directly Trust manager using key and certificate directly Optional refresh period for keys and certificates sslCustomKeyAndCert - configure SSL socket provider using custom private key and certificate loaders <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyAndCert\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomPrivateKeyLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.key\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-loader&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;cert-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomCertificateLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.ca.cert\"/&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/cert-loader&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom private key loader Identity manager using custom certificate key loader Trust manager using custom certificate key loader sslCustomKeyStore - configure SSL socket provider using a custom key store loader <markup lang=\"xml\" >&lt;socket-provider id=\"sslCustomKeyStore\"&gt; &lt;ssl&gt; &lt;identity-manager&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.server.keystore\"&gt;file:client.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.server.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;password system-property=\"test.server.key.password\"&gt;private&lt;/password&gt; &lt;/identity-manager&gt; &lt;trust-manager&gt; &lt;algorithm&gt;SunX509&lt;/algorithm&gt; &lt;key-store&gt; &lt;key-store-loader&gt; &lt;class-name&gt;com.oracle.coherence.guides.ssl.loaders.CustomKeyStoreLoader&lt;/class-name&gt; &lt;init-params&gt; &lt;init-param&gt; &lt;param-type&gt;string&lt;/param-type&gt; &lt;param-value system-property=\"test.trust.keystore\"&gt;file:trust.jks&lt;/param-value&gt; &lt;/init-param&gt; &lt;/init-params&gt; &lt;/key-store-loader&gt; &lt;password system-property=\"test.trust.keystore.password\"&gt;password&lt;/password&gt; &lt;/key-store&gt; &lt;/trust-manager&gt; &lt;/ssl&gt; &lt;/socket-provider&gt; Identity manager using custom key store loader Identity manager using custom key store loader Review the Test Classes The example code comprises the following classes, which are explained below: AbstractSSLExampleTest - abstract test implementation to create SSL configuration files and startup cluster using the required socket provider KeyStoreSSLExampleTest - test with socket provider using Java key-store KeyStoreAndCertSSLExampleTest - test with socket provider using key and certificate files only CustomKeyStoreSSLExampleTest - test with socket provider using a custom private key and certificate loaders CustomCertificateLoader - a custom certificate loader class CustomKeyStoreLoader - a custom key store loader class CustomPrivateKeyLoader - a custom private key loader class The tests use the Oracle Bedrock KeyTool utility to generate the require keys, stores and certificates. You should use your own generated artefacts and not use these for production usage. Review the AbstractSSLExampleTest class This abstract class contains various utilities used by all tests. A few snippets are included below: Generate Test Certificates <markup lang=\"java\" >/** * Create the required certificates. * * @throws Exception if any errors creating certificates. */ @BeforeAll public static void setupSSL() throws Exception { // only initialize once if (tmpDir == null) { KeyTool.assertCanCreateKeys(); tmpDir = FileHelper.createTempDir(); serverCACert = KeyTool.createCACert(tmpDir, \"server-ca\", \"PKCS12\"); serverKeyAndCert = KeyTool.createKeyCertPair(tmpDir, serverCACert, \"server\"); clientCACert = KeyTool.createCACert(tmpDir, \"client-ca\", \"PKCS12\"); clientKeyAndCert = KeyTool.createKeyCertPair(tmpDir, clientCACert, \"client\"); } } Set Cache Server Options <markup lang=\"java\" >/** * Create options to start cache servers. * * @param clusterPort cluster port * @param proxyPort proxy port * @param socketProvider socket provider to use * @param memberName member name * * @return new {@link OptionsByType} */ protected static OptionsByType createCacheServerOptions(int clusterPort, int proxyPort, String socketProvider, String memberName) { OptionsByType optionsByType = OptionsByType.empty(); optionsByType.addAll(JMXManagementMode.ALL, JmxProfile.enabled(), LocalStorage.enabled(), WellKnownAddress.of(hostName), Multicast.ttl(0), CacheConfig.of(SERVER_CACHE_CONFIG), OperationalOverride.of(OVERRIDE), Logging.at(6), ClusterName.of(\"ssl-cluster\"), MemberName.of(memberName), SystemProperty.of(\"test.socket.provider\", socketProvider), SystemProperty.of(\"test.server.keystore\", serverKeyAndCert.getKeystoreURI()), SystemProperty.of(\"test.trust.keystore\", serverCACert.getKeystoreURI()), SystemProperty.of(\"test.server.keystore.password\", serverKeyAndCert.storePasswordString()), SystemProperty.of(\"test.server.key.password\", serverKeyAndCert.keyPasswordString()), SystemProperty.of(\"test.trust.keystore.password\", serverCACert.storePasswordString()), SystemProperty.of(\"test.client.ca.cert\", clientCACert.getCertURI()), SystemProperty.of(\"test.server.key\", serverKeyAndCert.getKeyPEMNoPassURI()), SystemProperty.of(\"test.server.cert\", serverKeyAndCert.getCertURI()), SystemProperty.of(\"test.server.ca.cert\", serverCACert.getCertURI()), ClusterPort.of(clusterPort)); // enable proxy server if a proxy port is not -1 if (proxyPort != -1) { optionsByType.addAll(SystemProperty.of(\"test.extend.address\", hostName), SystemProperty.of(\"test.extend.port\", proxyPort), SystemProperty.of(\"test.proxy.enabled\", \"true\") ); } return optionsByType; } Run the Simple Test <markup lang=\"java\" >/** * Run a simple test using Coherence*Extend with the given socket-provider to validate * that SSL communications for the cluster and proxy are working. * * @param socketProvider socket provider to use */ protected void runTest(String socketProvider) { _startup(socketProvider); NamedCache&lt;Integer, String&gt; cache = getCache(socketProvider); cache.clear(); cache.put(1, \"one\"); assertEquals(\"one\", cache.get(1)); } Review the KeyStoreSSLExampleTest class which tests with a socket provider using Java key-store <markup lang=\"java\" >/** * Test SSL using Java key-store and trust-store. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyStoreSocketProvider() { runTest(\"sslKeyStore\"); } } Specify the SSL socket provider Review the KeyStoreAndCertSSLExampleTest class which tests with socket provider using key and certificate files only <markup lang=\"java\" >/** * Test SSL using Key and Certificate. * * @author Tim Middleton 2022.06.15 */ public class KeyStoreAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testKeyAndCertSocketProvider() throws Exception { runTest(\"sslKeyAndCert\"); } } Specify the SSL socket provider Review the CustomKeyStoreSSLExampleTest class which tests with socket provider using a custom private key and certificate loader <markup lang=\"java\" >/** * Test SSL using custom key-store loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyStoreSocketProvider() { runTest(\"sslCustomKeyStore\"); } } Specify the SSL socket provider Review the CustomKeyAndCertSSLExampleTest class which tests with socket provider using a custom key store loader <markup lang=\"java\" >/** * Test SSL using custom key and certificate loader. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyAndCertSSLExampleTest extends AbstractSSLExampleTest { @Test public void testCustomKeyAndCertSocketProvider() { runTest(\"sslCustomKeyAndCert\"); } } Specify the SSL socket provider Review the CustomCertificateLoader <markup lang=\"java\" >/** * An example implementation of a {@link CertificateLoader} which loads a certificate from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomCertificateLoader extends AbstractCertificateLoader { public CustomCertificateLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomKeyStoreLoader <markup lang=\"java\" >/** * An example implementation of a {@link KeyStoreLoader} which loads a key store from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomKeyStoreLoader extends AbstractKeyStoreLoader { public CustomKeyStoreLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Review the CustomPrivateKeyLoader <markup lang=\"java\" >/** * An example implementation of a {@link PrivateKeyLoader} which loads a private key from a file. * * @author Tim Middleton 2022.06.16 */ public class CustomPrivateKeyLoader extends AbstractPrivateKeyLoader { public CustomPrivateKeyLoader(String url) { super(url); } @Override protected InputStream getInputStream() throws IOException { try { return Resources.findInputStream(m_sName); } catch (IOException e) { throw new IOException(e); } } } Run the Examples Run the examples using the test case below. Run directly from your IDE by running either of the following test classes in the com.oracle.coherence.guides.ssl package. Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test has run you will see output from the various parts of the test code. See below for some key items to look out for. Messages indicating the cluster is using SSL socket provider <markup lang=\"bash\" >TCMP bound to /127.0.0.1:51684 using TCPDatagramSocketProvider[Delegate: SSLSocketProvider(SSLSocketProvider())] Cluster members connecting using tmbs (TCP Message bus over SSL) <markup lang=\"bash\" >tmbs://127.0.0.1:52311.51395 opening connection with tmbs://127.0.0.1:52315.47215 using SSLSocket(null /127.0.0.1:866404240, buffered{clear=0 encrypted=0 out=0}, handshake=NOT_HANDSHAKING, jobs=0 Cluster musing two way (key and trust stores) for communication <markup lang=\"bash\" > instantiated SSLSocketProviderDependencies: SSLSocketProvider(auth=two-way, identity=SunX509/.../examples/guides/210-ssl/target/test-classes/certs/server.jks, trust=SunX509//.../examples/guides/210-ssl/target/test-classes/certs/server-ca-ca.jks) Summary In this guide you learned how to secure Coherence communication between cluster members as well as Coherence*Extend clients. See Also Introduction to Coherence Security Using SSL to Secure Communication ",
            "title": "Securing with SSL"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Java - JDK 17 or higher Maven - 3.8.5 or higher ",
            "title": "Prerequisites"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "How to Get Coherence Community Edition"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Prerequisites Java - JDK 17 or higher Maven - 3.8.5 or higher How to Get Coherence Community Edition As Coherence is generally embedded into an application by using Coherence APIs, the natural place to consume this dependency is from Maven: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; You can also get Coherence from the official Docker image . For other language clients, use ( C&#43;&#43; and .NET ), and for the non-community edition, see Oracle Technology Network . ",
            "title": "Quick Start"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=23.03-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/23.03-SNAPSHOT/coherence-23.03-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 ",
            "title": " CohQL Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=23.03-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/23.03-SNAPSHOT/coherence-23.03-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": " Coherence Console"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates the procedure to start a storage enabled Coherence Server, followed by a storage disabled Coherence Console. Using the console, data is inserted, retrieved, and then the console is terminated. The console is restarted and data is once again retrieved to illustrate the permanence of the data. This example uses the out-of-the-box cache configuration and therefore explicitly specifying the console is storage disabled is unnecessary. Coherence cluster members discover each other via one of two mechanisms; multicast (default) or Well Known Addressing (deterministic broadcast). If your system does not support multicast, enable WKA by specifying -Dcoherence.wka=localhost for both processes started in the following console examples. CohQL Console To run a CohQL console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=23.03-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/23.03-SNAPSHOT/coherence-23.03-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select * from welcomes CohQL&gt; insert into welcomes key 'english' value 'Hello' CohQL&gt; insert into welcomes key 'spanish' value 'Hola' CohQL&gt; insert into welcomes key 'french' value 'Bonjour' CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; java -cp $COH_JAR com.tangosol.coherence.dslquery.QueryPlus CohQL&gt; select key(), value() from welcomes Results [\"french\", \"Bonjour\"] [\"english\", \"Hello\"] [\"spanish\", \"Hola\"] CohQL&gt; bye $&gt; kill %1 Coherence Console To run the Coherence console: <markup lang=\"shell\" >$&gt; mvn -DgroupId=com.oracle.coherence.ce -DartifactId=coherence -Dversion=23.03-SNAPSHOT dependency:get $&gt; export COH_JAR=~/.m2/repository/com/oracle/coherence/ce/coherence/23.03-SNAPSHOT/coherence-23.03-SNAPSHOT.jar $&gt; java -jar $COH_JAR &amp; $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): get english null Map (welcomes): put english Hello null Map (welcomes): put spanish Hola null Map (welcomes): put french Bonjour null Map (welcomes): get english Hello Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; java -cp $COH_JAR com.tangosol.net.CacheFactory Map (?): cache welcomes Map (welcomes): list french = Bonjour spanish = Hola english = Hello Map (welcomes): bye $&gt; kill %1 ",
            "title": "CLI Hello Coherence"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": "Build HelloCoherence "
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": " The following example illustrates starting a storage enabled Coherence server, followed by running the HelloCoherence application. The HelloCoherence application inserts and retrieves data from the Coherence server. Build HelloCoherence Create a maven project either manually or by using an archetype such as maven-archetype-quickstart Add a dependency to the pom file: <markup lang=\"xml\" title=\"pom.xml\" >&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; Copy and paste the following source to a file named src/main/java/HelloCoherence.java: <markup lang=\"java\" title=\"HelloCoherence.java\" >import com.tangosol.net.CacheFactory; import com.tangosol.net.NamedMap public class HelloCoherence { // ----- static methods ------------------------------------------------- public static void main(String[] asArgs) { NamedMap&lt;String, String&gt; map = CacheFactory.getCache(\"welcomes\"); System.out.printf(\"Accessing map \\\"%s\\\" containing %d entries\", map.getName(), map.size()); map.put(\"english\", \"Hello\"); map.put(\"spanish\", \"Hola\"); map.put(\"french\" , \"Bonjour\"); // list map.entrySet().forEach(System.out::println); } } Compile the maven project: <markup lang=\"shell\" >mvn package Start a Storage server <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"com.tangosol.net.DefaultCacheServer\" &amp; Run HelloCoherence <markup lang=\"shell\" >mvn exec:java -Dexec.mainClass=\"HelloCoherence\" Confirm that you see the output including the following: <markup lang=\"shell\" >Accessing map \"welcomes\" containing 3 entries ConverterEntry{Key=\"french\", Value=\"Bonjour\"} ConverterEntry{Key=\"spanish\", Value=\"Hola\"} ConverterEntry{Key=\"english\", Value=\"Hello\"} Kill the storage server started earlier: <markup lang=\"shell\" >kill %1 ",
            "title": " Programmatic Hello Coherence Example"
        },
        {
            "location": "/docs/about/03_quickstart",
            "text": "<markup lang=\"shell\" >$&gt; git clone git@github.com:oracle/coherence.git $&gt; cd coherence/prj # build all modules $&gt; mvn clean install # build all modules skipping tests $&gt; mvn clean install -DskipTests # build a specific module, including all dependent modules and run tests $&gt; mvn -am -pl test/functional/persistence clean verify # build only coherence.jar without running tests $&gt; mvn -am -pl coherence clean install -DskipTests # build only coherence.jar and skip compilation of CDBs and tests $&gt; mvn -am -pl coherence clean install -DskipTests -Dtde.compile.not.required ",
            "title": " Building"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI provides support for CDI (Contexts and Dependency Injection) within Coherence cluster members. It allows you both to inject Coherence-managed resources, such as NamedMap , NamedCache and Session instances into CDI managed beans, to inject CDI beans into Coherence-managed resources, such as event interceptors and cache stores, and to handle Coherence server-side events using CDI observer methods. In addition, Coherence CDI provides support for automatic injection of transient objects upon deserialization. This allows you to inject CDI managed beans such as services and repositories (to use DDD nomenclature) into transient objects, such as entry processor and even data class instances, greatly simplifying implementation of true Domain Driven applications. ",
            "title": "Coherence CDI"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Inject Views"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import jakarta.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject @SessionName(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . ",
            "title": "Injecting NamedMap , NamedCache and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import jakarta.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; ",
            "title": "Injecting NamedTopic and related objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; ",
            "title": " Cluster and OperationalContext Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; ",
            "title": "Named Session Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; ",
            "title": " Serializer Injection"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Inject a POF Serializer With a Specific POF Configuration"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Other Supported Injection Points"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import jakarta.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject @SessionName(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import jakarta.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. ",
            "title": "Injecting Coherence Objects into CDI Beans"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } ",
            "title": "Observe Specific Event Types"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } ",
            "title": "Filter Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. ",
            "title": "Transform Observed Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } ",
            "title": "Observe Events for Maps and Caches in Specific Services and Scopes"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using Asynchronous Observers"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Using CDI Observers to Handle Coherence Server-Side Events"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. ",
            "title": "Injecting CDI Beans into Coherence-managed Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Making transient classes Injectable "
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. ",
            "title": "Injecting CDI Beans into Transient Objects"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . ",
            "title": "Create the Custom Filter Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. ",
            "title": "Create the Custom Filter Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ",
            "title": "FilterBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ",
            "title": "PropertyExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); ",
            "title": "ChainedExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "PofExtractor"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) ",
            "title": "Built-In ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. ",
            "title": "Custom ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "ExtractorBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . ",
            "title": "Create the Custom Extractor Annotation"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. ",
            "title": "Create the Custom Extractor Factory"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Annotate the Injection Point"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "MapEventTransformerBinding Annotations"
        },
        {
            "location": "/coherence-cdi-server/README",
            "text": " In order to use Coherence CDI, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-cdi-server&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; Once the necessary dependency is in place, you can start using CDI to inject Coherence objects into managed CDI beans, and vice versa, as the following sections describe. Injecting Coherence Objects into CDI Beans Injecting NamedMap , NamedCache`, and related objects Injecting NamedMap or NamedCache Views Injecting NamedTopic and related objects Other Supported Injection Points Cluster and OperationalContext Injection Named Session Injection Serializer Injection Injecting CDI Beans into Coherence-managed Objects Using CDI Observers to Handle Coherence Server-Side Events Observer specific event types Filter the events to be observed Transform the events to be observed Observe events for maps and caches in specific scopes or services Using Asynchronous Observers Injecting CDI Beans into Transient Objects Making transient classes Injectable Filter Binding Annotations Extractor Binding Annotations Built-In Extractor Binding Annotations @PropertyExtractor @ChainedExtractor @PofExtractor Custom Extractor Binding Annotations MapEventTransformer Binding Annotations Injecting Coherence Objects into CDI Beans CDI, and dependency injection in general, make it easy for application classes to declare the dependencies they need and let the runtime provide them when necessary. This makes the applications easier to develop, test and reason about, and the code extremely clean. Coherence CDI allows you to do the same for Coherence objects, such as Cluster , Session , NamedMap , NamedCache , ContinuousQueryCache , ConfigurableCacheFactory , etc. Injecting NamedMap , NamedCache and related objects In order to inject an instance of a NamedMap into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import jakarta.inject.Inject; @Inject private NamedMap&lt;Long, Person&gt; people; In the example above we&#8217;ve assumed that the map name you want to inject is the same as the name of the field you are injecting into, people . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the map you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"people\") private NamedMap&lt;Long, Person&gt; m_people; This is also what you have to do if you are using constructor injection or setter injection: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } @Inject public void setPeople(@Name(\"people\") NamedMap&lt;Long, Person&gt; people) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the cache or map: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject @SessionName(\"Products\") private NamedCache&lt;Long, Product&gt; products; @Inject @SessionName(\"Customers\") private NamedCache&lt;Long, Customer&gt; customers; You can replace NamedMap or NamedCache in any of the examples above with AsyncNamedCache and AsyncNamedCache respectively, in order to inject asynchronous variant of those APIs: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private AsyncNamedMap&lt;Long, Person&gt; people; @Inject @SessionName(\"Products\") private AsyncNamedCache&lt;Long, Person&gt; Product; Inject Views You can also inject views , by simply adding View qualifier to either NamedMap or NamedCache : <markup lang=\"java\" >import com.oracle.coherence.cdi.View; import jakarta.inject.Inject; @Inject @View private NamedMap&lt;Long, Person&gt; people; @Inject @View private NamedCache&lt;Long, Product&gt; products; The examples above are equivalent, and both will bring all the data from the backing map into a local view, as they will use AlwaysFilter when constructing a view. If you want to limit the data in the view to a subset, you can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.WhereFilter; import jakarta.inject.Inject; @Inject @View @WhereFilter(\"gender = 'MALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; men; @Inject @View @WhereFilter(\"gender = 'FEMALE'\") @Name(\"people\") private NamedMap&lt;Long, Person&gt; women; The views also support transformation of the entry values on the server, in order to reduce both the amount of data stored locally, and the amount of data transferred over the network. For example, you may have a complex Person objects in the backing map, but only need their names in order to populate a drop down on the client UI. In that case, you can implement a custom ExtractorBinding (recommended), or use a built-in @PropertyExtractor for convenience: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.View; import com.oracle.coherence.cdi.PropertyExtractor; import jakarta.inject.Inject; @Inject @View @PropertyExtractor(\"fullName\") @Name(\"people\") private NamedMap&lt;Long, String&gt; names; Note that the value type in the example above has changed from Person to String , due to server-side transformation caused by the specified @PropertyExtractor . Injecting NamedTopic and related objects In order to inject an instance of a NamedTopic into your CDI bean, you simply need to define an injection point for it: <markup lang=\"java\" >import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject private NamedTopic&lt;Order&gt; orders; In the example above we&#8217;ve assumed that the topic name you want to inject is the same as the name of the field you are injecting into, in this case orders . If that&#8217;s not the case, you can use @Name qualifier to specify the name of the topic you want to obtain explicitly: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @Name(\"orders\") private NamedTopic&lt;Order&gt; topic; This is also what you have to do if you are using constructor or setter injection instead: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject public MyClass(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } @Inject public void setOrdersTopic(@Name(\"orders\") NamedTopic&lt;Order&gt; orders) { ... } All the examples above assume that you want to use the default scope, which is often, but not always the case. For example, you may have an Extend client that connects to multiple Coherence clusters, in which case you would have multiple scopes. In this case you would use @SessionName qualifier to specify the name of the configured Session , that will be used to supply the topic: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionName; import com.tangosol.net.NamedTopic; import jakarta.inject.Inject; @Inject @SessionName(\"Finance\") private NamedTopic&lt;PaymentRequest&gt; payments; @Inject @SessionName(\"Shipping\") private NamedTopic&lt;ShippingRequest&gt; shipments; The examples above allow you to inject a NamedTopic instance into your CDI bean, but it is often simpler and more convenient to inject Publisher or Subscriber for a given topic instead. This can be easily accomplished by replacing NamedTopic&lt;T&gt; in any of the examples above with either Publisher&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Publisher&lt;Order&gt; orders; @Inject @Name(\"orders\") private Publisher&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Publisher&lt;PaymentRequest&gt; payments; or Subscriber&lt;T&gt; : <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import com.oracle.coherence.cdi.SessionName; import jakarta.inject.Inject; @Inject private Subscriber&lt;Order&gt; orders; @Inject @Name(\"orders\") private Subscriber&lt;Order&gt; m_orders; @Inject @SessionName(\"Finance\") private Subscriber&lt;PaymentRequest&gt; payments; Topic metadata, such as topic name (based on either injection point name or the explicit name from @Name annotation), scope and message type, will be used under the hood to retrieve the NamedTopic , and to obtain Publisher or Subscriber from it. Additionally, if you want to place your Subscriber`s into a subscriber group (effectively turning a topic into a queue), you can easily accomplish that by adding `@SubscriberGroup qualifier to the injection point: <markup lang=\"java\" >import com.oracle.coherence.cdi.SubscriberGroup; import jakarta.inject.Inject; @Inject @SubscriberGroup(\"orders-queue\") private Subscriber&lt;Order&gt; orders; Other Supported Injection Points While the injection of a NamedMap , NamedCache , NamedTopic , and related instances, as shown above, is probably the single most used feature of Coherence CDI, it is certainly not the only one. The following sections describe other Coherence artifacts that can be injected using Coherence CDI. Cluster and OperationalContext Injection If you need an instance of a Cluster interface somewhere in your application, you can easily obtain it via injection: <markup lang=\"java\" >import com.tangosol.net.Cluster; import jakarta.inject.Inject; @Inject private Cluster cluster; You can do the same if you need an instance of an OperationalContext : <markup lang=\"java\" >import com.tangosol.net.OperationalContext; import jakarta.inject.Inject; @Inject private OperationalContext ctx; Named Session Injection On rare occasions when you need to use a Session directly, Coherence CDI makes it trivial to do so. Coherence will create a default Session when the CDI server starts, this will be created using the normal default cache configuration file. Other named sessions can be configured as CDI beans of type SessionConfiguration . For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; @ApplicationScoped public class MySession implements SessionInitializer { public String getName() { return \"Foo\"; } // implement session configuration methods } The bean above will create the configuration for a Session named Foo . When the CDI server starts the session will be created and can then be injected into other beans. A simpler way to create a SessionConfiguration is to implement the SessionIntializer interface and annotate the class. For example: <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Scope; import com.oracle.coherence.cdi.SessionInitializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @ApplicationScoped @Named(\"Foo\") @Scope(\"Foo\") @ConfigUri(\"my-coherence-config.xml\") public class MySession implements SessionInitializer { } The above configuration will create a Session bean with a name of Foo a scoep of Foo with an underlying ConfigurableCacheFactory created from the my-coherence-config.xml configuration file. To obtain an instance of the default Session , all you need to do is inject it into the class which needs to use it: <markup lang=\"java\" >import com.tangosol.net.Session; import jakarta.inject.Inject; @Inject private Session session; If you need a specific named Session you can simply qualify one using @Name qualifier and specifying the Session name: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"SessionOne\") private Session sessionOne; @Inject @Name(\"SessionTwo\") private Session sessionTwo; Serializer Injection While in most cases you won&#8217;t have to deal with serializers directly, Coherence CDI makes it simple to obtain named serializers (and to register new ones) when you need. To get a default Serializer for the current context class loader, you can simply inject it: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.inject.Inject; @Inject private Serializer defaultSerializer; However, it may be more useful to inject one of the named serializers defined in the operational configuration, which can be easily accomplished using @Name qualifier: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"java\") private Serializer javaSerializer; @Inject @Name(\"pof\") private Serializer pofSerializer; In addition to the serializers defined in the operational config, the example above will also perform BeanManager lookup for a named bean that implements Serializer interface. That means that if you implemented a custom Serializer bean, such as: <markup lang=\"java\" >import com.tangosol.io.Serializer; import jakarta.enterprise.context.ApplicationScoped; import jakarta.inject.Named; @Named(\"json\") @ApplicationScoped public class JsonSerializer implements Serializer { ... } it would be automatically discovered and registered by the CDI, and you would then be able to inject it just as easily as the named serializers defined in the operational config: <markup lang=\"java\" >import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"json\") private Serializer jsonSerializer; Inject a POF Serializer With a Specific POF Configuration POF serializers can be injected by using both the @Name and @ConfigUri qualifiers to inject a POF serializer which uses a specific POF configuration file. <markup lang=\"java\" >import com.oracle.coherence.cdi.ConfigUri; import com.oracle.coherence.cdi.Name; import jakarta.inject.Inject; @Inject @Name(\"pof\") @ConfigUri(\"test-pof-config.xml\") private Serializer pofSerializer; The code above will inject a POF serializer that uses test-pof-config.xml as its configuration file. Injecting CDI Beans into Coherence-managed Objects Coherence has a number of server-side extension points, which allow users to customize application behavior in different ways, typically by configuring their extensions within various sections of the cache configuration file. For example, the users can implement event interceptors and cache stores, in order to handle server-side events and integrate with the external data stores and other services. Coherence CDI provides a way to inject named CDI beans into these extension points using custom configuration namespace handler. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; Once you&#8217;ve declared the handler for the cdi namespace above, you can specify &lt;cdi:bean&gt; element in any place where you would normally use &lt;class-name&gt; or &lt;class-factory-name&gt; elements: <markup lang=\"xml\" >&lt;?xml version=\"1.0\"?&gt; &lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xmlns:cdi=\"class://com.oracle.coherence.cdi.server.CdiNamespaceHandler\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;registrationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;activationListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;cacheListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;distributed-scheme&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;local-storage system-property=\"coherence.distributed.localstorage\"&gt;true&lt;/local-storage&gt; &lt;partition-listener&gt; &lt;cdi:bean&gt;partitionListener&lt;/cdi:bean&gt; &lt;/partition-listener&gt; &lt;member-listener&gt; &lt;cdi:bean&gt;memberListener&lt;/cdi:bean&gt; &lt;/member-listener&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;cdi:bean&gt;storageListener&lt;/cdi:bean&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Note that you can only inject named CDI beans (beans with an explicit @Named annotations) via &lt;cdi:bean&gt; element. For example, the cacheListener interceptor bean used above would look similar to this: <markup lang=\"java\" >@ApplicationScoped @Named(\"cacheListener\") @EntryEvents(INSERTING) public class MyCacheListener implements EventInterceptor&lt;EntryEvent&lt;Long, String&gt;&gt; { @Override public void onEvent(EntryEvent&lt;Long, String&gt; e) { // handle INSERTING event } } Also keep in mind that only @ApplicationScoped beans can be injected, which implies that they may be shared. For example, because we&#8217;ve used a wildcard, * , as a cache name within the cache mapping in the example above, the same instance of cacheListener will receive events from multiple caches. This is typically fine, as the event itself provides the details about the context that raised it, including cache name, and the service it was raised from, but it does imply that any shared state that you may have within your listener class shouldn&#8217;t be context-specific, and it must be safe for concurrent access from multiple threads. If you can&#8217;t guarantee the latter, you may want to declare the onEvent method as synchronized , to ensure only one thread at a time can access any shared state you may have. Using CDI Observers to Handle Coherence Server-Side Events While the above examples show that you can implement any Coherence EventInterceptor as a CDI bean and register it using &lt;cdi:bean&gt; element within the cache configuration file, Coherence CDI also provides a much simpler way to accomplish the same goal using standard CDI Events and Observers. For example, to observe events raised by a NamedMap with the name people , with keys of type Long and values of type Person , you would define a CDI observer such as this one: <markup lang=\"java\" >private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Observe Specific Event Types The observer method above will receive all events for the people map, but you can also control the types of events received using event qualifiers: <markup lang=\"java\" >private void onUpdate(@Observes @Updated @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle UPDATED events raised by the 'people' map/cache } private void onChange(@Observes @Inserted @Updated @Removed @MapName(\"people\") EntryEvent&lt;?, ?&gt; event) { // handle INSERTED, UPDATED and REMOVED events raised by the 'people' map/cache } Filter Observed Events The events observed can be restricted further by using a Coherence Filter . If a filter has been specified, the events will be filtered on the server and will never be sent to the client. The filter that will be used is specified using a qualifier annotation that is itself annotated with @FilterBinding . You can implement a Custom FilterBinding (recommended), or use a built-in @WhereFilter for convenience, which allows you to specify a filter using CohQL. For example to receive all event types in the people map, but only for People with a lastName property value of Smith , the built-in @WhereFilter annotation can be used: <markup lang=\"java\" >@WhereFilter(\"lastName = 'Smith'\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { // handle all events raised by the 'people' map/cache } Transform Observed Events When an event observer does not want to receive the full cache or map value in an event, the event can be transformed into a different value to be observed. This is achieved using a MapEventTransformer that is applied to the observer method using either an ExtractorBinding annotation or a MapEventTransformerBinding annotation. Transformation of events happens on the server so can make observer&#8217;s more efficient as they do not need to receive the original event with the full old and new values. Transforming Events Using ExtractorBinding Annotations An ExtractorBinding annotation is an annotation that represents a Coherence ValueExtractor . When an observer method has been annotated with an ExtractorBinding annotation the resulting ValueExtractor is applied to the event&#8217;s values and a new event will be returned to the observer containing just the extracted properties. For example, an event observer that is observing events from a map named people , but only requires the last name, the built in @PropertyExtractor annotation can be used. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { // handle all events raised by the 'people' map/cache } Unlike the previous examples above the received events of type EntryEvent&lt;Long, Person&gt; this method will receive events of type EntryEvent&lt;Long, String&gt; because the property extractor will be applied to the Person values in the original event to extract just the lastName property, creating a new event with String values. There are a number of built in ExtractorBinding annotations, and it is also possible to create custom ExtractorBinding annotation - see the Custom ExtractorBinding Annotations section below. Multiple extractor binding annotations can be added to an injection point, in which case multiple properties will be extracted, and the event will contain a List of the extracted property values. For example, if the Person also contains an address field of type Address that contains a city field, this can be extracted with a @ChainedExtractor annotation. By combining this with the @PropertyExtractor in the example above both the lastName and city can be observed in the event. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") @ChainedExtractor({\"address\", \"city\"}) private void onMapChange(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { // handle all events raised by the 'people' map/cache } Note, now the event is of type EntryEvent&lt;Long, List&lt;String&gt;&gt; because multiple extracted values will be returned the event value is a List and in this case both properties are of tyep String , so the value can be List&lt;String&gt; . Transforming Events Using MapEventTransformerBinding Annotations If more complex event transformations are required than just extracting properties from event values, a custom MapEventTransformerBinding can be created that will produce a custom MapEventTransformer instance that will be applied to the observer&#8217;s events. See the Custom MapEventTransformerBinding Annotations section below for details on how to create MapEventTransformerBinding annotations. Observe Events for Maps and Caches in Specific Services and Scopes In addition, to the @MapName qualifier, you can also use @ServiceName and @ScopeName qualifiers as a way to limit the events received. The examples above show only how to handle EntryEvent s, but the same applies to all other server-side event types: <markup lang=\"java\" >private void onActivated(@Observes @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@Observes @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@Observes @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people' map/cache } Using Asynchronous Observers All the examples above used synchronous observers by specifying @Observes qualifier for each observer method. However, Coherence CDI fully supports asynchronous CDI observers as well. All you need to do is replace @Observes with @ObservesAsync in any of the examples above. <markup lang=\"java\" >private void onActivated(@ObservesAsync @Activated LifecycleEvent event) { // handle cache factory activation } private void onCreatedPeople(@ObservesAsync @Created @MapName(\"people\") CacheLifecycleEvent event) { // handle creation of the 'people' map/cache } private void onExecuted(@ObservesAsync @Executed @MapName(\"people\") @Processor(Uppercase.class) EntryProcessorEvent event) { // intercept 'Uppercase` entry processor execution against 'people', map/cache } Warning Coherence events fall into two categories: pre- and post-commit events. All the events whose name ends with \"ing\" , such as Inserting , Updating , Removing or Executing are pre-commit, which means that they can either modify the data or even veto the operation by throwing an exception, but in order to do so they must be synchronous to ensure that they are executed on the same thread that is executing the operation that triggered the event. That means that you can observe them using asynchronous CDI observers, but if you want to mutate the set of entries that are part of the event payload, or veto the event by throwing an exception, you must use synchronous CDI observer. Injecting CDI Beans into Transient Objects Using CDI to inject Coherence objects into your application classes, and CDI beans into Coherence-managed objects will allow you to support many use cases where dependency injection may be useful, but it doesn&#8217;t cover an important use case that is somewhat specific to Coherence. Coherence is a distributed system, and it uses serialization in order to send both the data and the processing requests from one cluster member (or remote client) to another, as well as to store data, both in memory and on disk. Processing requests, such as entry processors and aggregators, have to be deserialized on a target cluster member(s) in order to be executed. In some cases, they could benefit from dependency injection in order to avoid service lookups. Similarly, while the data is stored in a serialized, binary format, it may need to be deserialized into user supplied classes for server-side processing, such as when executing entry processors and aggregators. In this case, data classes can often also benefit from dependency injection (in order to support Domain-Driven Design (DDD), for example). While these transient objects are not managed by the CDI container, Coherence CDI does support their injection during deserialization, but for performance reasons requires that you explicitly opt-in by implementing com.oracle.coherence.cdi.Injectable interface. Making transient classes Injectable While not technically a true marker interface, Injectable can be treated as such for all intents and purposes. All you need to do is add it to the implements clause of your class in order for injection on deserialization to kick in: <markup lang=\"java\" >public class InjectableBean implements Injectable, Serializable { @Inject private Converter&lt;String, String&gt; converter; private String text; InjectableBean() { } InjectableBean(String text) { this.text = text; } String getConvertedText() { return converter.convert(text); } } Assuming that you have the following Converter service implementation in your application, it will be injected into InjectableBean during deserialization, and the getConvertedText method will return the value of the text field converted to upper case: <markup lang=\"java\" >@ApplicationScoped public class ToUpperConverter implements Converter&lt;String, String&gt; { @Override public String convert(String s) { return s.toUpperCase(); } } If your Injectable class has @PostConstruct callback method, it will be called after the injection. However, because we have no control over object&#8217;s lifecycle after that point, @PreDestroy callback will never be called). You should note that the above functionality is not dependent on the serialization format and will work with both Java and POF serialization (or any other custom serializer), and for any object that is deserialized on any Coherence member (or even on a remote client). While the deserialized transient objects are not true CDI managed beans, being able to inject CDI managed dependencies into them upon deserialization will likely satisfy most dependency injection requirements you will ever have in those application components. We hope you&#8217;ll find it useful. FilterBinding Annotations As already mentioned above, when creating views or subscribing to events, the view or events can be modified using Filters . The exact Filter implementation injected will be determined by the view or event observers qualifiers. Specifically any qualifier annotation that is itself annotated with the @FilterBinding annotation. This should be a familiar pattern to anyone who has worked with CDI interceptors. For example, if there is an injection point for a view that is a filtered view of an underlying map, but the filter required is more complex than those provided by the build in qualifiers, or is some custom filter implementation. The steps required are: Create a custom annotation class to represent the required Filter . Create a bean class implementing com.oracle.coherence.cdi.FilterFactory annotated with the custom annotation that will be the factory for producing instances of the custom Filter . Annotate the view injection point with the custom annotation. Create the Custom Filter Annotation Creating the filter annotation is simply creating a normal Java annotation class that is annotated with the @com.oracle.coherence.cdi.FilterBinding annotation. <markup lang=\"java\" >@Inherited @FilterBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomFilter { } The most important part is that this new annotation is annotated with FilterBinding so that the Coherence CDI extensions can recognise that it represents a Filter . Create the Custom Filter Factory Once the custom annotation has been created a FilterFactories implementation can be created that will be responsible for producing instances of the required Filter . <markup lang=\"java\" >@ApplicationScoped @CustomFilter static class CustomFilterSupplier implements FilterFactory&lt;CustomFilter, Object&gt; { @Override public Filter&lt;Object&gt; create(CustomFilter annotation) { return new CustomComplexFilter(); } } The CustomFilterSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomFilterSupplier class has been annotated with the new filter binding annotation @CustomFilter so that the Coherence CDI extension can locate it when it needs to create Filters . The CustomFilterSupplier implements the FilterFactories interface&#8217;s create method where it creates the custom Filter implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated FilterFactories , the injection point requiring the Filter can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomFilter private NamedMap&lt;Long, Person&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive events matching the same custom Filter then the method can be annotated with the same custom filter annotation. <markup lang=\"java\" >@CustomFilter private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, Person&gt; event) { ExtractorBinding Annotations Extractor bindings are annotations that are themselves annotated with @ExtractorBinding and are used in conjunction with an implementation of com.oracle.coherence.cdi.ExtractorFactory to produce Coherence ValueExtractor instances. There are a number of built-in extractor binding annotations in the Coherence CDI module and it is a simple process to provide custom implementations. Built-In ExtractorBinding Annotations PropertyExtractor The @PropertyExtractor annotation can used to obtain an extractor that extracts a named property from an object. The value field of the @PropertyExtractor annotation is name of the property to extract. For example, this @PropertyExtractor annotation represents a ValueExtractor that will extract the lastName property from a value. <markup lang=\"java\" >@PropertyExtractor(\"lastName\") The extractor produced will be an instance of com.tangosol.util.extractor.UniversalExtractor , so the example above is the same as calling: <markup lang=\"java\" >new UniversalExtractor(\"lastName\"); The @PropertyExtractor annotation can be applied multiple times to create a MultiExtractor that will extract a List of properties from a value. For example, if there was a map named people , where the map values are instances of Person , that has a firstName and a lastName property. The event observer below would observe events on that map, but the event received would only contain the event key, and a List containing the extracted firstName and lastName from the original event. where the event values will be a list of <markup lang=\"java\" >@PropertyExtractor(\"firstName\") @PropertyExtractor(\"lastName\") private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, List&lt;String&gt;&gt; event) { ChainedExtractor The @ChainedExtractor annotation can be used to extract a chain of properties. For example, a Person instance might contain an address property that contains a city property. The @ChainedExtractor takes the chain of fields to be extracted, in this case, extract the address from Person and then extract the city from the address . <markup lang=\"java\" >@ChainedExtractor(\"address\", \"city\") Each of the property names is used to create a UniversalExtractor , and the array of these extractors is used to create an instance of com.tangosol.util.extractor.ChainedExtractor . The example above would be the same as calling: <markup lang=\"java\" >UniversalExtractor[] chain = new UniversalExtractor[] { new UniversalExtractor(\"address\"), new UniversalExtractor(\"city\") }; ChainedExtractor extractor = new ChainedExtractor(chain); PofExtractor The @PofExtractor annotation can be used to produce extractors that can extract properties from POF encoded values. The value passed to the @PofExtractor annotation is the POF path to navigate to the property to extract. For example, if a Person value has been serialized using POF with a lastName property at index 4 a @PofExtractor annotation could be used like this: <markup lang=\"java\" >@PofExtractor(index = 4) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(null, 4); Sometimes (for example when dealing with certain types of Number ) the PofExtractor needs to know they type to be extracted. In this case the type value can be set in the @PofExtractor annotation. For example, if a Book value had a sales field of type Long at POF index 2, the sales field could be extracted using the following @PofExtractor annotation: <markup lang=\"java\" >@PofExtractor(index = {2}, type = Long.class) The code above will create a Coherence com.tangosol.util.extractor.PofExtractor equivalent to calling: <markup lang=\"java\" >com.tangosol.util.extractor.PofExtractor(Long.class, 2); The index value for a @PofExtractor annotation is an int array so multiple POF index values can be passed to navigate down a chain of properties to extract. For example if Person contained an Address at POF index 5 and Address contained a city property at POF index 3 the city could be extracted from a Person using the @PofExtractor annotation like this: <markup lang=\"java\" >@PofExtractor(index = {5, 3}) Alternatively if the value that will be extracted from is annotated with com.tangosol.io.pof.schema.annotation.PortableType and the POF serialization code for the class has been generated using the Coherence com.tangosol.io.pof.generator.PortableTypeGenerator then property names can be passed to the @PofExtractor annotation using its path field. For example to extract the lastName field from a POF serialized Person the @PofExtractor annotation can be used like this: <markup lang=\"java\" >@PofExtractor(path = \"lastName\") the address city example would be: <markup lang=\"java\" >@PofExtractor(path = {\"address\", \"city\"}) and the Book sales example would be: <markup lang=\"java\" >@PofExtractor(path = \"sales\", type Long.class) Custom ExtractorBinding Annotations When the built-in extractor bindings are not suitable, or when a custom ValueExtractor implementation is required, then a custom extractor binding annotation can be created with a corresponding com.oracle.coherence.cdi.ExtractorFactory implementation. The steps required are: Create a custom annotation class to represent the required ValueExtractor . Create a bean class implementing com.oracle.coherence.cdi.ExtractorFactory annotated with the custom annotation that will be the factory for producing instances of the custom ValueExtractor . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.ExtractorBinding annotation. <markup lang=\"java\" >@Inherited @ExtractorBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomExtractor { } The most important part is that this new annotation has been annotated with ExtractorBinding so that the Coherence CDI extensions can recognise that it represents a ValueExtractor . Create the Custom Extractor Factory Once the custom annotation has been created an ExtractorFactory implementation can be created that will be responsible for producing instances of the required ValueExtractor . <markup lang=\"java\" >@ApplicationScoped @CustomExtractor static class CustomExtractorSupplier implements ExtractorFactory&lt;CustomExtractor, Object, Object&gt; { @Override public ValueExtractor&lt;Object, Object&gt; create(CustomExtractor annotation) { return new CustomComplexExtractor(); } } The CustomExtractorSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomExtractorSupplier class has been annotated with the new extractor binding annotation @CustomExtractor so that the Coherence CDI extension can locate it when it needs to create ValueExtractor instances. The CustomExtractorSupplier implements the ExtractorFactory interface&#8217;s create method where it creates the custom ValueExtractor implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated ExtractorFactory , the injection point requiring the ValueExtractor can be annotated with the new annotation. <markup lang=\"java\" >@Inject @View @CustomExtractor private NamedMap&lt;Long, String&gt; people; As well as views, custom filter binding annotations can also be used for event observers. For example if there is an event observer method that should only receive transformed events using the custom extractor to transform the event: <markup lang=\"java\" >@CustomExtractor private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { MapEventTransformerBinding Annotations Coherence CDI supports event observers that can observe events for cache, or map, entries (see the Events section). The observer method can be annotated with a MapEventTransformerBinding annotation to indicate that the observer requires a transformer to be applied to the original event before it is observed. There are no built-in MapEventTransformerBinding annotations, this feature is to support use of custom MapEventTransformer implementations. The steps to create and use a MapEventTransformerBinding annotation are: Create a custom annotation class to represent the required MapEventTransformer . Create a bean class implementing com.oracle.coherence.cdi.MapEventTransformerFactory annotated with the custom annotation that will be the factory for producing instances of the custom MapEventTransformer . Annotate the view injection point with the custom annotation. Create the Custom Extractor Annotation Creating the extractor annotation is simply creating a normal Java annotation class which is annotated with the @com.oracle.coherence.cdi.MapEventTransformerBinding annotation. <markup lang=\"java\" >@Inherited @MapEventTransformerBinding @Documented @Retention(RetentionPolicy.RUNTIME) public @interface CustomTransformer { } The most important part is that this new annotation has been annotated with MapEventTransformerBinding so that the Coherence CDI extensions can recognise that it represents a MapEventTransformer . Create the Custom Extractor Factory Once the custom annotation has been created an MapEventTransformerFactory implementation can be created that will be responsible for producing instances of the required MapEventTransformer . <markup lang=\"java\" >@ApplicationScoped @CustomTransformer static class CustomTransformerSupplier implements MapEventTransformerFactory&lt;CustomTransformer, Object, Object, Object&gt; { @Override public MapEventTransformer&lt;Object, Object, Object&gt; create(CustomTransformer annotation) { return new CustomComplexTransformer(); } } The CustomTransformerSupplier class has been annotated with @ApplicationScoped to make is discoverable by CDI. The CustomTransformerSupplier class has been annotated with the new extractor binding annotation @CustomTransformer so that the Coherence CDI extension can locate it when it needs to create MapEventTransformer instances. The CustomTransformerSupplier implements the MapEventTransformerFactory interface&#8217;s create method where it creates the custom MapEventTransformer implementation. Annotate the Injection Point Now there is both a custom annotation, and an annotated MapEventTransformerFactory , the observer method requiring the MapEventTransformer can be annotated with the new annotation. <markup lang=\"java\" >@CustomTransformer private void onPerson(@Observes @MapName(\"people\") EntryEvent&lt;Long, String&gt; event) { ",
            "title": "Usage"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Tests Review the classes Review the cache configuration Run the Tests Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " This example comprises a number of tests showing various server-side events features. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "Running the Examples"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " In this example you will run a number of tests that demonstrate the following features of server-side events including: Understanding where to declare interceptors in your cache config Listening for cache events related to mutations of cache data, and execution of entry processors Listening for transfer events related to partition transfers and loss events Listening for partitioned cache events related to creation, destruction and truncating of caches Listening for lifecycle events for ConfigurableCacheFactory instantiation What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. Running the Examples This example comprises a number of tests showing various server-side events features. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " Review the Customer class. Some tests use the Customer class which has the following fields: <markup lang=\"java\" >private int id; private String name; private String address; private String customerType; private long creditLimit; Review the AuditEvent class. Some tests use the AuditEvent class which has the following fields: <markup lang=\"java\" >/** * Unique Id for the audit event. */ private UUID id; /** * The target of the event such as cache, partition, etc. */ private String target; /** * The type of event. */ private String eventType; /** * Specific event data. */ private String eventData; /** * Time of the event. */ private long eventTime; Review the AuditingInterceptor which audits any mutations to caches using post-commit events. See here for details of all Partitioned Cache events. <markup lang=\"java\" >@Interceptor(identifier = \"AuditingInterceptor\", order = Interceptor.Order.HIGH) @EntryEvents({EntryEvent.Type.INSERTED, EntryEvent.Type.UPDATED, EntryEvent.Type.REMOVED}) public class AuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryEvent&lt;?, ?&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;?, ?&gt; event) { String oldValue = null; String newValue = null; EntryEvent.Type eventType = event.getType(); Object key = event.getKey(); if (eventType == EntryEvent.Type.REMOVED || eventType == EntryEvent.Type.UPDATED) { oldValue = event.getOriginalValue().toString(); } if (eventType == EntryEvent.Type.INSERTED || eventType == EntryEvent.Type.UPDATED) { newValue = event.getValue().toString(); } AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), eventType.toString(), String.format(\"key=%s, old=%s, new=%s\", key, oldValue, newValue)); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name and optional order of HIGH or LOW as the priority Identifies the EntryEvents that will be intercepted. INSERTED, UPDATED and REMOVED are raised asynchronously after the event has happened Identifies the type of events, in this case EntryEvents Overrides method to respond to the event Identifies the type of event and sets the payload accordingly Adds the audit event to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link AuditingInterceptor} which will audit any changes to caches * that fall thought and match the '*' cache-mapping. */ @Test public void testAuditingInterceptor() { System.out.println(\"testAuditingInterceptor\"); CoherenceClusterMember member = getMember1(); // create two different caches to be audited which will match to the auditing-scheme NamedCache&lt;Integer, String&gt; cache1 = member.getCache(\"test-cache\"); NamedCache&lt;Integer, Customer&gt; cache2 = member.getCache(\"test-customer\"); cache1.truncate(); cache2.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // generate some mutations that will be audited cache1.put(1, \"one\"); cache1.put(2, \"two\"); cache1.put(1, \"ONE\"); cache1.remove(1); dumpAuditEvents(\"testAuditingInterceptor-1\"); // ensure 3 inserts and 1 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); auditEvents.clear(); // generate new set of mutations for customers cache2.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10000)); cache2.put(2, new Customer(2, \"John\", \"Address 2\", Customer.SILVER, 4000)); cache2.clear(); dumpAuditEvents(\"testAuditingInterceptor-2\"); // ensure 2 insert and 2 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.values().size(), Matchers.is(4)); } Review the EntryProcessorAuditingInterceptor which audits entry processors executions using post-commit events. <markup lang=\"java\" >@Interceptor(identifier = \"EntryProcessorAuditingInterceptor\") @EntryProcessorEvents({EntryProcessorEvent.Type.EXECUTED}) public class EntryProcessorAuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryProcessorEvent&gt;, Serializable { @Override public void onEvent(EntryProcessorEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Entries=%d, processor=%s\", event.getEntrySet().size(), event.getProcessor().toString())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the EntryProcessorEvents that will be intercepted. EXECUTED event is raised asynchronously after the event has happened Identifies the type of events, in this case EntryProcessorEvents Overrides method to respond to the event and add to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link EntryProcessorAuditingInterceptor} which will audit any entry processor * executions on caches that match the '*' cache-mapping. */ @Test public void testEntryProcessorInterceptor() { System.out.println(\"testEntryProcessorInterceptor\"); CoherenceClusterMember member = getMember1(); // create a cache to audit entry processor events on NamedCache&lt;Integer, Customer&gt; cache = member.getCache(\"test-customer\"); cache.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // add some entries cache.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10_000)); cache.put(2, new Customer(2, \"Tom\", \"Address 2\", Customer.SILVER, 10_000)); cache.put(3, new Customer(3, \"Helen\", \"Address 3\", Customer.BRONZE, 10_000)); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(3)); auditEvents.clear(); cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); // 2 entry processor events and 3 updates Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(5)); dumpAuditEvents(\"testEntryProcessorInterceptor-1\"); auditEvents.clear(); // invoke an entry processor across all customers to update credit limit to 100,000 cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); cache.invoke(1, Processors.update(Customer::setCreditLimit, 100_000L)); // ensure all audit events are received Eventually.assertDeferred(() -&gt; auditEvents.values(equal(AuditEvent::getEventType, \"EXECUTED\")).size(), Matchers.is(3)); dumpAuditEvents(\"testEntryProcessorInterceptor-2\"); } Review the UppercaseInterceptor which changes the name and address attributes to uppercase. <markup lang=\"java\" >@Interceptor(identifier = \"UppercaseInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class UppercaseInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customer = entry.getValue(); customer.setName(customer.getName().toUpperCase()); customer.setAddress(customer.getAddress().toUpperCase()); entry.setValue(customer); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Ensures the changes are persisted by calling entry.setValue() This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link UppercaseInterceptor} which is defined on the 'customers' cache only, * to update name and address fields to uppercase. */ @Test public void testCustomerUppercaseInterceptor() { System.out.println(\"testCustomerUppercaseInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // put a new Customer with lowercase names and addresses customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.GOLD, 10000L)); // validate that the name and address are uppercase Customer customer = customers.get(1); assertEquals(customer.getName(), \"TIM\"); assertEquals(customer.getAddress(), \"123 JAMES STREET, PERTH\"); // update a customers name and ensure that it is updated to uppercase customers.invoke(1, Processors.update(Customer::setName, \"timothy\")); assertEquals(customers.get(1).getName(), \"TIMOTHY\"); } Review the ValidationInterceptor which rejects or accepts changes based upon some simple business rules. <markup lang=\"java\" >@Interceptor(identifier = \"ValidationInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class ValidationInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customerOld = entry.getOriginalValue(); Customer customerNew = entry.getValue(); EntryEvent.Type eventType = event.getType(); if (eventType == EntryEvent.Type.INSERTING) { // Rule 1 - New customers cannot have credit limit above 1,000,000 unless they are GOLD if (customerNew.getCreditLimit() &gt;= 1_000_000L &amp;&amp; !customerNew.getCustomerType().equals(Customer.GOLD)) { // reject the update throw new RuntimeException(\"Only gold customers may have credit limits above 1,000,000\"); } } else if (eventType == EntryEvent.Type.UPDATING) { // Rule 2 - Cannot change customer type from BRONZE directly to GOLD, must go BRONZE -&gt; SILVER -&gt; GOLD if (customerNew.getCustomerType().equals(Customer.GOLD) &amp;&amp; customerOld.getCustomerType().equals(Customer.BRONZE)) { // reject the update throw new RuntimeException(\"Cannot update customer directly to GOLD from BRONZE\"); } } // otherwise, continue with update entry.setValue(customerNew); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Validates the first business rule if the event is an insert. If the rule fails, then throw a RuntimeException Validates the second business rule if the event is an update. If the rule fails, then throw a RuntimeException Saves the entry if all the business rules pass This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link ValidationInterceptor} which will reject updates if business rules fail. */ @Test public void testValidatingInterceptor() { System.out.println(\"testValidatingInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // try adding a BRONZE customer with credit limit &gt; 1,000,000 try { customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 2_000_000L)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Put was correctly rejected: %s\", e.getMessage()); } // should be rejected assertEquals(customers.size(), 0); // add a normal BRONZE customer, should succeed with credit limit 10,000 customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 10_000L)); assertEquals(customers.size(), 1); // try and update credit limit to GOLD from BRONZE, should fail try { customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Update was correctly rejected: %s\", e.getMessage()); } assertEquals(customers.get(1).getCustomerType(), Customer.BRONZE); } Review the TransferEventsInterceptor which audits partition transfer events. <markup lang=\"java\" >@Interceptor(identifier = \"TransferEventsInterceptor\") @TransferEvents({TransferEvent.Type.ARRIVED, TransferEvent.Type.DEPARTING, TransferEvent.Type.LOST}) public class TransferEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;TransferEvent&gt;, Serializable { @Override public void onEvent(TransferEvent event) { AuditEvent auditEvent = new AuditEvent(\"partition=\" + event.getPartitionId(), event.getType().toString(), String.format(\"Partitions from remote member %s\", event.getRemoteMember())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the TransferEvents that will be intercepted. Transfer events are raised while holding a lock on the partition being transferred that blocks any operations for the partition. Identifies the type of events, in this case TransferEvents Overrides method to respond to the event This is used in the following test in ServerPartitionEventsTest : <markup lang=\"java\" >@Test public void testPartitionEvents() { System.out.println(\"testPartitionEvents\"); CoherenceClusterMember member1 = getMember1(); CoherenceClusterMember member2 = getMember2(); NamedCache&lt;Integer, String&gt; cache = member1.getCache(\"test-cache\"); for (int i = 0; i &lt; 10; i++) { cache.put(i, \"value-\" + i); } // ensure all audit events are received = 10 insert events plus 2 cache created events Eventually.assertDeferred(()-&gt;auditEvents.size(), Matchers.is(12)); // shutdown the second member member2.close(); // wait for additional partition events to be received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.greaterThan(16)); dumpAuditEvents(\"testPartitionEvents\"); } Review the CacheLifecycleEventsInterceptor which audits cache lifecycle events. <markup lang=\"java\" >@Interceptor(identifier = \"CacheLifecycleEventsInterceptor\") @CacheLifecycleEvents( {CacheLifecycleEvent.Type.CREATED, CacheLifecycleEvent.Type.DESTROYED, CacheLifecycleEvent.Type.TRUNCATED}) public class CacheLifecycleEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;CacheLifecycleEvent&gt;, Serializable { @Override public void onEvent(CacheLifecycleEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Event from service %s\", event.getServiceName())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the CacheLifecycleEvent that will be intercepted. CREATED, DESTROYED and TRUNCATED are raised asynchronously after the operation is completed. Identifies the type of events, in this case CacheLifecycleEvent Overrides method to respond to the event This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >@Test public void testTruncate() { System.out.println(\"testTruncate\"); auditEvents.clear(); NamedCache&lt;Integer, String&gt; cache1 = getMember1().getCache(\"test-cache\"); cache1.truncate(); // ensure we get two events, one from each storage node Eventually.assertDeferred(() -&gt; auditEvents.values(equal(AuditEvent::getEventType, \"TRUNCATED\")).size(), Matchers.is(2)); dumpAuditEvents(\"truncate\"); } ",
            "title": "Review the classes"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " The interceptors are added via cache config and can be applied at the service or cache level. Review the Cache Scheme Mapping <markup lang=\"xml\" > &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;LifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.LifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;customers&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;UppercaseInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.UppercaseInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;ValidationInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.ValidationInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;!-- cache to store auditing events --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;audit-events&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;!-- any caches other than are defined above will be audited --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; Defines an interceptor called LifecycleEventsInterceptor to log any ConfigurableCacheFactory events. Defines customers cache which has the UppercaseInterceptor and ValidationInterceptor enabled for only this cache Review the Caching Schemes <markup lang=\"xml\" >&lt;!-- Any caches in this scheme will be audited and data put in \"audit-events\" cache. --&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCacheAudit&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;AuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.AuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;EntryProcessorAuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.EntryProcessorAuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;TransferEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.TransferEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;CacheLifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.CacheLifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; Defines auditing-scheme which has the AuditingInterceptor , EntryProcessorAuditingInterceptor , CacheLifecycleEventsInterceptor and TransferEventsInterceptor enabled for any caches using this scheme. ",
            "title": "Review the cache config"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " The example code comprises a number of classes: Tests ServerCacheEventsTest - tests for various cache events ServerPartitionEventsTest - tests for partition based events AbstractEventsTest - a class used by both tests which starts the clusters Model Customer - represents a fictional customer AuditEvent - represents an audit event Interceptors AuditingInterceptor - creates audit events after inserts, updates or removes on a cache EntryProcessorAuditingInterceptor - creates audit events after entry processor executions UppercaseInterceptor - a mutating interceptor that changes the name and address attributes to uppercase ValidationInterceptor - a mutating interceptor that optionally rejects updates if certain business rules are not met TransferEventsInterceptor - creates audit events after any partition transfers made CacheLifecycleEventsInterceptor - creates audit events after caches are created, truncated or destroyed LifecycleEventsInterceptor - logs a message when ConfigurableCacheFactories are activated or destroyed Review the classes Review the Customer class. Some tests use the Customer class which has the following fields: <markup lang=\"java\" >private int id; private String name; private String address; private String customerType; private long creditLimit; Review the AuditEvent class. Some tests use the AuditEvent class which has the following fields: <markup lang=\"java\" >/** * Unique Id for the audit event. */ private UUID id; /** * The target of the event such as cache, partition, etc. */ private String target; /** * The type of event. */ private String eventType; /** * Specific event data. */ private String eventData; /** * Time of the event. */ private long eventTime; Review the AuditingInterceptor which audits any mutations to caches using post-commit events. See here for details of all Partitioned Cache events. <markup lang=\"java\" >@Interceptor(identifier = \"AuditingInterceptor\", order = Interceptor.Order.HIGH) @EntryEvents({EntryEvent.Type.INSERTED, EntryEvent.Type.UPDATED, EntryEvent.Type.REMOVED}) public class AuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryEvent&lt;?, ?&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;?, ?&gt; event) { String oldValue = null; String newValue = null; EntryEvent.Type eventType = event.getType(); Object key = event.getKey(); if (eventType == EntryEvent.Type.REMOVED || eventType == EntryEvent.Type.UPDATED) { oldValue = event.getOriginalValue().toString(); } if (eventType == EntryEvent.Type.INSERTED || eventType == EntryEvent.Type.UPDATED) { newValue = event.getValue().toString(); } AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), eventType.toString(), String.format(\"key=%s, old=%s, new=%s\", key, oldValue, newValue)); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name and optional order of HIGH or LOW as the priority Identifies the EntryEvents that will be intercepted. INSERTED, UPDATED and REMOVED are raised asynchronously after the event has happened Identifies the type of events, in this case EntryEvents Overrides method to respond to the event Identifies the type of event and sets the payload accordingly Adds the audit event to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link AuditingInterceptor} which will audit any changes to caches * that fall thought and match the '*' cache-mapping. */ @Test public void testAuditingInterceptor() { System.out.println(\"testAuditingInterceptor\"); CoherenceClusterMember member = getMember1(); // create two different caches to be audited which will match to the auditing-scheme NamedCache&lt;Integer, String&gt; cache1 = member.getCache(\"test-cache\"); NamedCache&lt;Integer, Customer&gt; cache2 = member.getCache(\"test-customer\"); cache1.truncate(); cache2.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // generate some mutations that will be audited cache1.put(1, \"one\"); cache1.put(2, \"two\"); cache1.put(1, \"ONE\"); cache1.remove(1); dumpAuditEvents(\"testAuditingInterceptor-1\"); // ensure 3 inserts and 1 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); auditEvents.clear(); // generate new set of mutations for customers cache2.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10000)); cache2.put(2, new Customer(2, \"John\", \"Address 2\", Customer.SILVER, 4000)); cache2.clear(); dumpAuditEvents(\"testAuditingInterceptor-2\"); // ensure 2 insert and 2 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.values().size(), Matchers.is(4)); } Review the EntryProcessorAuditingInterceptor which audits entry processors executions using post-commit events. <markup lang=\"java\" >@Interceptor(identifier = \"EntryProcessorAuditingInterceptor\") @EntryProcessorEvents({EntryProcessorEvent.Type.EXECUTED}) public class EntryProcessorAuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryProcessorEvent&gt;, Serializable { @Override public void onEvent(EntryProcessorEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Entries=%d, processor=%s\", event.getEntrySet().size(), event.getProcessor().toString())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the EntryProcessorEvents that will be intercepted. EXECUTED event is raised asynchronously after the event has happened Identifies the type of events, in this case EntryProcessorEvents Overrides method to respond to the event and add to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link EntryProcessorAuditingInterceptor} which will audit any entry processor * executions on caches that match the '*' cache-mapping. */ @Test public void testEntryProcessorInterceptor() { System.out.println(\"testEntryProcessorInterceptor\"); CoherenceClusterMember member = getMember1(); // create a cache to audit entry processor events on NamedCache&lt;Integer, Customer&gt; cache = member.getCache(\"test-customer\"); cache.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // add some entries cache.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10_000)); cache.put(2, new Customer(2, \"Tom\", \"Address 2\", Customer.SILVER, 10_000)); cache.put(3, new Customer(3, \"Helen\", \"Address 3\", Customer.BRONZE, 10_000)); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(3)); auditEvents.clear(); cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); // 2 entry processor events and 3 updates Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(5)); dumpAuditEvents(\"testEntryProcessorInterceptor-1\"); auditEvents.clear(); // invoke an entry processor across all customers to update credit limit to 100,000 cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); cache.invoke(1, Processors.update(Customer::setCreditLimit, 100_000L)); // ensure all audit events are received Eventually.assertDeferred(() -&gt; auditEvents.values(equal(AuditEvent::getEventType, \"EXECUTED\")).size(), Matchers.is(3)); dumpAuditEvents(\"testEntryProcessorInterceptor-2\"); } Review the UppercaseInterceptor which changes the name and address attributes to uppercase. <markup lang=\"java\" >@Interceptor(identifier = \"UppercaseInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class UppercaseInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customer = entry.getValue(); customer.setName(customer.getName().toUpperCase()); customer.setAddress(customer.getAddress().toUpperCase()); entry.setValue(customer); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Ensures the changes are persisted by calling entry.setValue() This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link UppercaseInterceptor} which is defined on the 'customers' cache only, * to update name and address fields to uppercase. */ @Test public void testCustomerUppercaseInterceptor() { System.out.println(\"testCustomerUppercaseInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // put a new Customer with lowercase names and addresses customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.GOLD, 10000L)); // validate that the name and address are uppercase Customer customer = customers.get(1); assertEquals(customer.getName(), \"TIM\"); assertEquals(customer.getAddress(), \"123 JAMES STREET, PERTH\"); // update a customers name and ensure that it is updated to uppercase customers.invoke(1, Processors.update(Customer::setName, \"timothy\")); assertEquals(customers.get(1).getName(), \"TIMOTHY\"); } Review the ValidationInterceptor which rejects or accepts changes based upon some simple business rules. <markup lang=\"java\" >@Interceptor(identifier = \"ValidationInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class ValidationInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customerOld = entry.getOriginalValue(); Customer customerNew = entry.getValue(); EntryEvent.Type eventType = event.getType(); if (eventType == EntryEvent.Type.INSERTING) { // Rule 1 - New customers cannot have credit limit above 1,000,000 unless they are GOLD if (customerNew.getCreditLimit() &gt;= 1_000_000L &amp;&amp; !customerNew.getCustomerType().equals(Customer.GOLD)) { // reject the update throw new RuntimeException(\"Only gold customers may have credit limits above 1,000,000\"); } } else if (eventType == EntryEvent.Type.UPDATING) { // Rule 2 - Cannot change customer type from BRONZE directly to GOLD, must go BRONZE -&gt; SILVER -&gt; GOLD if (customerNew.getCustomerType().equals(Customer.GOLD) &amp;&amp; customerOld.getCustomerType().equals(Customer.BRONZE)) { // reject the update throw new RuntimeException(\"Cannot update customer directly to GOLD from BRONZE\"); } } // otherwise, continue with update entry.setValue(customerNew); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Validates the first business rule if the event is an insert. If the rule fails, then throw a RuntimeException Validates the second business rule if the event is an update. If the rule fails, then throw a RuntimeException Saves the entry if all the business rules pass This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link ValidationInterceptor} which will reject updates if business rules fail. */ @Test public void testValidatingInterceptor() { System.out.println(\"testValidatingInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // try adding a BRONZE customer with credit limit &gt; 1,000,000 try { customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 2_000_000L)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Put was correctly rejected: %s\", e.getMessage()); } // should be rejected assertEquals(customers.size(), 0); // add a normal BRONZE customer, should succeed with credit limit 10,000 customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 10_000L)); assertEquals(customers.size(), 1); // try and update credit limit to GOLD from BRONZE, should fail try { customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Update was correctly rejected: %s\", e.getMessage()); } assertEquals(customers.get(1).getCustomerType(), Customer.BRONZE); } Review the TransferEventsInterceptor which audits partition transfer events. <markup lang=\"java\" >@Interceptor(identifier = \"TransferEventsInterceptor\") @TransferEvents({TransferEvent.Type.ARRIVED, TransferEvent.Type.DEPARTING, TransferEvent.Type.LOST}) public class TransferEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;TransferEvent&gt;, Serializable { @Override public void onEvent(TransferEvent event) { AuditEvent auditEvent = new AuditEvent(\"partition=\" + event.getPartitionId(), event.getType().toString(), String.format(\"Partitions from remote member %s\", event.getRemoteMember())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the TransferEvents that will be intercepted. Transfer events are raised while holding a lock on the partition being transferred that blocks any operations for the partition. Identifies the type of events, in this case TransferEvents Overrides method to respond to the event This is used in the following test in ServerPartitionEventsTest : <markup lang=\"java\" >@Test public void testPartitionEvents() { System.out.println(\"testPartitionEvents\"); CoherenceClusterMember member1 = getMember1(); CoherenceClusterMember member2 = getMember2(); NamedCache&lt;Integer, String&gt; cache = member1.getCache(\"test-cache\"); for (int i = 0; i &lt; 10; i++) { cache.put(i, \"value-\" + i); } // ensure all audit events are received = 10 insert events plus 2 cache created events Eventually.assertDeferred(()-&gt;auditEvents.size(), Matchers.is(12)); // shutdown the second member member2.close(); // wait for additional partition events to be received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.greaterThan(16)); dumpAuditEvents(\"testPartitionEvents\"); } Review the CacheLifecycleEventsInterceptor which audits cache lifecycle events. <markup lang=\"java\" >@Interceptor(identifier = \"CacheLifecycleEventsInterceptor\") @CacheLifecycleEvents( {CacheLifecycleEvent.Type.CREATED, CacheLifecycleEvent.Type.DESTROYED, CacheLifecycleEvent.Type.TRUNCATED}) public class CacheLifecycleEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;CacheLifecycleEvent&gt;, Serializable { @Override public void onEvent(CacheLifecycleEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Event from service %s\", event.getServiceName())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the CacheLifecycleEvent that will be intercepted. CREATED, DESTROYED and TRUNCATED are raised asynchronously after the operation is completed. Identifies the type of events, in this case CacheLifecycleEvent Overrides method to respond to the event This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >@Test public void testTruncate() { System.out.println(\"testTruncate\"); auditEvents.clear(); NamedCache&lt;Integer, String&gt; cache1 = getMember1().getCache(\"test-cache\"); cache1.truncate(); // ensure we get two events, one from each storage node Eventually.assertDeferred(() -&gt; auditEvents.values(equal(AuditEvent::getEventType, \"TRUNCATED\")).size(), Matchers.is(2)); dumpAuditEvents(\"truncate\"); } Review the cache config The interceptors are added via cache config and can be applied at the service or cache level. Review the Cache Scheme Mapping <markup lang=\"xml\" > &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;LifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.LifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;customers&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;UppercaseInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.UppercaseInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;ValidationInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.ValidationInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;!-- cache to store auditing events --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;audit-events&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;!-- any caches other than are defined above will be audited --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; Defines an interceptor called LifecycleEventsInterceptor to log any ConfigurableCacheFactory events. Defines customers cache which has the UppercaseInterceptor and ValidationInterceptor enabled for only this cache Review the Caching Schemes <markup lang=\"xml\" >&lt;!-- Any caches in this scheme will be audited and data put in \"audit-events\" cache. --&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCacheAudit&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;AuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.AuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;EntryProcessorAuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.EntryProcessorAuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;TransferEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.TransferEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;CacheLifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.CacheLifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; Defines auditing-scheme which has the AuditingInterceptor , EntryProcessorAuditingInterceptor , CacheLifecycleEventsInterceptor and TransferEventsInterceptor enabled for any caches using this scheme. ",
            "title": "Review the Tests"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.serverevents.ServerPartitionEventsTest com.oracle.coherence.guides.serverevents.ServerCacheEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code. Output has been truncated and formatted for easier reading. testPartitions Output <markup lang=\"bash\" >testPartitionEvents Dumping the audit events testPartitionEvents AuditEvent{id=2E1E1FE69E, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209123} AuditEvent{id=54A54A5CED, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209129} AuditEvent{id=AAA54A5CEE, target='cache=test-cache', eventType='INSERTED', eventData='key=0, old=null, new=value-0', eventTime=1652255209135} AuditEvent{id=A51E1FE69F, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=value-1', eventTime=1652255209141} ... AuditEvent{id=A1A54A5CF3, target='cache=test-cache', eventType='INSERTED', eventData='key=9, old=null, new=value-9', eventTime=1652255209169} ... AuditEvent{id=961E1FE6A3, target='partition=0', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209572} AuditEvent{id=261E1FE6A4, target='partition=1', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209580} ... AuditEvent{id=531E1FE6B1, target='partition=14', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209587} Lifecycle events from creation of cache from two storage nodes Insert events for cache entries Partitions arriving from remove member before shutdown testTruncate Output <markup lang=\"bash\" >testTruncate Dumping the audit events truncate AuditEvent{id=B8127D2701, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218772} AuditEvent{id=6BD64A90EA, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218784} AuditEvent{id=7E127D2702, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218802} AuditEvent{id=17D64A90EB, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218806} Both CREATED and TRUNCATED events are shown. testEntryProcessorInterceptor Output <markup lang=\"bash\" >testEntryProcessorInterceptor Dumping the audit events testEntryProcessorInterceptor-1 AuditEvent{id=AE5BC2D3EB, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(Customer$setCreditLimit...', eventTime=1652319479550} AuditEvent{id=C25BC2D3EC, target='cache=test-customer', eventType='UPDATED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=100000}', eventTime=1652319479553} AuditEvent{id=3D82ADF7F7, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(Customer$setCreditLimit...'}}, arguments=[]}}, 100000)', eventTime=1652319479553} AuditEvent{id=4382ADF7F8, target='cache=test-customer', eventType='UPDATED', eventData='key=2, old=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=10000}, new=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=100000}', eventTime=1652319479556} AuditEvent{id=575BC2D3ED, target='cache=test-customer', eventType='UPDATED', eventData='key=3, old=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=10000}, new=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=100000}', eventTime=1652319479556} Dumping the audit events testEntryProcessorInterceptor-2 AuditEvent{id=F05BC2D3EE, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479577} AuditEvent{id=7982ADF7F9, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479578} AuditEvent{id=235BC2D3EF, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479584} Three insert events and two entry processor events. One from each storage-enabled node Three entry processor events, one for an individual invoke() on a key and two from the invokeAll as per item 1 testValidatingInterceptor Output testValidatingInterceptor Output <markup lang=\"bash\" >Put was correctly rejected: Failed to execute [put] with arguments [1, Customer{id=1, name='tim', address='123 james street, perth', customerType='BRONZE', balance=2000000}] Update was correctly rejected: Failed to execute [invoke] with arguments [1, UpdaterProcessor(com.oracle.coherence.guides.serverevents.ServerCacheEventsTest$$Lambda$475/0x00000008003da040@783ecb80, GOLD)] testCustomerUppercaseInterceptor Messages from rejected updates testAuditingInterceptor Output <markup lang=\"bash\" >testAuditingInterceptor Dumping the audit events testAuditingInterceptor-1 AuditEvent{id=1D127D270E, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=one', eventTime=1652255219418} AuditEvent{id=25D64A90F4, target='cache=test-cache', eventType='INSERTED', eventData='key=2, old=null, new=two', eventTime=1652255219428} AuditEvent{id=A5127D270F, target='cache=test-cache', eventType='UPDATED', eventData='key=1, old=one, new=ONE', eventTime=1652255219432} AuditEvent{id=EF127D2710, target='cache=test-cache', eventType='REMOVED', eventData='key=1, old=ONE, new=null', eventTime=1652255219436} Dumping the audit events testAuditingInterceptor-2 AuditEvent{id=A5127D2711, target='cache=test-customer', eventType='INSERTED', eventData='key=1, old=null, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}', eventTime=1652255219456} AuditEvent{id=5BD64A90F5, target='cache=test-customer', eventType='INSERTED', eventData='key=2, old=null, new=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}', eventTime=1652255219460} AuditEvent{id=CAD64A90F6, target='cache=test-customer', eventType='REMOVED', eventData='key=2, old=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}, new=null', eventTime=1652255219466} AuditEvent{id=27127D2712, target='cache=test-customer', eventType='REMOVED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=null', eventTime=1652255219466} Two inserts, one update and a remove Two inserts and two removes as a result of clear() ",
            "title": "Run the Tests"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " In this guide we walked you through how to use server-side events within Coherence to listen for various events on a Coherence NamedMap or NamedCache . ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " Develop Applications using Server Side Events Client Side Events ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/142-server-events/README",
            "text": " This guide walks you through how to use server-side events, (also known as \"Live Events\"), within Coherence to listen for various events on a Coherence NamedMap or NamedCache . Coherence provides an event programming model that allows extensibility within a cluster when performing operations against a data grid. The model uses events to represent observable occurrences of cluster operations. The events that are currently supported include: Partitioned Cache Events – A set of events that represent the operations being performed against a set of entries in a cache. Partitioned cache events include both entry events and entry processor events. Entry events are related to inserting, removing, and updating entries in a cache. Entry processor events are related to the execution of entry processors. Partitioned Cache Lifecycle Events – A set of events that represent the operations for creating a cache, destroying a cache, and clearing all entries from a cache. Partitioned Service Events – A set of events that represent the operations being performed by a partitioned service. Partitioned service events include both partition transfer events and partition transaction events. Partition transfer events are related to the movement of partitions among cluster members. Partition transaction events are related to changes that may span multiple caches and are performed within the context of a single request. Lifecycle Events – A set of events that represent the activation and disposal of a ConfigurableCacheFactory instance. Federation Events – A set of events that represent the operations being performed by a federation service. Federation events include both Federated connection events and federated change events. Federated connection events are related to the interaction of federated participants and federated change events are related to cache updates. In this example we will not cover Federation Events. Events are registered in the cache configuration against either a cache service or individual caches via cache mappings. The classes are annotated to identify what types of events they will receive. For more information on server-side events, see the Coherence documentation. Please see the Coherence documentation for more information on client events. Table of Contents What You Will Build What You Need Building the Example Code Review the Tests Review the classes Review the cache configuration Run the Tests Summary See Also What You Will Build In this example you will run a number of tests that demonstrate the following features of server-side events including: Understanding where to declare interceptors in your cache config Listening for cache events related to mutations of cache data, and execution of entry processors Listening for transfer events related to partition transfers and loss events Listening for partitioned cache events related to creation, destruction and truncating of caches Listening for lifecycle events for ConfigurableCacheFactory instantiation What You Need About 20 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build You can skip the tests in the initial build by adding the following options: -DskipTests for Maven or -x test for Gradle. Running the Examples This example comprises a number of tests showing various server-side events features. Running each example Each example can be run direct from the IDE, or can be run via executing the tests. <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test Review the Tests The example code comprises a number of classes: Tests ServerCacheEventsTest - tests for various cache events ServerPartitionEventsTest - tests for partition based events AbstractEventsTest - a class used by both tests which starts the clusters Model Customer - represents a fictional customer AuditEvent - represents an audit event Interceptors AuditingInterceptor - creates audit events after inserts, updates or removes on a cache EntryProcessorAuditingInterceptor - creates audit events after entry processor executions UppercaseInterceptor - a mutating interceptor that changes the name and address attributes to uppercase ValidationInterceptor - a mutating interceptor that optionally rejects updates if certain business rules are not met TransferEventsInterceptor - creates audit events after any partition transfers made CacheLifecycleEventsInterceptor - creates audit events after caches are created, truncated or destroyed LifecycleEventsInterceptor - logs a message when ConfigurableCacheFactories are activated or destroyed Review the classes Review the Customer class. Some tests use the Customer class which has the following fields: <markup lang=\"java\" >private int id; private String name; private String address; private String customerType; private long creditLimit; Review the AuditEvent class. Some tests use the AuditEvent class which has the following fields: <markup lang=\"java\" >/** * Unique Id for the audit event. */ private UUID id; /** * The target of the event such as cache, partition, etc. */ private String target; /** * The type of event. */ private String eventType; /** * Specific event data. */ private String eventData; /** * Time of the event. */ private long eventTime; Review the AuditingInterceptor which audits any mutations to caches using post-commit events. See here for details of all Partitioned Cache events. <markup lang=\"java\" >@Interceptor(identifier = \"AuditingInterceptor\", order = Interceptor.Order.HIGH) @EntryEvents({EntryEvent.Type.INSERTED, EntryEvent.Type.UPDATED, EntryEvent.Type.REMOVED}) public class AuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryEvent&lt;?, ?&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;?, ?&gt; event) { String oldValue = null; String newValue = null; EntryEvent.Type eventType = event.getType(); Object key = event.getKey(); if (eventType == EntryEvent.Type.REMOVED || eventType == EntryEvent.Type.UPDATED) { oldValue = event.getOriginalValue().toString(); } if (eventType == EntryEvent.Type.INSERTED || eventType == EntryEvent.Type.UPDATED) { newValue = event.getValue().toString(); } AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), eventType.toString(), String.format(\"key=%s, old=%s, new=%s\", key, oldValue, newValue)); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name and optional order of HIGH or LOW as the priority Identifies the EntryEvents that will be intercepted. INSERTED, UPDATED and REMOVED are raised asynchronously after the event has happened Identifies the type of events, in this case EntryEvents Overrides method to respond to the event Identifies the type of event and sets the payload accordingly Adds the audit event to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link AuditingInterceptor} which will audit any changes to caches * that fall thought and match the '*' cache-mapping. */ @Test public void testAuditingInterceptor() { System.out.println(\"testAuditingInterceptor\"); CoherenceClusterMember member = getMember1(); // create two different caches to be audited which will match to the auditing-scheme NamedCache&lt;Integer, String&gt; cache1 = member.getCache(\"test-cache\"); NamedCache&lt;Integer, Customer&gt; cache2 = member.getCache(\"test-customer\"); cache1.truncate(); cache2.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // generate some mutations that will be audited cache1.put(1, \"one\"); cache1.put(2, \"two\"); cache1.put(1, \"ONE\"); cache1.remove(1); dumpAuditEvents(\"testAuditingInterceptor-1\"); // ensure 3 inserts and 1 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); auditEvents.clear(); // generate new set of mutations for customers cache2.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10000)); cache2.put(2, new Customer(2, \"John\", \"Address 2\", Customer.SILVER, 4000)); cache2.clear(); dumpAuditEvents(\"testAuditingInterceptor-2\"); // ensure 2 insert and 2 remove events are received Eventually.assertDeferred(() -&gt; auditEvents.values().size(), Matchers.is(4)); } Review the EntryProcessorAuditingInterceptor which audits entry processors executions using post-commit events. <markup lang=\"java\" >@Interceptor(identifier = \"EntryProcessorAuditingInterceptor\") @EntryProcessorEvents({EntryProcessorEvent.Type.EXECUTED}) public class EntryProcessorAuditingInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;EntryProcessorEvent&gt;, Serializable { @Override public void onEvent(EntryProcessorEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Entries=%d, processor=%s\", event.getEntrySet().size(), event.getProcessor().toString())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the EntryProcessorEvents that will be intercepted. EXECUTED event is raised asynchronously after the event has happened Identifies the type of events, in this case EntryProcessorEvents Overrides method to respond to the event and add to the auditing cache This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link EntryProcessorAuditingInterceptor} which will audit any entry processor * executions on caches that match the '*' cache-mapping. */ @Test public void testEntryProcessorInterceptor() { System.out.println(\"testEntryProcessorInterceptor\"); CoherenceClusterMember member = getMember1(); // create a cache to audit entry processor events on NamedCache&lt;Integer, Customer&gt; cache = member.getCache(\"test-customer\"); cache.truncate(); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(4)); // clear the audit-events cache, so we miss the created and truncated events auditEvents.clear(); // add some entries cache.put(1, new Customer(1, \"Tim\", \"Address 1\", Customer.GOLD, 10_000)); cache.put(2, new Customer(2, \"Tom\", \"Address 2\", Customer.SILVER, 10_000)); cache.put(3, new Customer(3, \"Helen\", \"Address 3\", Customer.BRONZE, 10_000)); Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(3)); auditEvents.clear(); cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); // 2 entry processor events and 3 updates Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.is(5)); dumpAuditEvents(\"testEntryProcessorInterceptor-1\"); auditEvents.clear(); // invoke an entry processor across all customers to update credit limit to 100,000 cache.invokeAll(Processors.update(Customer::setCreditLimit, 100_000L)); cache.invoke(1, Processors.update(Customer::setCreditLimit, 100_000L)); // ensure all audit events are received Eventually.assertDeferred(() -&gt; auditEvents.values(equal(AuditEvent::getEventType, \"EXECUTED\")).size(), Matchers.is(3)); dumpAuditEvents(\"testEntryProcessorInterceptor-2\"); } Review the UppercaseInterceptor which changes the name and address attributes to uppercase. <markup lang=\"java\" >@Interceptor(identifier = \"UppercaseInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class UppercaseInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customer = entry.getValue(); customer.setName(customer.getName().toUpperCase()); customer.setAddress(customer.getAddress().toUpperCase()); entry.setValue(customer); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Ensures the changes are persisted by calling entry.setValue() This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link UppercaseInterceptor} which is defined on the 'customers' cache only, * to update name and address fields to uppercase. */ @Test public void testCustomerUppercaseInterceptor() { System.out.println(\"testCustomerUppercaseInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // put a new Customer with lowercase names and addresses customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.GOLD, 10000L)); // validate that the name and address are uppercase Customer customer = customers.get(1); assertEquals(customer.getName(), \"TIM\"); assertEquals(customer.getAddress(), \"123 JAMES STREET, PERTH\"); // update a customers name and ensure that it is updated to uppercase customers.invoke(1, Processors.update(Customer::setName, \"timothy\")); assertEquals(customers.get(1).getName(), \"TIMOTHY\"); } Review the ValidationInterceptor which rejects or accepts changes based upon some simple business rules. <markup lang=\"java\" >@Interceptor(identifier = \"ValidationInterceptor\") @EntryEvents({EntryEvent.Type.INSERTING, EntryEvent.Type.UPDATING}) public class ValidationInterceptor implements EventInterceptor&lt;EntryEvent&lt;Integer, Customer&gt;&gt;, Serializable { @Override public void onEvent(EntryEvent&lt;Integer, Customer&gt; event) { BinaryEntry&lt;Integer, Customer&gt; entry = event.getEntry(); Customer customerOld = entry.getOriginalValue(); Customer customerNew = entry.getValue(); EntryEvent.Type eventType = event.getType(); if (eventType == EntryEvent.Type.INSERTING) { // Rule 1 - New customers cannot have credit limit above 1,000,000 unless they are GOLD if (customerNew.getCreditLimit() &gt;= 1_000_000L &amp;&amp; !customerNew.getCustomerType().equals(Customer.GOLD)) { // reject the update throw new RuntimeException(\"Only gold customers may have credit limits above 1,000,000\"); } } else if (eventType == EntryEvent.Type.UPDATING) { // Rule 2 - Cannot change customer type from BRONZE directly to GOLD, must go BRONZE -&gt; SILVER -&gt; GOLD if (customerNew.getCustomerType().equals(Customer.GOLD) &amp;&amp; customerOld.getCustomerType().equals(Customer.BRONZE)) { // reject the update throw new RuntimeException(\"Cannot update customer directly to GOLD from BRONZE\"); } } // otherwise, continue with update entry.setValue(customerNew); } } Defines the interceptor name Identifies the EntryEvents that will be intercepted. INSERTING and UPDATING are raised synchronously before the operation is performed. Care must be taken to ensure these operations take as short amount of time as possible as implicit locks are held for the keys while updating. Identifies the type of events, in this case EntryEvent and the key and value are also defined using generics Overrides method to respond to the event Validates the first business rule if the event is an insert. If the rule fails, then throw a RuntimeException Validates the second business rule if the event is an update. If the rule fails, then throw a RuntimeException Saves the entry if all the business rules pass This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >/** * Test the {@link ValidationInterceptor} which will reject updates if business rules fail. */ @Test public void testValidatingInterceptor() { System.out.println(\"testValidatingInterceptor\"); NamedCache&lt;Integer, Customer&gt; customers = getMember1().getCache(\"customers\"); customers.truncate(); // try adding a BRONZE customer with credit limit &gt; 1,000,000 try { customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 2_000_000L)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Put was correctly rejected: %s\", e.getMessage()); } // should be rejected assertEquals(customers.size(), 0); // add a normal BRONZE customer, should succeed with credit limit 10,000 customers.put(1, new Customer(1, \"tim\", \"123 james street, perth\", Customer.BRONZE, 10_000L)); assertEquals(customers.size(), 1); // try and update credit limit to GOLD from BRONZE, should fail try { customers.invoke(1, Processors.update(Customer::setCustomerType, Customer.GOLD)); fail(\"Put succeeded but should have failed\"); } catch (Exception e) { System.out.printf(\"Update was correctly rejected: %s\", e.getMessage()); } assertEquals(customers.get(1).getCustomerType(), Customer.BRONZE); } Review the TransferEventsInterceptor which audits partition transfer events. <markup lang=\"java\" >@Interceptor(identifier = \"TransferEventsInterceptor\") @TransferEvents({TransferEvent.Type.ARRIVED, TransferEvent.Type.DEPARTING, TransferEvent.Type.LOST}) public class TransferEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;TransferEvent&gt;, Serializable { @Override public void onEvent(TransferEvent event) { AuditEvent auditEvent = new AuditEvent(\"partition=\" + event.getPartitionId(), event.getType().toString(), String.format(\"Partitions from remote member %s\", event.getRemoteMember())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the TransferEvents that will be intercepted. Transfer events are raised while holding a lock on the partition being transferred that blocks any operations for the partition. Identifies the type of events, in this case TransferEvents Overrides method to respond to the event This is used in the following test in ServerPartitionEventsTest : <markup lang=\"java\" >@Test public void testPartitionEvents() { System.out.println(\"testPartitionEvents\"); CoherenceClusterMember member1 = getMember1(); CoherenceClusterMember member2 = getMember2(); NamedCache&lt;Integer, String&gt; cache = member1.getCache(\"test-cache\"); for (int i = 0; i &lt; 10; i++) { cache.put(i, \"value-\" + i); } // ensure all audit events are received = 10 insert events plus 2 cache created events Eventually.assertDeferred(()-&gt;auditEvents.size(), Matchers.is(12)); // shutdown the second member member2.close(); // wait for additional partition events to be received Eventually.assertDeferred(() -&gt; auditEvents.size(), Matchers.greaterThan(16)); dumpAuditEvents(\"testPartitionEvents\"); } Review the CacheLifecycleEventsInterceptor which audits cache lifecycle events. <markup lang=\"java\" >@Interceptor(identifier = \"CacheLifecycleEventsInterceptor\") @CacheLifecycleEvents( {CacheLifecycleEvent.Type.CREATED, CacheLifecycleEvent.Type.DESTROYED, CacheLifecycleEvent.Type.TRUNCATED}) public class CacheLifecycleEventsInterceptor extends AbstractAuditingInterceptor implements EventInterceptor&lt;CacheLifecycleEvent&gt;, Serializable { @Override public void onEvent(CacheLifecycleEvent event) { AuditEvent auditEvent = new AuditEvent(\"cache=\" + event.getCacheName(), event.getType().toString(), String.format(\"Event from service %s\", event.getServiceName())); getAuditCache().put(auditEvent.getId(), auditEvent); } } Defines the interceptor name Identifies the CacheLifecycleEvent that will be intercepted. CREATED, DESTROYED and TRUNCATED are raised asynchronously after the operation is completed. Identifies the type of events, in this case CacheLifecycleEvent Overrides method to respond to the event This is used in the following test in ServerCacheEventsTest : <markup lang=\"java\" >@Test public void testTruncate() { System.out.println(\"testTruncate\"); auditEvents.clear(); NamedCache&lt;Integer, String&gt; cache1 = getMember1().getCache(\"test-cache\"); cache1.truncate(); // ensure we get two events, one from each storage node Eventually.assertDeferred(() -&gt; auditEvents.values(equal(AuditEvent::getEventType, \"TRUNCATED\")).size(), Matchers.is(2)); dumpAuditEvents(\"truncate\"); } Review the cache config The interceptors are added via cache config and can be applied at the service or cache level. Review the Cache Scheme Mapping <markup lang=\"xml\" > &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;LifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.LifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;customers&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;UppercaseInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.UppercaseInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;ValidationInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.ValidationInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/cache-mapping&gt; &lt;!-- cache to store auditing events --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;audit-events&lt;/cache-name&gt; &lt;scheme-name&gt;server-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;!-- any caches other than are defined above will be audited --&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;/cache-mapping&gt; Defines an interceptor called LifecycleEventsInterceptor to log any ConfigurableCacheFactory events. Defines customers cache which has the UppercaseInterceptor and ValidationInterceptor enabled for only this cache Review the Caching Schemes <markup lang=\"xml\" >&lt;!-- Any caches in this scheme will be audited and data put in \"audit-events\" cache. --&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;auditing-scheme&lt;/scheme-name&gt; &lt;service-name&gt;DistributedCacheAudit&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme/&gt; &lt;/backing-map-scheme&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;name&gt;AuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.AuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;EntryProcessorAuditingInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.EntryProcessorAuditingInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;TransferEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.TransferEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;interceptor&gt; &lt;name&gt;CacheLifecycleEventsInterceptor&lt;/name&gt; &lt;instance&gt; &lt;class-name&gt; com.oracle.coherence.guides.serverevents.interceptors.CacheLifecycleEventsInterceptor &lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;/distributed-scheme&gt; Defines auditing-scheme which has the AuditingInterceptor , EntryProcessorAuditingInterceptor , CacheLifecycleEventsInterceptor and TransferEventsInterceptor enabled for any caches using this scheme. Run the Tests Run the examples using the test case below. Run directly from your IDE by running either of the following test classes: com.oracle.coherence.guides.serverevents.ServerPartitionEventsTest com.oracle.coherence.guides.serverevents.ServerCacheEventsTest Run using Maven or Gradle E.g. for Maven use: <markup lang=\"bash\" >./mvnw clean verify or <markup lang=\"bash\" >./gradlew clean test When the test is run you will see output from the various parts of the test code. Output has been truncated and formatted for easier reading. testPartitions Output <markup lang=\"bash\" >testPartitionEvents Dumping the audit events testPartitionEvents AuditEvent{id=2E1E1FE69E, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209123} AuditEvent{id=54A54A5CED, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255209129} AuditEvent{id=AAA54A5CEE, target='cache=test-cache', eventType='INSERTED', eventData='key=0, old=null, new=value-0', eventTime=1652255209135} AuditEvent{id=A51E1FE69F, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=value-1', eventTime=1652255209141} ... AuditEvent{id=A1A54A5CF3, target='cache=test-cache', eventType='INSERTED', eventData='key=9, old=null, new=value-9', eventTime=1652255209169} ... AuditEvent{id=961E1FE6A3, target='partition=0', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209572} AuditEvent{id=261E1FE6A4, target='partition=1', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209580} ... AuditEvent{id=531E1FE6B1, target='partition=14', eventType='ARRIVED', eventData='Partitions from remote member Member(Id=1, ...', eventTime=1652255209587} Lifecycle events from creation of cache from two storage nodes Insert events for cache entries Partitions arriving from remove member before shutdown testTruncate Output <markup lang=\"bash\" >testTruncate Dumping the audit events truncate AuditEvent{id=B8127D2701, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218772} AuditEvent{id=6BD64A90EA, target='cache=test-cache', eventType='CREATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218784} AuditEvent{id=7E127D2702, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218802} AuditEvent{id=17D64A90EB, target='cache=test-cache', eventType='TRUNCATED', eventData='Event from service DistributedCacheAudit', eventTime=1652255218806} Both CREATED and TRUNCATED events are shown. testEntryProcessorInterceptor Output <markup lang=\"bash\" >testEntryProcessorInterceptor Dumping the audit events testEntryProcessorInterceptor-1 AuditEvent{id=AE5BC2D3EB, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(Customer$setCreditLimit...', eventTime=1652319479550} AuditEvent{id=C25BC2D3EC, target='cache=test-customer', eventType='UPDATED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=100000}', eventTime=1652319479553} AuditEvent{id=3D82ADF7F7, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(Customer$setCreditLimit...'}}, arguments=[]}}, 100000)', eventTime=1652319479553} AuditEvent{id=4382ADF7F8, target='cache=test-customer', eventType='UPDATED', eventData='key=2, old=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=10000}, new=Customer{id=2, name='Tom', address='Address 2', customerType='SILVER', balance=100000}', eventTime=1652319479556} AuditEvent{id=575BC2D3ED, target='cache=test-customer', eventType='UPDATED', eventData='key=3, old=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=10000}, new=Customer{id=3, name='Helen', address='Address 3', customerType='BRONZE', balance=100000}', eventTime=1652319479556} Dumping the audit events testEntryProcessorInterceptor-2 AuditEvent{id=F05BC2D3EE, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=2, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479577} AuditEvent{id=7982ADF7F9, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479578} AuditEvent{id=235BC2D3EF, target='cache=test-customer', eventType='EXECUTED', eventData='Entries=1, processor=UpdaterProcessor(...'}}, arguments=[]}}, 100000)', eventTime=1652319479584} Three insert events and two entry processor events. One from each storage-enabled node Three entry processor events, one for an individual invoke() on a key and two from the invokeAll as per item 1 testValidatingInterceptor Output testValidatingInterceptor Output <markup lang=\"bash\" >Put was correctly rejected: Failed to execute [put] with arguments [1, Customer{id=1, name='tim', address='123 james street, perth', customerType='BRONZE', balance=2000000}] Update was correctly rejected: Failed to execute [invoke] with arguments [1, UpdaterProcessor(com.oracle.coherence.guides.serverevents.ServerCacheEventsTest$$Lambda$475/0x00000008003da040@783ecb80, GOLD)] testCustomerUppercaseInterceptor Messages from rejected updates testAuditingInterceptor Output <markup lang=\"bash\" >testAuditingInterceptor Dumping the audit events testAuditingInterceptor-1 AuditEvent{id=1D127D270E, target='cache=test-cache', eventType='INSERTED', eventData='key=1, old=null, new=one', eventTime=1652255219418} AuditEvent{id=25D64A90F4, target='cache=test-cache', eventType='INSERTED', eventData='key=2, old=null, new=two', eventTime=1652255219428} AuditEvent{id=A5127D270F, target='cache=test-cache', eventType='UPDATED', eventData='key=1, old=one, new=ONE', eventTime=1652255219432} AuditEvent{id=EF127D2710, target='cache=test-cache', eventType='REMOVED', eventData='key=1, old=ONE, new=null', eventTime=1652255219436} Dumping the audit events testAuditingInterceptor-2 AuditEvent{id=A5127D2711, target='cache=test-customer', eventType='INSERTED', eventData='key=1, old=null, new=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}', eventTime=1652255219456} AuditEvent{id=5BD64A90F5, target='cache=test-customer', eventType='INSERTED', eventData='key=2, old=null, new=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}', eventTime=1652255219460} AuditEvent{id=CAD64A90F6, target='cache=test-customer', eventType='REMOVED', eventData='key=2, old=Customer{id=2, name='John', address='Address 2', customerType='SILVER', balance=4000}, new=null', eventTime=1652255219466} AuditEvent{id=27127D2712, target='cache=test-customer', eventType='REMOVED', eventData='key=1, old=Customer{id=1, name='Tim', address='Address 1', customerType='GOLD', balance=10000}, new=null', eventTime=1652255219466} Two inserts, one update and a remove Two inserts and two removes as a result of clear() Summary In this guide we walked you through how to use server-side events within Coherence to listen for various events on a Coherence NamedMap or NamedCache . See Also Develop Applications using Server Side Events Client Side Events ",
            "title": "Server-Side Events"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " What You Will Build What You Need Building The Example Code Example Data Model Why to use Views Using a ContinuousQueryCache Observing Continuous Query Caches Continuous Aggregation Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The example code is written as a set of unit tests, showing you how can create Views against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . ",
            "title": "Example Data Model"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " With Views , also referred to as Continuous Queries , you can ensure that a query always retrieves the latest results from a cache in real-time. For instance, in the queries guide , we used a Filter to query for a subset of data from a Coherence cache. However, what happens with the underlying cache if changes DO happen, and you need the updates immediately? Queries, as used previously, will only retrieve a snapshot of the underlying data. They will not reflect future data changes. Thus, let&#8217;s revisit a previous example that queries a cache containing Countries using a Filter . The Filter , as in the previous query example, will ensure that only countries with a population of 60 million or more people are returned. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithChanges() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results, hasSize(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results, hasSize(2)); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the NamedCache The result should be 2 countries only We add a new country Mexico to the map Assert that still only France and Germany were selected In this test we have added a new country Mexico to the countries Map but as you can see, the change will not be reflected in the already filtered results map. In order to get updates in real-time, we have to use a ContinuousQueryCache . Views are extremely useful in all those situations where we need immediate access to any changes of the underlying data, such as trading systems or Complex Event Processing (CEP) systems. They can be used in both client-based and server-based applications and are reminiscent of SQL Views. ",
            "title": "Why to use Views"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " The following test will look almost exactly the same as the previous test. However, instead of calling the entrySet() method on the NamedCache , we will create a new instance of ContinuousQueryCache and pass in the Filter and the Coherence map as constructor arguments. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithContinuousQueryCache() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Create a new instance of ContinuousQueryCache The result should consist of 2 countries only We add a new country Mexico to the original map Assert that the ContinuousQueryCache now contains 3 countries Under the covers, the ContinuousQueryCache will use Coherence cache events on the map to react to changes in the Coherence NamedCache . In order to create a ContinuousQueryCache without filtering, use the AlwaysFilter , e.g. new ContinuousQueryCache(map, AlwaysFilter.instance) . ",
            "title": "Using a ContinuousQueryCache"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " Proactively querying for updates is all fun and games but what if you need to execute logic as soon as data changes happen? The ContinuousQueryCache implements the ObservableMap interface to react to Coherence cache events. As such, you can subscribe to cache events by registering MapListener implementations. In the following test, we will add a MapListener to keep track of countries being added to the underlying NamedCache . But because this listener is added to the ContinuousQueryCache , the listener will only get invoked for countries that have a population of 60 million or more. <markup lang=\"java\" >@Test void testContinuousQueryCacheWithListener() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); AtomicInteger counter = new AtomicInteger(0); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { counter.incrementAndGet(); }); results.addMapListener(listener); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(counter::get, is(1)); } Create a counter to keep track of added countries Instantiate a MapListener that will increment the counter for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the ContinuousQueryCache contains 2 countries Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The counter of the MapListener should have increased by 1 ",
            "title": "Observing Continuous Query Caches"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " What about aggregated results? In an earlier example for Queries, we had used a Filter and a BigDecimalSum aggregator to calculate the sum of the population for those countries whose population is at least 60 million. We can use a MapListener to achieve that, as the ContinuousQueryCache does not directly support aggregators. <markup lang=\"java\" >@Test void testAggregate() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ReflectionExtractor&lt;Country, Double&gt; extractor = new ReflectionExtractor&lt;&gt;(\"getPopulation\"); ContinuousQueryCache&lt;String, Country, Double&gt; results = new ContinuousQueryCache(map, filter, extractor); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(new IdentityExtractor&lt;&gt;()); AtomicReference&lt;BigDecimal&gt; aggregatedPopulation = new AtomicReference&lt;&gt;(formatNumber(results.aggregate(aggregator))); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { aggregatedPopulation.set(formatNumber(results.aggregate(aggregator))); }); results.addMapListener(listener); assertThat(aggregatedPopulation.get(), is(formatNumber(150.6))); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(aggregatedPopulation::get, is(formatNumber(276.61))); } Create a BigDecimalSum aggregator. The IdentityExtractor will use the actual value (does not actually extract anything) Create a holder for the aggregated population and trigger the initial aggregation explicitly Instantiate a MapListener that will trigger the aggregation of the population for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the aggregated population is initially 150.6 million Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The aggregated population should now be 276.61 million ",
            "title": "Continuous Aggregation"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " In this guide we showed, how you can easily create Views with a ContinuousQueryCache that reflects changes of the data in the underlying Coherence NamedCache in real-time. Please see the Coherence reference guide, specifically the chapter Using Continuous Query Caching for more details. ",
            "title": "Summary"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " Using Continuous Query Caching Using Map Events Querying Caches Built-In Aggregators ",
            "title": "See Also"
        },
        {
            "location": "/examples/guides/124-views/README",
            "text": " This guide walks you through the concepts of creating Views , also known as Continuous Queries . Views allow you to execute queries against your Coherence data with the added benefit that Views stay up-to-date, allowing you to retrieve the latest results of your query from the Coherence cache in real-time. Table of Contents What You Will Build What You Need Building The Example Code Example Data Model Why to use Views Using a ContinuousQueryCache Observing Continuous Query Caches Continuous Aggregation Summary See Also What You Will Build The example code is written as a set of unit tests, showing you how can create Views against your Coherence data. What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Example Data Model The data model for this guide consists of a single class named Country . This model class represents a country with the following properties: name capital population The data is being stored in a Coherence cache named countries with the key being the two-letter ISO 3166 country code . Why to use Views With Views , also referred to as Continuous Queries , you can ensure that a query always retrieves the latest results from a cache in real-time. For instance, in the queries guide , we used a Filter to query for a subset of data from a Coherence cache. However, what happens with the underlying cache if changes DO happen, and you need the updates immediately? Queries, as used previously, will only retrieve a snapshot of the underlying data. They will not reflect future data changes. Thus, let&#8217;s revisit a previous example that queries a cache containing Countries using a Filter . The Filter , as in the previous query example, will ensure that only countries with a population of 60 million or more people are returned. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithChanges() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); Set&lt;Map.Entry&lt;String, Country&gt;&gt; results = map.entrySet(filter); assertThat(results, hasSize(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results, hasSize(2)); } Get the countries Map We create a Filter that will select only countries with at least 60 million people using a GreaterEqualsFilter Apply the Filter by invoking entrySet(filter) on the NamedCache The result should be 2 countries only We add a new country Mexico to the map Assert that still only France and Germany were selected In this test we have added a new country Mexico to the countries Map but as you can see, the change will not be reflected in the already filtered results map. In order to get updates in real-time, we have to use a ContinuousQueryCache . Views are extremely useful in all those situations where we need immediate access to any changes of the underlying data, such as trading systems or Complex Event Processing (CEP) systems. They can be used in both client-based and server-based applications and are reminiscent of SQL Views. Using a ContinuousQueryCache The following test will look almost exactly the same as the previous test. However, instead of calling the entrySet() method on the NamedCache , we will create a new instance of ContinuousQueryCache and pass in the Filter and the Coherence map as constructor arguments. <markup lang=\"java\" >@Test void testGreaterEqualsFilterWithContinuousQueryCache() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); } Get the countries Map We create a Filter that will select only countries with more than 60 million people using a GreaterEqualsFilter Create a new instance of ContinuousQueryCache The result should consist of 2 countries only We add a new country Mexico to the original map Assert that the ContinuousQueryCache now contains 3 countries Under the covers, the ContinuousQueryCache will use Coherence cache events on the map to react to changes in the Coherence NamedCache . In order to create a ContinuousQueryCache without filtering, use the AlwaysFilter , e.g. new ContinuousQueryCache(map, AlwaysFilter.instance) . Observing Continuous Query Caches Proactively querying for updates is all fun and games but what if you need to execute logic as soon as data changes happen? The ContinuousQueryCache implements the ObservableMap interface to react to Coherence cache events. As such, you can subscribe to cache events by registering MapListener implementations. In the following test, we will add a MapListener to keep track of countries being added to the underlying NamedCache . But because this listener is added to the ContinuousQueryCache , the listener will only get invoked for countries that have a population of 60 million or more. <markup lang=\"java\" >@Test void testContinuousQueryCacheWithListener() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ContinuousQueryCache results = new ContinuousQueryCache(map, filter); AtomicInteger counter = new AtomicInteger(0); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { counter.incrementAndGet(); }); results.addMapListener(listener); assertThat(results.size(), is(2)); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(counter::get, is(1)); } Create a counter to keep track of added countries Instantiate a MapListener that will increment the counter for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the ContinuousQueryCache contains 2 countries Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The counter of the MapListener should have increased by 1 Continuous Aggregation What about aggregated results? In an earlier example for Queries, we had used a Filter and a BigDecimalSum aggregator to calculate the sum of the population for those countries whose population is at least 60 million. We can use a MapListener to achieve that, as the ContinuousQueryCache does not directly support aggregators. <markup lang=\"java\" >@Test void testAggregate() { NamedCache&lt;String, Country&gt; map = getMap(\"countries\"); Filter filter = new GreaterEqualsFilter(\"getPopulation\", 60.0); ReflectionExtractor&lt;Country, Double&gt; extractor = new ReflectionExtractor&lt;&gt;(\"getPopulation\"); ContinuousQueryCache&lt;String, Country, Double&gt; results = new ContinuousQueryCache(map, filter, extractor); BigDecimalSum&lt;BigDecimal&gt; aggregator = new BigDecimalSum(new IdentityExtractor&lt;&gt;()); AtomicReference&lt;BigDecimal&gt; aggregatedPopulation = new AtomicReference&lt;&gt;(formatNumber(results.aggregate(aggregator))); MapListener&lt;String, Double&gt; listener = new SimpleMapListener&lt;String, Double&gt;() .addInsertHandler((event) -&gt; { aggregatedPopulation.set(formatNumber(results.aggregate(aggregator))); }); results.addMapListener(listener); assertThat(aggregatedPopulation.get(), is(formatNumber(150.6))); Country mexico = new Country(\"Mexico\", \"Ciudad de México\", 126.01); map.put(\"mx\", mexico); assertThat(results.size(), is(3)); Eventually.assertDeferred(aggregatedPopulation::get, is(formatNumber(276.61))); } Create a BigDecimalSum aggregator. The IdentityExtractor will use the actual value (does not actually extract anything) Create a holder for the aggregated population and trigger the initial aggregation explicitly Instantiate a MapListener that will trigger the aggregation of the population for each new country being added Add the MapListener to the ContinuousQueryCache Assert that the aggregated population is initially 150.6 million Add a new country with a population larger than 60 million Assert that the ContinuousQueryCache now contain 3 countries The aggregated population should now be 276.61 million Summary In this guide we showed, how you can easily create Views with a ContinuousQueryCache that reflects changes of the data in the underlying Coherence NamedCache in real-time. Please see the Coherence reference guide, specifically the chapter Using Continuous Query Caching for more details. See Also Using Continuous Query Caching Using Map Events Querying Caches Built-In Aggregators ",
            "title": "Views"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Persistence Configuration Listening to JMX Notifications Build and Run the Example Enable Active Persistence Enable a Snapshot Archiver Summary See Also ",
            "title": "Table of Contents"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " You will review the requirements for running both on-demand and active persistence and carry out the following: Start one or more cache servers with on-demand persistence Start a CohQL session to insert data and create and manage snapshots Start a JMX MBean listener to monitor Persistence operations Change on-demand to active persistence and show this in action Work with archiving snapshots ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " About 20-30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build ",
            "title": "What You Need"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " The project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. cache-server - Runs a DefaultCacheServer cohql - Runs a CohQL session notifications - Runs a process to subscribe to JMX Notification events cache-server - Runs a DefaultCacheServer <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cache-server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cache-server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.mode=on-demand&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.base.dir=persistence-data&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Set on-demand mode, which is the default Set the base directory for all persistence directories cohql - Runs a CohQL session <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;persistence-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.base.dir&lt;/key&gt; &lt;value&gt;persistence-data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.mode&lt;/key&gt; &lt;value&gt;on-demand&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; notifications - Runs a process to subscribe to JMX Notification events <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;notifications&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;notifications&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx128m&lt;/argument&gt; &lt;argument&gt;-Xms128m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.localstorage=false&lt;/argument&gt; &lt;argument&gt;com.oracle.coherence.tutorials.persistence.NotificationWatcher&lt;/argument&gt; &lt;argument&gt;PartitionedCache&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; ",
            "title": "Maven Configuration"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " By default, any partitioned service, including Federated services, will default to on-demand mode. This mode allows you to create and manage snapshots to default directories without any setup. A coherence directory off the users home directory is used to store all persistence-related data. If you wish to enable active persistence mode you can use a system property -Dcoherence.distributed.persistence.mode=active and this will use the default directories as described above. In this example we are also defining the base persistence directory using a system property -Dcoherence.distributed.persistence.base.dir=persistence-data . All other persistence directories will be created below this directory. Please see here for more details on configuring your persistence locations. In this tutorial, Persistence is configured in two files: An operational override file is used to configure non-default persistence environments and archive locations A cache configuration with file the &lt;persistence&gt; element is used to associate services with persistence environments, if you are not using the defaults. (This is initially commented out.) Cache Configuration File <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The above cache configuration has the &lt;persistence&gt; element commented out for the first part of this tutorial. Operational Override File <markup lang=\"xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;snapshot-archivers&gt; &lt;directory-archiver id=\"shared-directory-archiver\"&gt; &lt;archive-directory system-property=\"coherence.distributed.persistence.archive.dir\"&gt;persistence-data/archives&lt;/archive-directory&gt; &lt;/directory-archiver&gt; &lt;/snapshot-archivers&gt; &lt;/cluster-config&gt; &lt;management-config&gt; &lt;managed-nodes system-property=\"coherence.management\"&gt;all&lt;/managed-nodes&gt; &lt;/management-config&gt; &lt;/coherence&gt; Defines a snapshot archiver to archive to a given directory ",
            "title": "Persistence Configuration"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Persistence operations generate JMX notifications. You can register for these notifications to monitor and understand how long these operations are taking. You can use a tool such as VisualVM with the Coherence VisualVM plugin to monitor and manage persistence. See the VisualVM Plugin project on GitHub . Main entry point <markup lang=\"java\" >public static void main(String[] args) { if (args.length == 0) { System.out.println(\"Please provide a list of services to listen for notifications on\"); System.exit(1); } Set&lt;String&gt; setServices = new HashSet&lt;&gt;(Arrays.asList(args)); System.out.println(\"Getting MBeanServer...\"); Cluster cluster = CacheFactory.ensureCluster(); MBeanServer server = MBeanHelper.findMBeanServer(); Registry registry = cluster.getManagement(); if (server == null) { throw new RuntimeException(\"Unable to find MBeanServer\"); } try { for (String serviceName : setServices) { System.out.println(\"Registering listener for \" + serviceName); String mBeanName = \"Coherence:\" + CachePersistenceHelper.getMBeanName(serviceName); waitForRegistration(registry, mBeanName); ObjectName beanName = new ObjectName(mBeanName); NotificationListener listener = new PersistenceNotificationListener(serviceName); server.addNotificationListener(beanName, listener, null, null); mapListeners.put(beanName, listener); } System.out.println(\"Waiting for notifications. Use CTRL-C to interrupt.\"); Thread.sleep(Long.MAX_VALUE); } catch (Exception e) { e.printStackTrace(); } finally { // unregister all registered notifications mapListeners.forEach((k, v) -&gt; { try { server.removeNotificationListener(k, v); } catch (Exception eIgnore) { // ignore } }); } } Join the cluster and retrieve the MBeanServer and Registry Loop through the services provided as arguments and get the MBean name for the Persistence MBean Ensure the MBean is registered Add a notification listener on the Persistence MBean PersistenceNotificationListener implementation <markup lang=\"java\" >public static class PersistenceNotificationListener implements NotificationListener { Handle the notification <markup lang=\"java\" >@Override public synchronized void handleNotification(Notification notification, Object oHandback) { counter.incrementAndGet(); String userData = notification.getUserData().toString(); String message = notification.getMessage() + \" \" + notification.getUserData(); // default // determine if it's a begin or end notification String type = notification.getType(); if (type.indexOf(BEGIN) &gt; 0) { // handle begin notification and save the start time mapNotify.put(type, notification.getTimeStamp()); message = notification.getMessage(); } else if (type.indexOf(END) &gt; 0) { // handle end notification and try and find the matching begin notification String begin = type.replaceAll(END, BEGIN); Long start = mapNotify.get(begin); if (start != null) { message = \" \" + notification.getMessage() + (userData == null || userData.isEmpty() ? \"\" : userData) + \" (Duration=\" + (notification.getTimeStamp() - start) + \"ms)\"; mapNotify.remove(begin); } } else { message = serviceName + \": \" + type + \"\"; } System.out.println(new Date(notification.getTimeStamp()) + \" : \" + serviceName + \" (\" + type + \") \" + message); } Store the details of the begin notification Handle the end notification and determine the operation length ",
            "title": "Listening to JMX Notifications"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Maven Configuration The project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. cache-server - Runs a DefaultCacheServer cohql - Runs a CohQL session notifications - Runs a process to subscribe to JMX Notification events cache-server - Runs a DefaultCacheServer <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cache-server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cache-server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.mode=on-demand&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.base.dir=persistence-data&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Set on-demand mode, which is the default Set the base directory for all persistence directories cohql - Runs a CohQL session <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;persistence-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.base.dir&lt;/key&gt; &lt;value&gt;persistence-data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.mode&lt;/key&gt; &lt;value&gt;on-demand&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; notifications - Runs a process to subscribe to JMX Notification events <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;notifications&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;notifications&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx128m&lt;/argument&gt; &lt;argument&gt;-Xms128m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.localstorage=false&lt;/argument&gt; &lt;argument&gt;com.oracle.coherence.tutorials.persistence.NotificationWatcher&lt;/argument&gt; &lt;argument&gt;PartitionedCache&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Persistence Configuration By default, any partitioned service, including Federated services, will default to on-demand mode. This mode allows you to create and manage snapshots to default directories without any setup. A coherence directory off the users home directory is used to store all persistence-related data. If you wish to enable active persistence mode you can use a system property -Dcoherence.distributed.persistence.mode=active and this will use the default directories as described above. In this example we are also defining the base persistence directory using a system property -Dcoherence.distributed.persistence.base.dir=persistence-data . All other persistence directories will be created below this directory. Please see here for more details on configuring your persistence locations. In this tutorial, Persistence is configured in two files: An operational override file is used to configure non-default persistence environments and archive locations A cache configuration with file the &lt;persistence&gt; element is used to associate services with persistence environments, if you are not using the defaults. (This is initially commented out.) Cache Configuration File <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The above cache configuration has the &lt;persistence&gt; element commented out for the first part of this tutorial. Operational Override File <markup lang=\"xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;snapshot-archivers&gt; &lt;directory-archiver id=\"shared-directory-archiver\"&gt; &lt;archive-directory system-property=\"coherence.distributed.persistence.archive.dir\"&gt;persistence-data/archives&lt;/archive-directory&gt; &lt;/directory-archiver&gt; &lt;/snapshot-archivers&gt; &lt;/cluster-config&gt; &lt;management-config&gt; &lt;managed-nodes system-property=\"coherence.management\"&gt;all&lt;/managed-nodes&gt; &lt;/management-config&gt; &lt;/coherence&gt; Defines a snapshot archiver to archive to a given directory Listening to JMX Notifications Persistence operations generate JMX notifications. You can register for these notifications to monitor and understand how long these operations are taking. You can use a tool such as VisualVM with the Coherence VisualVM plugin to monitor and manage persistence. See the VisualVM Plugin project on GitHub . Main entry point <markup lang=\"java\" >public static void main(String[] args) { if (args.length == 0) { System.out.println(\"Please provide a list of services to listen for notifications on\"); System.exit(1); } Set&lt;String&gt; setServices = new HashSet&lt;&gt;(Arrays.asList(args)); System.out.println(\"Getting MBeanServer...\"); Cluster cluster = CacheFactory.ensureCluster(); MBeanServer server = MBeanHelper.findMBeanServer(); Registry registry = cluster.getManagement(); if (server == null) { throw new RuntimeException(\"Unable to find MBeanServer\"); } try { for (String serviceName : setServices) { System.out.println(\"Registering listener for \" + serviceName); String mBeanName = \"Coherence:\" + CachePersistenceHelper.getMBeanName(serviceName); waitForRegistration(registry, mBeanName); ObjectName beanName = new ObjectName(mBeanName); NotificationListener listener = new PersistenceNotificationListener(serviceName); server.addNotificationListener(beanName, listener, null, null); mapListeners.put(beanName, listener); } System.out.println(\"Waiting for notifications. Use CTRL-C to interrupt.\"); Thread.sleep(Long.MAX_VALUE); } catch (Exception e) { e.printStackTrace(); } finally { // unregister all registered notifications mapListeners.forEach((k, v) -&gt; { try { server.removeNotificationListener(k, v); } catch (Exception eIgnore) { // ignore } }); } } Join the cluster and retrieve the MBeanServer and Registry Loop through the services provided as arguments and get the MBean name for the Persistence MBean Ensure the MBean is registered Add a notification listener on the Persistence MBean PersistenceNotificationListener implementation <markup lang=\"java\" >public static class PersistenceNotificationListener implements NotificationListener { Handle the notification <markup lang=\"java\" >@Override public synchronized void handleNotification(Notification notification, Object oHandback) { counter.incrementAndGet(); String userData = notification.getUserData().toString(); String message = notification.getMessage() + \" \" + notification.getUserData(); // default // determine if it's a begin or end notification String type = notification.getType(); if (type.indexOf(BEGIN) &gt; 0) { // handle begin notification and save the start time mapNotify.put(type, notification.getTimeStamp()); message = notification.getMessage(); } else if (type.indexOf(END) &gt; 0) { // handle end notification and try and find the matching begin notification String begin = type.replaceAll(END, BEGIN); Long start = mapNotify.get(begin); if (start != null) { message = \" \" + notification.getMessage() + (userData == null || userData.isEmpty() ? \"\" : userData) + \" (Duration=\" + (notification.getTimeStamp() - start) + \"ms)\"; mapNotify.remove(begin); } } else { message = serviceName + \": \" + type + \"\"; } System.out.println(new Date(notification.getTimeStamp()) + \" : \" + serviceName + \" (\" + type + \") \" + message); } Store the details of the begin notification Handle the end notification and determine the operation length ",
            "title": "Review the Project"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P cache-server Start a CohQL session. <markup lang=\"bash\" >./mvnw exec:java -P cohql Start a JMX Listener. <markup lang=\"bash\" >./mvnw exec:exec -P notifications ",
            "title": "Maven"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./gradlew runCacheServer Start a CohQL session. <markup lang=\"bash\" >./gradlew runCohql --console=plain and <markup lang=\"bash\" >./gradlew runNotifications --console=plain ",
            "title": "Gradle"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " In the CohQL session, run the following commands to add data: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Create a snapshot containing this data <markup lang=\"bash\" >CohQL&gt; list snapshots Results \"PartitionedCache\": [] CohQL&gt; create snapshot \"data\" \"PartitionedCache\" Are you sure you want to create a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Creating snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:03 AWST 2022 : PartitionedCache (create.snapshot.begin) Building snapshot \"data\" Tue Apr 26 10:57:06 AWST 2022 : PartitionedCache (create.snapshot.end) Successfully created snapshot \"data\" (Duration=3445ms) Clear the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; delete from test Results CohQL&gt; select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-11 16:23:06.691/499.700 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been suspended 2022-04-11 16:23:09.247/502.256 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been resumed Results \"Success\" select count() from test Results 3 You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.snapshot.begin) Recovering Snapshot \"data\" Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.begin) Recovering snapshot \"data\" Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.end) Recovery Completed (Duration=623ms) Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.snapshot.end) Successfully recovered snapshot \"data\" (Duration=631ms) You will be able to see the snapshots in the directory persistence-data/snapshots . ",
            "title": "Run the following commands to exercise on-demand Persistence"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Once you have built the project as described earlier in this document, you can run it via Maven or Gradle. Maven Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P cache-server Start a CohQL session. <markup lang=\"bash\" >./mvnw exec:java -P cohql Start a JMX Listener. <markup lang=\"bash\" >./mvnw exec:exec -P notifications Gradle Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./gradlew runCacheServer Start a CohQL session. <markup lang=\"bash\" >./gradlew runCohql --console=plain and <markup lang=\"bash\" >./gradlew runNotifications --console=plain Run the following commands to exercise on-demand Persistence In the CohQL session, run the following commands to add data: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Create a snapshot containing this data <markup lang=\"bash\" >CohQL&gt; list snapshots Results \"PartitionedCache\": [] CohQL&gt; create snapshot \"data\" \"PartitionedCache\" Are you sure you want to create a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Creating snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:03 AWST 2022 : PartitionedCache (create.snapshot.begin) Building snapshot \"data\" Tue Apr 26 10:57:06 AWST 2022 : PartitionedCache (create.snapshot.end) Successfully created snapshot \"data\" (Duration=3445ms) Clear the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; delete from test Results CohQL&gt; select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-11 16:23:06.691/499.700 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been suspended 2022-04-11 16:23:09.247/502.256 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been resumed Results \"Success\" select count() from test Results 3 You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.snapshot.begin) Recovering Snapshot \"data\" Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.begin) Recovering snapshot \"data\" Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.end) Recovery Completed (Duration=623ms) Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.snapshot.end) Successfully recovered snapshot \"data\" (Duration=631ms) You will be able to see the snapshots in the directory persistence-data/snapshots . ",
            "title": "Run the Example"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " After shutting down all running processes, in the file pom.xml change on-demand to active for the cohql and cache-server profiles to enable active persistence. Run the cache-server , cohql and notifications as described above. In the CohQL session, run the following commands to add data <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Shutdown all three processes and restart the cache-server and cohql processes, then continue below. Re-query the test cache <markup lang=\"bash\" >CohQL&gt; select * from test Results \"two\" \"three\" \"one\" You can see that the cache data has automatically been recovered from disk during cluster startup. This active persistence data is stored in the directory persistence-data/active below the persistence tutorial directory. ",
            "title": "Run the following commands to exercise active Persistence"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Run the following commands to exercise active Persistence After shutting down all running processes, in the file pom.xml change on-demand to active for the cohql and cache-server profiles to enable active persistence. Run the cache-server , cohql and notifications as described above. In the CohQL session, run the following commands to add data <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Shutdown all three processes and restart the cache-server and cohql processes, then continue below. Re-query the test cache <markup lang=\"bash\" >CohQL&gt; select * from test Results \"two\" \"three\" \"one\" You can see that the cache data has automatically been recovered from disk during cluster startup. This active persistence data is stored in the directory persistence-data/active below the persistence tutorial directory. ",
            "title": "Enable Active Persistence"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Snapshots can be archived to a central location and then later retrieved and restored. Archiving snapshots requires defining the directory where archives are stored and configuring cache services to use an archive directory. To enable a snapshot archiver in this example, you need to uncomment the &lt;persistence&gt; element in the cache config file src/main/resources/persistence-cache-config.xml , and rebuild the project using Maven or Gradle. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Archive the existing data snapshot. <markup lang=\"bash\" >CohQL&gt; archive snapshot \"data\" \"PartitionedCache\" Are you sure you want to archive a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Archiving snapshot 'data' for service 'PartitionedCache' Results \"Success\" Inspect the archive directory contents to determine the cluster name <markup lang=\"bash\" >$ cd persistence-data/archives/ $ ls timmiddleton-s-cluster In the above the cluster name is timmiddleton&#8217;s cluster but a sanitized directory of timmiddleton-s-cluster has been used. View the contents of the archived snapshot directory, substituting your cluster directory. <markup lang=\"bash\" >$ cd timmiddleton-s-cluster/PartitionedCache/data/ $ ls 0-1-18063c8cc4c-1 12-1-18063c8cc4c-1 16-1-18063c8cc4c-1 2-1-18063c8cc4c-1 23-1-18063c8cc4c-1 27-1-18063c8cc4c-1 30-1-18063c8cc4c-1 7-1-18063c8cc4c-1 1-1-18063c8cc4c-1 13-1-18063c8cc4c-1 17-1-18063c8cc4c-1 20-1-18063c8cc4c-1 24-1-18063c8cc4c-1 28-1-18063c8cc4c-1 4-1-18063c8cc4c-1 8-1-18063c8cc4c-1 10-1-18063c8cc4c-1 14-1-18063c8cc4c-1 18-1-18063c8cc4c-1 21-1-18063c8cc4c-1 25-1-18063c8cc4c-1 29-1-18063c8cc4c-1 5-1-18063c8cc4c-1 9-1-18063c8cc4c-1 11-1-18063c8cc4c-1 15-1-18063c8cc4c-1 19-1-18063c8cc4c-1 22-1-18063c8cc4c-1 26-1-18063c8cc4c-1 3-1-18063c8cc4c-1 6-1-18063c8cc4c-1 meta.properties The directory shows 31 different data files and a meta.properties file that contains some metadata. These files are binary files and can only be used by recovering an archived snapshot. Validate the archived snapshot The following command will ensure that the archived snapshot can be retrieved and is valid. You should always use this command to ensure the integrity of your archived snapshots. <markup lang=\"bash\" >CohQL&gt; validate archived snapshot \"data\" \"PartitionedCache\" verbose ... various messages left out ... Results Attribute Value ---------------------------- ------------------------------------------------------------- Partition Count 31 Archived Snapshot Name=data, Service=PartitionedCache Original Storage Format BDB Storage Version 0 Implementation Version 0 Number of Partitions Present 31 Is Complete? true Is Archived Snapshot? true Persistence Version 14 Statistics test Size=3, Bytes=41, Indexes=0, Triggers=0, Listeners=0, Locks=0 Remove the local snapshot In this tutorial, we remove the local snapshot so that we can then retrieve the archived snapshot. A local snapshot of the same name cannot exist already if we want to retrieve an archived snapshot. <markup lang=\"bash\" >CohQL&gt; remove snapshot \"data\" \"PartitionedCache\" Are you sure you want to remove snapshot called 'data' for service 'PartitionedCache'? (y/n): y Removing snapshot 'data' for service 'PartitionedCache' Results \"Success CohQL&gt; list snapshots Results \"PartitionedCache\": [] Retrieve the archived snapshot <markup lang=\"bash\" >CohQL&gt; retrieve archived snapshot \"data\" \"PartitionedCache\" Are you sure you want to retrieve a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Retrieving snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] Remove all data from the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; select count() from test Results 3 CohQL&gt; delete from test Results select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-26 12:53:26.866/1734.102 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been suspended 2022-04-26 12:53:28.709/1735.944 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been resumed Results \"Success\" CohQL&gt; select count() from test Results 3 ",
            "title": "Enable a Snapshot Archiver"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " In this tutorial, you have learnt about Coherence Persistence and how you can use it with the Coherence Query Language (CohQL) to create, recover and managed snapshots, monitor snapshot operations via JMX MBean notifications as well as work with archived snapshots. ",
            "title": "Summary"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " Persistence Configuration ",
            "title": "See Also"
        },
        {
            "location": "/examples/tutorials/200-persistence/README",
            "text": " This tutorial walks through Coherence Persistence by using Coherence Query Language (CohQL) to create, recover and manage snapshots, monitor snapshot operations via JMX MBean notifications and work with archived snapshots. Coherence Persistence is a set of tools and technologies that manage the persistence and recovery of Coherence distributed caches. Cached data is persisted so that it can be quickly recovered after a catastrophic failure or after a cluster restart due to planned maintenance. Persistence can operate in two modes: On-Demand persistence mode – a cache service is manually persisted and recovered upon request using the persistence coordinator. The persistence coordinator is exposed as an MBean interface that provides operations for creating, archiving, and recovering snapshots of a cache service. Active persistence mode – In this mode, cache contents are automatically persisted on all mutations and are automatically recovered on cluster/service startup. The persistence coordinator can still be used in active persistence mode to perform on-demand snapshots. For more information on Coherence Persistence, please see the Coherence Documentation . Table of Contents What You Will Build What You Need Building the Example Code Review the Project Maven Configuration Persistence Configuration Listening to JMX Notifications Build and Run the Example Enable Active Persistence Enable a Snapshot Archiver Summary See Also What You Will Build You will review the requirements for running both on-demand and active persistence and carry out the following: Start one or more cache servers with on-demand persistence Start a CohQL session to insert data and create and manage snapshots Start a JMX MBean listener to monitor Persistence operations Change on-demand to active persistence and show this in action Work with archiving snapshots What You Need About 20-30 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package Build with Gradle Using the included Gradle wrapper the example can be built with the command: <markup lang=\"bash\" >./gradlew build Review the Project Maven Configuration The project is a Coherence project and imports the coherence-bom and coherence-dependencies POMs as shown below: <markup lang=\"xml\" >&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence-bom&lt;/artifactId&gt; &lt;version&gt;${coherence.version}&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; The coherence library is also included: <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;${coherence.group.id}&lt;/groupId&gt; &lt;artifactId&gt;coherence&lt;/artifactId&gt; &lt;/dependency&gt; We also define a number of profiles to run the DefaultCacheServer for each cluster and CohQL for each cluster. cache-server - Runs a DefaultCacheServer cohql - Runs a CohQL session notifications - Runs a process to subscribe to JMX Notification events cache-server - Runs a DefaultCacheServer <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cache-server&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cache-server&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx512m&lt;/argument&gt; &lt;argument&gt;-Xms512m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.mode=on-demand&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.persistence.base.dir=persistence-data&lt;/argument&gt; &lt;argument&gt;com.tangosol.net.DefaultCacheServer&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Set on-demand mode, which is the default Set the base directory for all persistence directories cohql - Runs a CohQL session <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;cohql&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;cohql&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;systemProperties&gt; &lt;property&gt; &lt;key&gt;coherence.cacheconfig&lt;/key&gt; &lt;value&gt;persistence-cache-config.xml&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.base.dir&lt;/key&gt; &lt;value&gt;persistence-data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.persistence.mode&lt;/key&gt; &lt;value&gt;on-demand&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.wka&lt;/key&gt; &lt;value&gt;127.0.0.1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;key&gt;coherence.distributed.localstorage&lt;/key&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/systemProperties&gt; &lt;mainClass&gt;com.tangosol.coherence.dslquery.QueryPlus&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; notifications - Runs a process to subscribe to JMX Notification events <markup lang=\"xml\" >&lt;profile&gt; &lt;id&gt;notifications&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;notifications&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;exec-maven-plugin&lt;/artifactId&gt; &lt;version&gt;${maven.exec.plugin.version}&lt;/version&gt; &lt;configuration&gt; &lt;executable&gt;java&lt;/executable&gt; &lt;arguments&gt; &lt;argument&gt;-classpath&lt;/argument&gt;&lt;classpath/&gt; &lt;argument&gt;-Dcoherence.log.level=3&lt;/argument&gt; &lt;argument&gt;-Dcoherence.wka=127.0.0.1&lt;/argument&gt; &lt;argument&gt;-Xmx128m&lt;/argument&gt; &lt;argument&gt;-Xms128m&lt;/argument&gt; &lt;argument&gt;-Dcoherence.cacheconfig=persistence-cache-config.xml&lt;/argument&gt; &lt;argument&gt;-Dcoherence.distributed.localstorage=false&lt;/argument&gt; &lt;argument&gt;com.oracle.coherence.tutorials.persistence.NotificationWatcher&lt;/argument&gt; &lt;argument&gt;PartitionedCache&lt;/argument&gt; &lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; Persistence Configuration By default, any partitioned service, including Federated services, will default to on-demand mode. This mode allows you to create and manage snapshots to default directories without any setup. A coherence directory off the users home directory is used to store all persistence-related data. If you wish to enable active persistence mode you can use a system property -Dcoherence.distributed.persistence.mode=active and this will use the default directories as described above. In this example we are also defining the base persistence directory using a system property -Dcoherence.distributed.persistence.base.dir=persistence-data . All other persistence directories will be created below this directory. Please see here for more details on configuring your persistence locations. In this tutorial, Persistence is configured in two files: An operational override file is used to configure non-default persistence environments and archive locations A cache configuration with file the &lt;persistence&gt; element is used to associate services with persistence environments, if you are not using the defaults. (This is initially commented out.) Cache Configuration File <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; The above cache configuration has the &lt;persistence&gt; element commented out for the first part of this tutorial. Operational Override File <markup lang=\"xml\" >&lt;coherence xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-operational-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-operational-config coherence-operational-config.xsd\"&gt; &lt;cluster-config&gt; &lt;snapshot-archivers&gt; &lt;directory-archiver id=\"shared-directory-archiver\"&gt; &lt;archive-directory system-property=\"coherence.distributed.persistence.archive.dir\"&gt;persistence-data/archives&lt;/archive-directory&gt; &lt;/directory-archiver&gt; &lt;/snapshot-archivers&gt; &lt;/cluster-config&gt; &lt;management-config&gt; &lt;managed-nodes system-property=\"coherence.management\"&gt;all&lt;/managed-nodes&gt; &lt;/management-config&gt; &lt;/coherence&gt; Defines a snapshot archiver to archive to a given directory Listening to JMX Notifications Persistence operations generate JMX notifications. You can register for these notifications to monitor and understand how long these operations are taking. You can use a tool such as VisualVM with the Coherence VisualVM plugin to monitor and manage persistence. See the VisualVM Plugin project on GitHub . Main entry point <markup lang=\"java\" >public static void main(String[] args) { if (args.length == 0) { System.out.println(\"Please provide a list of services to listen for notifications on\"); System.exit(1); } Set&lt;String&gt; setServices = new HashSet&lt;&gt;(Arrays.asList(args)); System.out.println(\"Getting MBeanServer...\"); Cluster cluster = CacheFactory.ensureCluster(); MBeanServer server = MBeanHelper.findMBeanServer(); Registry registry = cluster.getManagement(); if (server == null) { throw new RuntimeException(\"Unable to find MBeanServer\"); } try { for (String serviceName : setServices) { System.out.println(\"Registering listener for \" + serviceName); String mBeanName = \"Coherence:\" + CachePersistenceHelper.getMBeanName(serviceName); waitForRegistration(registry, mBeanName); ObjectName beanName = new ObjectName(mBeanName); NotificationListener listener = new PersistenceNotificationListener(serviceName); server.addNotificationListener(beanName, listener, null, null); mapListeners.put(beanName, listener); } System.out.println(\"Waiting for notifications. Use CTRL-C to interrupt.\"); Thread.sleep(Long.MAX_VALUE); } catch (Exception e) { e.printStackTrace(); } finally { // unregister all registered notifications mapListeners.forEach((k, v) -&gt; { try { server.removeNotificationListener(k, v); } catch (Exception eIgnore) { // ignore } }); } } Join the cluster and retrieve the MBeanServer and Registry Loop through the services provided as arguments and get the MBean name for the Persistence MBean Ensure the MBean is registered Add a notification listener on the Persistence MBean PersistenceNotificationListener implementation <markup lang=\"java\" >public static class PersistenceNotificationListener implements NotificationListener { Handle the notification <markup lang=\"java\" >@Override public synchronized void handleNotification(Notification notification, Object oHandback) { counter.incrementAndGet(); String userData = notification.getUserData().toString(); String message = notification.getMessage() + \" \" + notification.getUserData(); // default // determine if it's a begin or end notification String type = notification.getType(); if (type.indexOf(BEGIN) &gt; 0) { // handle begin notification and save the start time mapNotify.put(type, notification.getTimeStamp()); message = notification.getMessage(); } else if (type.indexOf(END) &gt; 0) { // handle end notification and try and find the matching begin notification String begin = type.replaceAll(END, BEGIN); Long start = mapNotify.get(begin); if (start != null) { message = \" \" + notification.getMessage() + (userData == null || userData.isEmpty() ? \"\" : userData) + \" (Duration=\" + (notification.getTimeStamp() - start) + \"ms)\"; mapNotify.remove(begin); } } else { message = serviceName + \": \" + type + \"\"; } System.out.println(new Date(notification.getTimeStamp()) + \" : \" + serviceName + \" (\" + type + \") \" + message); } Store the details of the begin notification Handle the end notification and determine the operation length Run the Example Once you have built the project as described earlier in this document, you can run it via Maven or Gradle. Maven Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./mvnw exec:exec -P cache-server Start a CohQL session. <markup lang=\"bash\" >./mvnw exec:java -P cohql Start a JMX Listener. <markup lang=\"bash\" >./mvnw exec:exec -P notifications Gradle Start one or more DefaultCache servers in separate terminals. <markup lang=\"bash\" >./gradlew runCacheServer Start a CohQL session. <markup lang=\"bash\" >./gradlew runCohql --console=plain and <markup lang=\"bash\" >./gradlew runNotifications --console=plain Run the following commands to exercise on-demand Persistence In the CohQL session, run the following commands to add data: <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Create a snapshot containing this data <markup lang=\"bash\" >CohQL&gt; list snapshots Results \"PartitionedCache\": [] CohQL&gt; create snapshot \"data\" \"PartitionedCache\" Are you sure you want to create a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Creating snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:03 AWST 2022 : PartitionedCache (create.snapshot.begin) Building snapshot \"data\" Tue Apr 26 10:57:06 AWST 2022 : PartitionedCache (create.snapshot.end) Successfully created snapshot \"data\" (Duration=3445ms) Clear the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; delete from test Results CohQL&gt; select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-11 16:23:06.691/499.700 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been suspended 2022-04-11 16:23:09.247/502.256 Oracle Coherence GE 14.1.1.0.0 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=3): Service PartitionedCache has been resumed Results \"Success\" select count() from test Results 3 You should see messages similar to the following in the notifications window: <markup lang=\"bash\" >Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.snapshot.begin) Recovering Snapshot \"data\" Tue Apr 26 10:57:48 AWST 2022 : PartitionedCache (recover.begin) Recovering snapshot \"data\" Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.end) Recovery Completed (Duration=623ms) Tue Apr 26 10:57:49 AWST 2022 : PartitionedCache (recover.snapshot.end) Successfully recovered snapshot \"data\" (Duration=631ms) You will be able to see the snapshots in the directory persistence-data/snapshots . Enable Active Persistence Run the following commands to exercise active Persistence After shutting down all running processes, in the file pom.xml change on-demand to active for the cohql and cache-server profiles to enable active persistence. Run the cache-server , cohql and notifications as described above. In the CohQL session, run the following commands to add data <markup lang=\"bash\" >CohQL&gt; select count() from test Results 0 CohQL&gt; CohQL&gt; insert into test key(1) value(\"one\") CohQL&gt; insert into test key(2) value(\"two\") CohQL&gt; insert into test key(3) value(\"three\") select count() from test Results 3 Shutdown all three processes and restart the cache-server and cohql processes, then continue below. Re-query the test cache <markup lang=\"bash\" >CohQL&gt; select * from test Results \"two\" \"three\" \"one\" You can see that the cache data has automatically been recovered from disk during cluster startup. This active persistence data is stored in the directory persistence-data/active below the persistence tutorial directory. Enable a Snapshot Archiver Snapshots can be archived to a central location and then later retrieved and restored. Archiving snapshots requires defining the directory where archives are stored and configuring cache services to use an archive directory. To enable a snapshot archiver in this example, you need to uncomment the &lt;persistence&gt; element in the cache config file src/main/resources/persistence-cache-config.xml , and rebuild the project using Maven or Gradle. <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;caching-scheme-mapping&gt; &lt;cache-mapping&gt; &lt;cache-name&gt;*&lt;/cache-name&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;/cache-mapping&gt; &lt;/caching-scheme-mapping&gt; &lt;caching-schemes&gt; &lt;distributed-scheme&gt; &lt;scheme-name&gt;server&lt;/scheme-name&gt; &lt;service-name&gt;PartitionedCache&lt;/service-name&gt; &lt;partition-count&gt;31&lt;/partition-count&gt; &lt;backing-map-scheme&gt; &lt;local-scheme&gt; &lt;unit-calculator&gt;BINARY&lt;/unit-calculator&gt; &lt;/local-scheme&gt; &lt;/backing-map-scheme&gt; &lt;!-- initially commented out as we are using system properties &lt;persistence&gt; &lt;environment&gt;default-active&lt;/environment&gt; &lt;archiver&gt;shared-directory-archiver&lt;/archiver&gt; &lt;/persistence&gt; --&gt; &lt;autostart&gt;true&lt;/autostart&gt; &lt;/distributed-scheme&gt; &lt;/caching-schemes&gt; &lt;/cache-config&gt; Archive the existing data snapshot. <markup lang=\"bash\" >CohQL&gt; archive snapshot \"data\" \"PartitionedCache\" Are you sure you want to archive a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Archiving snapshot 'data' for service 'PartitionedCache' Results \"Success\" Inspect the archive directory contents to determine the cluster name <markup lang=\"bash\" >$ cd persistence-data/archives/ $ ls timmiddleton-s-cluster In the above the cluster name is timmiddleton&#8217;s cluster but a sanitized directory of timmiddleton-s-cluster has been used. View the contents of the archived snapshot directory, substituting your cluster directory. <markup lang=\"bash\" >$ cd timmiddleton-s-cluster/PartitionedCache/data/ $ ls 0-1-18063c8cc4c-1 12-1-18063c8cc4c-1 16-1-18063c8cc4c-1 2-1-18063c8cc4c-1 23-1-18063c8cc4c-1 27-1-18063c8cc4c-1 30-1-18063c8cc4c-1 7-1-18063c8cc4c-1 1-1-18063c8cc4c-1 13-1-18063c8cc4c-1 17-1-18063c8cc4c-1 20-1-18063c8cc4c-1 24-1-18063c8cc4c-1 28-1-18063c8cc4c-1 4-1-18063c8cc4c-1 8-1-18063c8cc4c-1 10-1-18063c8cc4c-1 14-1-18063c8cc4c-1 18-1-18063c8cc4c-1 21-1-18063c8cc4c-1 25-1-18063c8cc4c-1 29-1-18063c8cc4c-1 5-1-18063c8cc4c-1 9-1-18063c8cc4c-1 11-1-18063c8cc4c-1 15-1-18063c8cc4c-1 19-1-18063c8cc4c-1 22-1-18063c8cc4c-1 26-1-18063c8cc4c-1 3-1-18063c8cc4c-1 6-1-18063c8cc4c-1 meta.properties The directory shows 31 different data files and a meta.properties file that contains some metadata. These files are binary files and can only be used by recovering an archived snapshot. Validate the archived snapshot The following command will ensure that the archived snapshot can be retrieved and is valid. You should always use this command to ensure the integrity of your archived snapshots. <markup lang=\"bash\" >CohQL&gt; validate archived snapshot \"data\" \"PartitionedCache\" verbose ... various messages left out ... Results Attribute Value ---------------------------- ------------------------------------------------------------- Partition Count 31 Archived Snapshot Name=data, Service=PartitionedCache Original Storage Format BDB Storage Version 0 Implementation Version 0 Number of Partitions Present 31 Is Complete? true Is Archived Snapshot? true Persistence Version 14 Statistics test Size=3, Bytes=41, Indexes=0, Triggers=0, Listeners=0, Locks=0 Remove the local snapshot In this tutorial, we remove the local snapshot so that we can then retrieve the archived snapshot. A local snapshot of the same name cannot exist already if we want to retrieve an archived snapshot. <markup lang=\"bash\" >CohQL&gt; remove snapshot \"data\" \"PartitionedCache\" Are you sure you want to remove snapshot called 'data' for service 'PartitionedCache'? (y/n): y Removing snapshot 'data' for service 'PartitionedCache' Results \"Success CohQL&gt; list snapshots Results \"PartitionedCache\": [] Retrieve the archived snapshot <markup lang=\"bash\" >CohQL&gt; retrieve archived snapshot \"data\" \"PartitionedCache\" Are you sure you want to retrieve a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Retrieving snapshot 'data' for service 'PartitionedCache' Results \"Success\" CohQL&gt; list snapshots Results \"PartitionedCache\": [\"data\"] Remove all data from the cache and recover the snapshot <markup lang=\"bash\" >CohQL&gt; select count() from test Results 3 CohQL&gt; delete from test Results select count() from test Results 0 CohQL&gt; recover snapshot \"data\" \"PartitionedCache\" Are you sure you want to recover a snapshot called 'data' for service 'PartitionedCache'? (y/n): y Recovering snapshot 'data' for service 'PartitionedCache' 2022-04-26 12:53:26.866/1734.102 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been suspended 2022-04-26 12:53:28.709/1735.944 Oracle Coherence CE 22.06 &lt;D5&gt; (thread=DistributedCache:PartitionedCache, member=2): Service PartitionedCache has been resumed Results \"Success\" CohQL&gt; select count() from test Results 3 Summary In this tutorial, you have learnt about Coherence Persistence and how you can use it with the Coherence Query Language (CohQL) to create, recover and managed snapshots, monitor snapshot operations via JMX MBean notifications as well as work with archived snapshots. See Also Persistence Configuration ",
            "title": "Persistence"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " This guide will build simple examples showing different uses of the Health Check API from application code and in containerized environments. The Basic Health Check API introduces the basic health check APIs. Application Health Checks shows how to add custom application health checks Container Health Checks shows how to add health checks to be used in containers Docker Image Health Checks shows adding health checks to an image built with Docker Buildah Image Health Checks shows adding health checks to an image built with Buildah ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package ",
            "title": "Building the Example Code"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package ",
            "title": "What You Need"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Version 22.06 of Coherence introduced a Health Check API to provide simple checks for the overall health of a Coherence member. This guide shows some ways this API can be used. What You Will Build This guide will build simple examples showing different uses of the Health Check API from application code and in containerized environments. The Basic Health Check API introduces the basic health check APIs. Application Health Checks shows how to add custom application health checks Container Health Checks shows how to add health checks to be used in containers Docker Image Health Checks shows adding health checks to an image built with Docker Buildah Image Health Checks shows adding health checks to an image built with Buildah What You Need About 15 minutes A favorite text editor or IDE JDK 17 or later Maven 3.8+ or Gradle 4+ Although the source comes with the Maven and Gradle wrappers included, so they can be built without first installing either build tool. You can also import the code straight into your IDE: IntelliJ IDEA Building the Example Code The source code for the guides and tutorials can be found in the Coherence CE GitHub repo The example source code is structured as both a Maven and a Gradle project and can be easily built with either of those build tools. The examples are stand-alone projects so each example can be built from the specific project directory without needing to build the whole Coherence project. Build with Maven Using the included Maven wrapper the example can be built with the command: <markup lang=\"bash\" >./mvnw clean package ",
            "title": "Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The first test in BasicHealthIT checks that everything is \"started\". The Coherence instance is obtained (there is only one instance running in this case so the Coherence.getInstance() method can be used). From the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(() -&gt; registry.allHealthChecksStarted(), is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksStarted() returns true , which it should as soon as all services are started. At this point Coherence may not be \"ready\" or \"safe\", but it is started. ",
            "title": "Check All Health Checks are Started"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The second test in BasicHealthIT checks that everything is \"ready\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(() -&gt; registry.allHealthChecksStarted(), is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksReady() returns true , which it should as soon as all services reach the \"ready\" state. ",
            "title": "Check All Health Checks are Ready"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The third test in BasicHealthIT checks that everything is \"safe\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(() -&gt; registry.allHealthChecksStarted(), is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksSafe() returns true , which it should as soon as all services reach the \"safe\" state. ",
            "title": "Check All Health Checks are Safe"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " A Collection of health checks can be obtained using the Registry instances getHealthChecks() method. The example below shows a simple test case that obtains all the registered health checks. There is an assertion that the collection returned is not empty. As the test uses the default Coherence cache configuration file, this will start a distributed cache service named PartitionedCache , so there will be a health check registered with this name. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthChecks() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Collection&lt;HealthCheck&gt; healthChecks = registry.getHealthChecks(); assertThat(healthChecks.isEmpty(), is(false)); HealthCheck healthCheck = healthChecks.stream() .filter(h -&gt; \"PartitionedCache\".equals(h.getName())) .findFirst() .orElse(null); assertThat(healthCheck, is(notNullValue())); } ",
            "title": "Gat All Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Instead of getting the collection of all health checks, a single health check can be obtained by using its name. The Registry instances getHealthCheck(String name) method can be used to obtain a health check instance by name. The method returns an Optional that will be empty if there is no health check registered with the specified name. The example below obtains the health check named PartitionedCache , which should exist as the test uses the default Coherence cache configuration file. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthCheckByName() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(\"PartitionedCache\"); assertThat(optional.isPresent(), is(true)); } ",
            "title": "Get a Health Check by Name"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The Registry health check API has methods to obtain instances of the health checks that have been registered on the local member. Gat All Health Checks A Collection of health checks can be obtained using the Registry instances getHealthChecks() method. The example below shows a simple test case that obtains all the registered health checks. There is an assertion that the collection returned is not empty. As the test uses the default Coherence cache configuration file, this will start a distributed cache service named PartitionedCache , so there will be a health check registered with this name. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthChecks() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Collection&lt;HealthCheck&gt; healthChecks = registry.getHealthChecks(); assertThat(healthChecks.isEmpty(), is(false)); HealthCheck healthCheck = healthChecks.stream() .filter(h -&gt; \"PartitionedCache\".equals(h.getName())) .findFirst() .orElse(null); assertThat(healthCheck, is(notNullValue())); } Get a Health Check by Name Instead of getting the collection of all health checks, a single health check can be obtained by using its name. The Registry instances getHealthCheck(String name) method can be used to obtain a health check instance by name. The method returns an Optional that will be empty if there is no health check registered with the specified name. The example below obtains the health check named PartitionedCache , which should exist as the test uses the default Coherence cache configuration file. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthCheckByName() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(\"PartitionedCache\"); assertThat(optional.isPresent(), is(true)); } ",
            "title": "Get Health Check Instances"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The basic health check API includes methods to check the local member&#8217;s health and obtain health check instances. This API is demonstrated using a simple integration test in src/test/java/com/oracle/coherence/guides/health/BasicHealthIT.java The test first bootstraps a Coherence storage member using the Coherence bootstrap API. The test will fail if Coherence takes longer than five minutes to start (it should be up in seconds). <markup lang=\"java\" title=\"BasicHealthIT.java\" > @BeforeAll static void startCoherence() throws Exception { Coherence.clusterMember() .start() .get(5, TimeUnit.MINUTES); } When the tests finish Coherence is shut down. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @AfterAll static void cleanup() { Coherence coherence = Coherence.getInstance(); if (coherence != null) { coherence.close(); } } Check All Health Checks are Started The first test in BasicHealthIT checks that everything is \"started\". The Coherence instance is obtained (there is only one instance running in this case so the Coherence.getInstance() method can be used). From the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(() -&gt; registry.allHealthChecksStarted(), is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksStarted() returns true , which it should as soon as all services are started. At this point Coherence may not be \"ready\" or \"safe\", but it is started. Check All Health Checks are Ready The second test in BasicHealthIT checks that everything is \"ready\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(() -&gt; registry.allHealthChecksStarted(), is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksReady() returns true , which it should as soon as all services reach the \"ready\" state. Check All Health Checks are Safe The third test in BasicHealthIT checks that everything is \"safe\". The Coherence instance is obtained and from the Coherence instance the management Registry is obtained. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldEventuallyBeStarted() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Eventually.assertDeferred(() -&gt; registry.allHealthChecksStarted(), is(true)); } The test then asserts that \"eventually\", the call to registry.allHealthChecksSafe() returns true , which it should as soon as all services reach the \"safe\" state. Get Health Check Instances The Registry health check API has methods to obtain instances of the health checks that have been registered on the local member. Gat All Health Checks A Collection of health checks can be obtained using the Registry instances getHealthChecks() method. The example below shows a simple test case that obtains all the registered health checks. There is an assertion that the collection returned is not empty. As the test uses the default Coherence cache configuration file, this will start a distributed cache service named PartitionedCache , so there will be a health check registered with this name. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthChecks() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Collection&lt;HealthCheck&gt; healthChecks = registry.getHealthChecks(); assertThat(healthChecks.isEmpty(), is(false)); HealthCheck healthCheck = healthChecks.stream() .filter(h -&gt; \"PartitionedCache\".equals(h.getName())) .findFirst() .orElse(null); assertThat(healthCheck, is(notNullValue())); } Get a Health Check by Name Instead of getting the collection of all health checks, a single health check can be obtained by using its name. The Registry instances getHealthCheck(String name) method can be used to obtain a health check instance by name. The method returns an Optional that will be empty if there is no health check registered with the specified name. The example below obtains the health check named PartitionedCache , which should exist as the test uses the default Coherence cache configuration file. <markup lang=\"java\" title=\"BasicHealthIT.java\" > @Test void shouldGetHealthCheckByName() { Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(\"PartitionedCache\"); assertThat(optional.isPresent(), is(true)); } ",
            "title": "The Basic Health Check API"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Application health checks can be automatically registered by Coherence during start-up. When Coherence starts, it will use the Java ServiceLoader to discover any HealthCheck implementations, and automatically register them. To automatically register the example ApplicationHealth class above, create a META-INF/service/com.tangosol.util.HealthCheck file, containing a single line that is the name of the application health check. <markup title=\"META-INF/service/com.tangosol.util.HealthCheck\" >com.oracle.coherence.guides.health.ApplicationHealth; Alternatively, if using a module-info.java file add the health check using the provides clause. <markup lang=\"java\" title=\"module-info.java\" >module coherence.guides.health { provides com.tangosol.util.HealthCheck with com.oracle.coherence.guides.health.ApplicationHealth; } When Coherence starts, it will use ServiceLoader to load HealthCheck instances, which will discover and load an ApplicationHealth instance. ",
            "title": "Health Check Auto-Registration"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Applications can add custom health checks by creating a class that implements the com.tangosol.util.HealthCheck interface, and registering the health check with the Registry . The example ApplicationHealth class below implements the HealthCheck interface. The getName() method returns \"Demo\" , which is a unique name for this health check. In this example, the class does not have any processing in the health check methods. In a real application health check these methods would perform custom application specific checks. <markup lang=\"java\" title=\"ApplicationHealth.java\" >/** * A simple custom health check. */ public class ApplicationHealth implements HealthCheck { /** * The health check name. */ public static final String NAME = \"Demo\"; @Override public String getName() { return NAME; } @Override public boolean isReady() { return true; } @Override public boolean isLive() { return true; } @Override public boolean isStarted() { return true; } @Override public boolean isSafe() { return true; } } The health check can be registered in application code using the Registry.register(HealthCheck hc) method. <markup lang=\"java\" > Coherence coherence = Coherence.getInstance(); Registry registry = coherence.getManagement(); ApplicationHealth healthCheck = new ApplicationHealth(); registry.register(healthCheck); Optional&lt;HealthCheck&gt; optional = registry.getHealthCheck(ApplicationHealth.NAME); assertThat(optional.isPresent(), is(true)); When no longer required, the health check can be unregistered using the Registry.unregister(String name) method. <markup lang=\"java\" > registry.unregister(healthCheck); optional = registry.getHealthCheck(ApplicationHealth.NAME); assertThat(optional.isPresent(), is(false)); Health Check Auto-Registration Application health checks can be automatically registered by Coherence during start-up. When Coherence starts, it will use the Java ServiceLoader to discover any HealthCheck implementations, and automatically register them. To automatically register the example ApplicationHealth class above, create a META-INF/service/com.tangosol.util.HealthCheck file, containing a single line that is the name of the application health check. <markup title=\"META-INF/service/com.tangosol.util.HealthCheck\" >com.oracle.coherence.guides.health.ApplicationHealth; Alternatively, if using a module-info.java file add the health check using the provides clause. <markup lang=\"java\" title=\"module-info.java\" >module coherence.guides.health { provides com.tangosol.util.HealthCheck with com.oracle.coherence.guides.health.ApplicationHealth; } When Coherence starts, it will use ServiceLoader to load HealthCheck instances, which will discover and load an ApplicationHealth instance. ",
            "title": "Application Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The example above can be built and tested using a simple Maven command. The command will run a Maven build with the docker profile enabled, which will use Docker to build an image. The name of the image is configured in the properties' section of the example pom.xml to be coherence-health:1.0.0 . <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker A container can then be run using the image. The normal docker run command is used, in this case the container is given the name test . <markup lang=\"bash\" >docker run -d --name test coherence-health:1.0.0 After starting the container, the set of running containers can be listed using docker ps , which should display the test container: <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 2 seconds (health: starting) test Because the image has a health check configured the status in this case included the current health state Up 2 seconds (health: starting) . At this point the container is still starting, so the Coherence health endpoint ( http://127.0.0.1:6676/ready ) has not returned a 200 response, as Coherence is still starting. Once Coherence has started and te health check reports ready, the container status will change to healthy . <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 4 minutes (healthy) test ",
            "title": "Build and Run the Image"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " To use health checks in Docker, the HEALTHCHECK instruction can be used in the Dockerfile . The format of the HEALTHCHECK instruction is shown below: <markup >HEALTHCHECK [OPTIONS] CMD command The command is typically a simple command line, such as curl or a shell script, or Java command line. For example, if the Coherence health check endpoint is enabled on a fixed port 6676 , then the HEALTHCHECK instruction&#8217;s CMD can be set to curl -f http://127.0.0.1:6676/ready An example of a simple Coherence Dockerfile with a health check is shown below. This example image uses OpenJDK as a base image. Coherence jar is added to the image and the health check port fixed to 6676 using the COHERENCE_HEALTH_HTTP_PORT environment variable. When the image runs, the entry point will just start Coherence . <markup title=\"src/docker/OpenJDK.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM openjdk:11-jre ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD curl -f http://127.0.0.1:6676/ready || exit 1 ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] Build and Run the Image The example above can be built and tested using a simple Maven command. The command will run a Maven build with the docker profile enabled, which will use Docker to build an image. The name of the image is configured in the properties' section of the example pom.xml to be coherence-health:1.0.0 . <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker A container can then be run using the image. The normal docker run command is used, in this case the container is given the name test . <markup lang=\"bash\" >docker run -d --name test coherence-health:1.0.0 After starting the container, the set of running containers can be listed using docker ps , which should display the test container: <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 2 seconds (health: starting) test Because the image has a health check configured the status in this case included the current health state Up 2 seconds (health: starting) . At this point the container is still starting, so the Coherence health endpoint ( http://127.0.0.1:6676/ready ) has not returned a 200 response, as Coherence is still starting. Once Coherence has started and te health check reports ready, the container status will change to healthy . <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 4 minutes (healthy) test ",
            "title": "Docker Image Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " The Podman and Buildah tools are common replacements for Docker when running in Linux. When using Buildah to create images, health checks can be added to an image using the Buildah CLI. To support health checks Buildah must be configured to use \"Docker\" format. The simplest way to do this is to export the BUILDAH_FORMAT environment variable <markup lang=\"bash\" >export BUILDAH_FORMAT=docker Now, the Buildah CLI can be used to create an image. <markup lang=\"bash\" >buildah from --name coherence openjdk:11-jre buildah copy coherence coherence.jar /coherence/lib/coherence.jar buildah config --healthcheck-start-period 10s --healthcheck-interval 10s \\ --healthcheck \"CMD curl -f http://127.0.0.1:6676/ready || exit 1\" coherence buildah config \\ --entrypoint '[\"java\"]' --cmd '-cp /coherence/lib/* com.tangosol.net.Coherence' \\ -e COHERENCE_HEALTH_HTTP_PORT=6676 \\ coherence buildah commit coherence coherence-health:1.0.0 buildah push -f v2s2 coherence-health:1.0.0 docker-daemon:coherence-health:1.0.0 The Buildah commands above build the same image that was built with the Dockerfile in the previous section. The final command above pushes the coherence-health:1.0.0 image built by Buildah into a Docker daemon, so it can be run using Docker. ",
            "title": "Buildah Image Health Checks"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Sometimes a distroless image is used as a base image for applications. These are images that do not contain a Linux distribution. There are various reasons for this such as image size, but mainly security, as the base image does not contain a lot of Linux utilities that may introduce CVEs. The example Coherence images use distroless base images. When using a distroless base image, the curl utility is not present, so it cannot be used as the health check command. In the distroless base images used by Coherence, all that is present is a Linux kernel and Java. This means that the only way to run any health check commands would be to execute a Java command. As part of the Coherence health check API there is a simple http client class com.tangosol.util.HealthCheckClient that can be used to execute a health check as a Java command. The Java command line to execute a health check would be: <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.util.HealthCheckClient http://127.0.0.1:6676/ready This Distroless.Dockerfile in the source code contains an example of using a Java health check command. Because the health check command is running Java and not a simple O/S command, the format of the CMD parameters is slightly different than the previous example. <markup title=\"src/docker/Distroless.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM gcr.io/distroless/java17-debian11 ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD [\"java\", \"-cp\", \"/coherence/lib/coherence.jar\", \"com.tangosol.util.HealthCheckClient\", \"http://127.0.0.1:6676/ready\", \"||\", \"exit\", \"1\"] ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] The example distroless image can be built using Maven, as before but specifying the distroless Dockerfile. <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker -Ddocker.file=Distroless.Dockerfile The Maven command builds the same test image with the tag coherence-health:1.0.0 which can be run in the same way as the previous examples. ",
            "title": "Health Checks in Distroless Base Images"
        },
        {
            "location": "/examples/guides/090-health-checks/README",
            "text": " Health checks are extremely useful when running Coherence in containers, as they can signal to the container management system (e.g. Docker, or Kubernetes) that the Coherence container is running and healthy. The OCI specification allows an image to define a command to run to check its health. This is supported by image build tools such as Docker and Buildah. Docker Image Health Checks To use health checks in Docker, the HEALTHCHECK instruction can be used in the Dockerfile . The format of the HEALTHCHECK instruction is shown below: <markup >HEALTHCHECK [OPTIONS] CMD command The command is typically a simple command line, such as curl or a shell script, or Java command line. For example, if the Coherence health check endpoint is enabled on a fixed port 6676 , then the HEALTHCHECK instruction&#8217;s CMD can be set to curl -f http://127.0.0.1:6676/ready An example of a simple Coherence Dockerfile with a health check is shown below. This example image uses OpenJDK as a base image. Coherence jar is added to the image and the health check port fixed to 6676 using the COHERENCE_HEALTH_HTTP_PORT environment variable. When the image runs, the entry point will just start Coherence . <markup title=\"src/docker/OpenJDK.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM openjdk:11-jre ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD curl -f http://127.0.0.1:6676/ready || exit 1 ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] Build and Run the Image The example above can be built and tested using a simple Maven command. The command will run a Maven build with the docker profile enabled, which will use Docker to build an image. The name of the image is configured in the properties' section of the example pom.xml to be coherence-health:1.0.0 . <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker A container can then be run using the image. The normal docker run command is used, in this case the container is given the name test . <markup lang=\"bash\" >docker run -d --name test coherence-health:1.0.0 After starting the container, the set of running containers can be listed using docker ps , which should display the test container: <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 2 seconds (health: starting) test Because the image has a health check configured the status in this case included the current health state Up 2 seconds (health: starting) . At this point the container is still starting, so the Coherence health endpoint ( http://127.0.0.1:6676/ready ) has not returned a 200 response, as Coherence is still starting. Once Coherence has started and te health check reports ready, the container status will change to healthy . <markup lang=\"bash\" >CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 520559d772e3 coherence-health:1.0.0 \"java -cp /coherence…\" 3 seconds ago Up 4 minutes (healthy) test Buildah Image Health Checks The Podman and Buildah tools are common replacements for Docker when running in Linux. When using Buildah to create images, health checks can be added to an image using the Buildah CLI. To support health checks Buildah must be configured to use \"Docker\" format. The simplest way to do this is to export the BUILDAH_FORMAT environment variable <markup lang=\"bash\" >export BUILDAH_FORMAT=docker Now, the Buildah CLI can be used to create an image. <markup lang=\"bash\" >buildah from --name coherence openjdk:11-jre buildah copy coherence coherence.jar /coherence/lib/coherence.jar buildah config --healthcheck-start-period 10s --healthcheck-interval 10s \\ --healthcheck \"CMD curl -f http://127.0.0.1:6676/ready || exit 1\" coherence buildah config \\ --entrypoint '[\"java\"]' --cmd '-cp /coherence/lib/* com.tangosol.net.Coherence' \\ -e COHERENCE_HEALTH_HTTP_PORT=6676 \\ coherence buildah commit coherence coherence-health:1.0.0 buildah push -f v2s2 coherence-health:1.0.0 docker-daemon:coherence-health:1.0.0 The Buildah commands above build the same image that was built with the Dockerfile in the previous section. The final command above pushes the coherence-health:1.0.0 image built by Buildah into a Docker daemon, so it can be run using Docker. Health Checks in Distroless Base Images Sometimes a distroless image is used as a base image for applications. These are images that do not contain a Linux distribution. There are various reasons for this such as image size, but mainly security, as the base image does not contain a lot of Linux utilities that may introduce CVEs. The example Coherence images use distroless base images. When using a distroless base image, the curl utility is not present, so it cannot be used as the health check command. In the distroless base images used by Coherence, all that is present is a Linux kernel and Java. This means that the only way to run any health check commands would be to execute a Java command. As part of the Coherence health check API there is a simple http client class com.tangosol.util.HealthCheckClient that can be used to execute a health check as a Java command. The Java command line to execute a health check would be: <markup lang=\"bash\" >java -cp coherence.jar com.tangosol.util.HealthCheckClient http://127.0.0.1:6676/ready This Distroless.Dockerfile in the source code contains an example of using a Java health check command. Because the health check command is running Java and not a simple O/S command, the format of the CMD parameters is slightly different than the previous example. <markup title=\"src/docker/Distroless.Dockerfile\" ># Copyright (c) 2022, Oracle and/or its affiliates. # # Licensed under the Universal Permissive License v 1.0 as shown at # https://oss.oracle.com/licenses/upl. # FROM gcr.io/distroless/java17-debian11 ADD coherence.jar /coherence/lib/coherence.jar ENV COHERENCE_HEALTH_HTTP_PORT=6676 HEALTHCHECK --start-period=30s --interval=30s \\ CMD [\"java\", \"-cp\", \"/coherence/lib/coherence.jar\", \"com.tangosol.util.HealthCheckClient\", \"http://127.0.0.1:6676/ready\", \"||\", \"exit\", \"1\"] ENTRYPOINT [\"java\"] CMD [\"-cp\", \"/coherence/lib/*\", \"com.tangosol.net.Coherence\"] The example distroless image can be built using Maven, as before but specifying the distroless Dockerfile. <markup lang=\"bash\" >mvn clean package -DskipTests -Pdocker -Ddocker.file=Distroless.Dockerfile The Maven command builds the same test image with the tag coherence-health:1.0.0 which can be run in the same way as the previous examples. ",
            "title": "Container Health Checks"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " Coherence MP Metrics provides support for [Eclipse MicroProfile Metrics] ( https://microprofile.io/project/eclipse/microprofile-metrics ) within Coherence cluster members. This is a very simple module that allows you to publish Coherence metrics into MicroProfile Metric Registries available at runtime, and adds Coherence-specific tags to all the metrics published within the process, in order to distinguish them on the monitoring server, such as Prometheus. ",
            "title": "Coherence MicroProfile Metrics"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " In order to use Coherence MP Metrics, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" >&lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-metrics&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; That&#8217;s it&#8201;&#8212;&#8201;once the module above is in the class path, Coherence will discover MpMetricRegistryAdapter service it provides, and use it to publish all standard Coherence metrics to the vendor registry, and any user-defined application metrics to the application registry. All the metrics will be published as gauges, because they represent point-in-time values of various MBean attributes. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/metrics/README",
            "text": " There could be hundreds of members in a Coherence cluster, with each member publishing potentially the same set of metrics. There could also be many Coherence clusters in the environment, possibly publishing to the same monitoring server instance. In order to distinguish metrics coming from different clusters, as well as from different members of the same cluster, Coherence MP Metrics will automatically add several tags to ALL the metrics published within the process. The tags added are: Tag Name Tag Value cluster the cluster name site the site the member belongs to (if set) machine the machine member is on (if set) member the name of the member (if set) node_id the node ID of the member role the member&#8217;s role This ensures that the metrics published by one member do not collide with and overwrite the metrics published by another members, and allows you to query and aggregate metrics based on the values of the tags above if desired. ",
            "title": "Coherence Global Tags"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " About 15 Minutes IntelliJ IDEA JDK 17 or later ",
            "title": "What You Need"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. ",
            "title": "Installing IntelliJ IDEA"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git ",
            "title": "Clone the Coherence CE Repository"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. ",
            "title": "Importing a Guide or Tutorial"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " IntelliJ IDEA ",
            "title": "See Also"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " You will pick a Coherence guide or example and import it into IntelliJ IDEA. Then you can then read and follow the individual guide or tutorial documentation. What You Need About 15 Minutes IntelliJ IDEA JDK 17 or later Installing IntelliJ IDEA If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. Clone the Coherence CE Repository To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git Importing a Guide or Tutorial With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. See Also IntelliJ IDEA ",
            "title": "What You Will Build"
        },
        {
            "location": "/examples/setup/intellij",
            "text": " This guide walks you through importing one of the Coherence guides or tutorials into IntelliJ IDEA. What You Will Build You will pick a Coherence guide or example and import it into IntelliJ IDEA. Then you can then read and follow the individual guide or tutorial documentation. What You Need About 15 Minutes IntelliJ IDEA JDK 17 or later Installing IntelliJ IDEA If you don’t have IntelliJ IDEA (Ultimate Edition) installed yet, visit the link up above. From there, you can download a copy for your platform. To install it simply unpack the downloaded archive. When you’re done, go ahead and launch IntelliJ IDEA. Clone the Coherence CE Repository To import an existing project you need to clone the Coherence CE repository if you have not already done so. <markup lang=\"bash\" >$ git clone https://github.com/oracle/coherence.git Importing a Guide or Tutorial With IntelliJ IDEA up and running, click Import Project on the Welcome Screen, or File | New Project From Existing Sources . Navigate to the cloned repository directory, then prj/examples and then to the guide or tutorial directory. For example for Near Caching you would open prj/examples/guides/130-near-caching directory. In the pop-up dialog select either Maven or Gradle as the external model to import from. Once the project is imported, follow the README to build or run the example or guide. See Also IntelliJ IDEA ",
            "title": "Import a Project Into IntelliJ IDEA"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " The coherence-micrometer module provides integration between Coherence metrics and Micrometer allowing Coherence metrics to be published via any of the Micrometer registries. ",
            "title": "Coherence Micrometer Metrics"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " In order to use Coherence Micrometer metrics, you need to declare the module as a dependency in your pom.xml and bind your Micrometer registry with the Coherence metrics adapter: <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-micrometer&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; The coherence-micrometer provides a Micrometer MeterBinder implementation class called CoherenceMicrometerMetrics . This class is a singleton and cannot be constructed, to access it use the CoherenceMicrometerMetrics.INSTANCE field. Micrometer provides many registry implementations to support different metrics applications and formats. For example, to bind Coherence metrics to the Micrometer PrometheusMeterRegistry , create the PrometheusMeterRegistry as documented in the Micrometer documentation , and call the CoherenceMicrometerMetrics class&#8217;s bindTo method: <markup lang=\"java\" >PrometheusMeterRegistry prometheusRegistry = new PrometheusMeterRegistry(PrometheusConfig.DEFAULT); // complete registy configuration... CoherenceMicrometerMetrics.INSTANCE.bindTo(prometheusRegistry); Micrometer registries can be bound to Coherence at any time, before or after Coherence starts. As Coherence creates or removed metrics they will be registered with or removed from the Micrometer registries. ",
            "title": "Usage"
        },
        {
            "location": "/coherence-micrometer/README",
            "text": " Micrometer has a global registry available which Coherence will bind to automatically if the coherence.micrometer.bind.to.global system property has been set to true (this property is false by default). ",
            "title": "Automatic Global Registry Binding"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config provides support for Eclipse MicroProfile Config within Coherence cluster members. It allows you both to configure various Coherence parameters from the values specified in any of the supported config sources, and to use Coherence cache as another, mutable config source. ",
            "title": "Coherence MicroProfile Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " In order to use Coherence MP Config, you need to declare it as a dependency in your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;com.oracle.coherence.ce&lt;/groupId&gt; &lt;artifactId&gt;coherence-mp-config&lt;/artifactId&gt; &lt;version&gt;23.03-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; You will also need an implementation of the Eclipse MP Config specification as a dependency. For example, if you are using Helidon , add the following to your pom.xml : <markup lang=\"xml\" > &lt;dependency&gt; &lt;groupId&gt;io.helidon.microprofile.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-microprofile-config&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- optional: add it if you want YAML config file support --&gt; &lt;dependency&gt; &lt;groupId&gt;io.helidon.config&lt;/groupId&gt; &lt;artifactId&gt;helidon-config-yaml&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; ",
            "title": "Usage"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence provides a number of configuration properties that can be specified by the users in order to define certain attributes or to customize cluster member behavior at runtime. For example, attributes such as cluster and role name, as well as whether a cluster member should or should not store data, can be specified via system properties: -Dcoherence.cluster=MyCluster -Dcoherence.role=Proxy -Dcoherence.distributed.localstorage=false Most of these attributes can also be defined within the operational or cache configuration file. For example, you could define first two attributes, cluster name and role, within the operational config override file: <markup lang=\"xml\" > &lt;cluster-config&gt; &lt;member-identity&gt; &lt;cluster-name&gt;MyCluster&lt;/cluster-name&gt; &lt;role-name&gt;Proxy&lt;/role-name&gt; &lt;/member-identity&gt; &lt;/cluster-config&gt; While these two options are more than enough in most cases, there are some issues with them being the only way to configure Coherence: When you are using one of Eclipse MicroProfile implementations, such as Helidon as the foundation of your application, it would be nice to define some of Coherence configuration parameters along with your other configuration parameters, and not in the separate file or via system properties. In some environments, such as Kubernetes, Java system properties are cumbersome to use, and environment variables are a preferred way of passing configuration properties to containers. Unfortunately, neither of the two use cases above is supported out of the box, but that&#8217;s the gap Coherence MP Config is designed to fill. As long as you have coherence-mp-config and an implementation of Eclipse MP Config specification to your class path, Coherence will use any of the standard or custom config sources to resolve various configuration options it understands. Standard config sources in MP Config include META-INF/microprofile-config.properties file, if present in the class path, environment variables, and system properties (in that order, with the properties in the latter overriding the ones from the former). That will directly address problem #2 above, and allow you to specify Coherence configuration options via environment variables within Kubernetes YAML files, for example: <markup lang=\"yaml\" > containers: - name: my-app image: my-company/my-app:1.0.0 env: - name: COHERENCE_CLUSTER value: \"MyCluster\" - name: COHERENCE_ROLE value: \"Proxy\" - name: COHERENCE_DISTRIBUTED_LOCALSTORAGE value: \"false\" Of course, the above is just an example&#8201;&#8212;&#8201;if you are running your Coherence cluster in Kubernetes, you should really be using Coherence Operator instead, as it will make both the configuration and the operation of your Coherence cluster much easier. You will also be able to specify Coherence configuration properties along with the other configuration properties of your application, which will allow you to keep everything in one place, and not scattered across many files. For example, if you are writing a Helidon application, you can simply add coherence section to your application.yaml : <markup lang=\"yaml\" >coherence: cluster: MyCluster role: Proxy distributed: localstorage: false ",
            "title": "Configuring Coherence using MP Config"
        },
        {
            "location": "/coherence-mp/config/README",
            "text": " Coherence MP Config also provides an implementation of Eclipse MP Config ConfigSource interface, which allows you to store configuration parameters in a Coherence cache. This has several benefits: Unlike pretty much all of the default configuration sources, which are static, configuration options stored in a Coherence cache can be modified without forcing you to rebuild your application JARs or Docker images. You can change the value in one place, and it will automatically be visible and up to date on all the members. While the features above give you incredible amount of flexibility, we also understand that such flexibility is not always desired, and the feature is disabled by default. If you want to enable it, you need to do so explicitly, by registering CoherenceConfigSource as a global interceptor in your cache configuration file: <markup lang=\"xml\" >&lt;cache-config xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://xmlns.oracle.com/coherence/coherence-cache-config\" xsi:schemaLocation=\"http://xmlns.oracle.com/coherence/coherence-cache-config coherence-cache-config.xsd\"&gt; &lt;interceptors&gt; &lt;interceptor&gt; &lt;instance&gt; &lt;class-name&gt;com.oracle.coherence.mp.config.CoherenceConfigSource&lt;/class-name&gt; &lt;/instance&gt; &lt;/interceptor&gt; &lt;/interceptors&gt; &lt;!-- your cache mappings and schemes... --&gt; &lt;/cache-config&gt; Once you do that, CoherenceConfigSource will be activated as soon as your cache factory is initialized, and injected into the list of available config sources for your application to use via standard MP Config APIs. By default, it will be configured with a priority (ordinal) of 500, making it higher priority than all the standard config sources, thus allowing you to override the values provided via config files, environment variables and system properties. However, you have full control over that behavior and can specify different ordinal via coherence.mp.config.source.ordinal configuration property. ",
            "title": "Using Coherence Cache as a Config Source"
        }
 ]
}